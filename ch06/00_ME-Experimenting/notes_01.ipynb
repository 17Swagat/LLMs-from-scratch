{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2853bb5c",
   "metadata": {},
   "source": [
    "# **Book [Temp]**\n",
    "> Right now the loaded model seems to be off. The outputs are not as expected. \n",
    "\n",
    "> It looks like the model pretrained weights are not loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6f3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#####################################\n",
    "# Chapter 3\n",
    "#####################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 4\n",
    "#####################################\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c53d40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62909328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbc42577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\$LLMS\\LLMs-from-scratch\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "# Loading PreTrained GPT2 124M Model Weights\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "model_dir = '../../ch05/00_ME-Experimenting/gpt2/124M/'\n",
    "tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n",
    "params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "\n",
    "print(settings)\n",
    "print(params.keys()) # Model parameters loaded successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eab854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "\n",
    "\n",
    "load_weights_into_gpt(gpt, params)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "482547c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # New (not in book): numerical stability tip to get equivalent results on mps device\n",
    "            # subtract rowwise max before softmax\n",
    "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eaf2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9427aed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you further forward, from being able to live on your meager earnings to investing in the futures industry as your dream of a more just\n",
      "Every effort moves you toward achieving perfection when you realize a certain threshold set by your goals - for example, taking a long day to walk the gym\n",
      "Every effort moves you along! You decide which strategy wins from the first decision! - Every game begins the player as long as it will help a\n",
      "Every effort moves you through many forms, but it is what we need of every creature in this world that ultimately reveals our worth. We feel pain\n",
      "Every effort moves you faster for less than is worthwhile but if you work towards perfection, success isn't about your ego-slovenience â€“\n",
      "Every effort moves you through that universe. What kind of relationship do you think that lasts with an emotion?\" If someone can take it seriously that means\n",
      "Every effort moves you forward, and your hope doesn't come with any loss when it does . It may be slow, especially for an athlete,\n",
      "Every effort moves you further away but to maintain a good equilibrium which continues to get stronger all while increasing both your own health and all the more so\n",
      "Every effort moves you towards better decisions. (I've been so worried I think I've made progress lately, even at our first meeting the hardest\n",
      "Every effort moves you towards learning how to run better. It gets us ready to start running for our life! Please continue reading!ðŸ¦ðŸ¦This\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    token_ids = generate(\n",
    "        model=gpt,\n",
    "        idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "        max_new_tokens=25,\n",
    "        context_size=NEW_CONFIG[\"context_length\"],\n",
    "        top_k=50,\n",
    "        temperature=1.5\n",
    "    )\n",
    "    print(token_ids_to_text(token_ids, tokenizer).replace('\\n', 'ðŸ¦'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03fae9",
   "metadata": {},
   "source": [
    "# **Prev Chapter STUFF âŒ**\n",
    "| ðŸ“Œ Not sure what but I'm `doing something wrong`, due to which correct `OpenAI` Pretrained weights are not loaded properly. |\n",
    "| --- |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff71cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\$LLMS\\LLMs-from-scratch\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "# Loading PreTrained GPT2 124M Model Weights\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "model_dir = '../../ch05/00_ME-Experimenting/gpt2/124M/'\n",
    "tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n",
    "params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "\n",
    "print(settings)\n",
    "print(params.keys()) # Model parameters loaded successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a0c33f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# (OLDER) GPT-2 small (124M) config\n",
    "GPT_CONFIG_124M = { \n",
    "    \"vocab_size\": 50257, \n",
    "    \"context_length\": 256, #1024, \n",
    "    \"emb_dim\": 768, \n",
    "    \"n_heads\": 12, \n",
    "    \"n_layers\": 4, #12,  # Transformer-Block-Layers\n",
    "    \"drop_rate\": 0.1, \n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.eps = 1e-5\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        '''x: 3D Tensor'''\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False) # unbiased=False => Division by `n`, rather than `n-1`\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return (x_norm * self.scale + self.shift)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config['emb_dim'], 4 * config['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear( 4 * config['emb_dim'], config['emb_dim'])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (0.5 * x * (1 + torch.tanh(\n",
    "            (torch.sqrt(torch.tensor(2/torch.pi))) + (x + 0.044715 * torch.pow(x, 3))\n",
    "        )))\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, n_heads, context_length, dropout=0.5, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % n_heads == 0)\n",
    "\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.d_head = (d_out // n_heads)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer('mask', torch.ones(context_length, context_length).triu(1).bool())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''x: 3D. x => (batch_size, num_tokens, token_embed)'''\n",
    "        b, n_tokens, token_embed = x.shape\n",
    "        assert self.d_in == token_embed\n",
    "        \n",
    "        Q = self.W_q(x) # (b, n_tokens, d_out)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        Q = Q.view(b, n_tokens, self.n_heads, self.d_head) # (b, n_tokens, n_heads, d_head)\n",
    "        K = K.view(b, n_tokens, self.n_heads, self.d_head) \n",
    "        V = V.view(b, n_tokens, self.n_heads, self.d_head) \n",
    "\n",
    "        Q = Q.transpose(1, 2) # (b, n_heads, n_tokens, d_head)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / self.d_head**0.5 #K.shape[-1]**0.5\n",
    "        attn_scores = attn_scores.masked_fill(self.mask[: n_tokens, : n_tokens], -torch.inf)\n",
    "        attn_weights = attn_scores.softmax(-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vectors = attn_weights @ V\n",
    "        context_vectors = context_vectors.transpose(1, 2)\n",
    "        context_vectors = context_vectors.contiguous().view(b, n_tokens, self.d_out)\n",
    "        return self.out_proj(context_vectors)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiheadAttention(\n",
    "            d_in=cfg['emb_dim'],    # 768\n",
    "            d_out=cfg['emb_dim'],   # 768\n",
    "            n_heads=cfg['n_heads'], # 12\n",
    "            context_length=cfg['context_length'], # 1024\n",
    "            dropout=cfg['drop_rate'], # 0.1\n",
    "            qkv_bias=cfg['qkv_bias']\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm_1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm_2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Part 1:\n",
    "        shortcut = x\n",
    "        x = self.norm_1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        # Part 2:\n",
    "        shortcut = x\n",
    "        x = self.norm_2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config['vocab_size'], config['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(config['context_length'], config['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(config['drop_rate'])\n",
    "        self.transf_layers = nn.Sequential(*[TransformerBlock(config) for _ in range(config['n_layers'])])\n",
    "        self.final_norm = LayerNorm(config['emb_dim'])\n",
    "        self.out_head = nn.Linear(config['emb_dim'], config['vocab_size'], bias=False)\n",
    "    \n",
    "    def forward(self, x, show_info=False):\n",
    "        '''x: 2D Matrix'''\n",
    "        batch_size, seq_len = x.shape \n",
    "        tok_emb = self.tok_emb(x) \n",
    "        pos_emb = self.pos_emb(\n",
    "            torch.arange(seq_len).to(x.device)  # Ensure pos indices are on the same device as x\n",
    "        )\n",
    "        x = tok_emb + pos_emb\n",
    "        if show_info:\n",
    "            print(f'Token-Embed(shape): {tok_emb.shape}')\n",
    "            print(f'POS-Embed(shape): {pos_emb.shape}')\n",
    "            print(f'i/p Before TransfBlocks(shape): {x.shape}')\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transf_layers(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4278b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 Model Configurations\n",
    "model_configs = { \n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12}, \n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16}, \n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20}, \n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# GPT2 124M Base Configuration:\n",
    "MODEL_CONFIG = {\n",
    "    'vocab_size': 50257, \n",
    "    'context_length': 256, \n",
    "    'emb_dim': 768, \n",
    "    'n_heads': 12, \n",
    "    'n_layers': 12, \n",
    "    'drop_rate': 0.1, \n",
    "    'qkv_bias': True, \n",
    "}\n",
    "\n",
    "MODEL_CONFIG.update({\n",
    "    'context_length': settings['n_ctx'],\n",
    "    'emb_dim': settings['n_embd'],\n",
    "})\n",
    "\n",
    "gpt = GPTModel(MODEL_CONFIG) # Uninitialized weights\n",
    "gpt.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea4b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading pretrained weights from TensorFlow checkpoint \n",
    "import numpy as np\n",
    "\n",
    "def assignParams(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {left.shape} vs {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assignParams(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assignParams(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.transf_layers[b].attn.W_q.weight = assignParams(\n",
    "            gpt.transf_layers[b].attn.W_q.weight, q_w.T)\n",
    "        gpt.transf_layers[b].attn.W_k.weight = assignParams(\n",
    "            gpt.transf_layers[b].attn.W_k.weight, k_w.T)\n",
    "        gpt.transf_layers[b].attn.W_v.weight = assignParams(\n",
    "            gpt.transf_layers[b].attn.W_v.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.transf_layers[b].attn.W_q.bias = assignParams(\n",
    "            gpt.transf_layers[b].attn.W_q.bias, q_b)\n",
    "        gpt.transf_layers[b].attn.W_k.bias = assignParams(\n",
    "            gpt.transf_layers[b].attn.W_k.bias, k_b)\n",
    "        gpt.transf_layers[b].attn.W_v.bias = assignParams(\n",
    "            gpt.transf_layers[b].attn.W_v.bias, v_b)\n",
    "\n",
    "        gpt.transf_layers[b].attn.out_proj.weight = assignParams(\n",
    "            gpt.transf_layers[b].attn.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.transf_layers[b].attn.out_proj.bias = assignParams(\n",
    "            gpt.transf_layers[b].attn.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.transf_layers[b].ff.layers[0].weight = assignParams(\n",
    "            gpt.transf_layers[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.transf_layers[b].ff.layers[0].bias = assignParams(\n",
    "            gpt.transf_layers[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.transf_layers[b].ff.layers[2].weight = assignParams(\n",
    "            gpt.transf_layers[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.transf_layers[b].ff.layers[2].bias = assignParams(\n",
    "            gpt.transf_layers[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.transf_layers[b].norm_1.scale = assignParams(\n",
    "            gpt.transf_layers[b].norm_1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.transf_layers[b].norm_1.shift = assignParams(\n",
    "            gpt.transf_layers[b].norm_1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.transf_layers[b].norm_2.scale = assignParams(\n",
    "            gpt.transf_layers[b].norm_2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.transf_layers[b].norm_2.shift = assignParams(\n",
    "            gpt.transf_layers[b].norm_2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assignParams(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assignParams(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assignParams(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f39b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Model:\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # New (not in book): numerical stability tip to get equivalent results on mps device\n",
    "            # subtract rowwise max before softmax\n",
    "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6a87c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "every effort.\"ðŸ¦ðŸ¦One recent analysis showed that the number of refugees fleeing the country's deadliest civil war could increase substantially if both Syria\n",
      "every effort,\" he said. Asked why the government tried trying to solve all this on Saturday, Barstow explained that while his job\n",
      "every effort. If some of it is done correctly (I hope that he's not!) I may even have given myself the time time\n",
      "every effort at preventing death,\" says Dr. Sirota. More research needs to be done over the long term, for instance where\n",
      "every effort is still under process, so the public's feedback cannot be verified and no guarantees to receive changes. For more information about how\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# token_ids = generate(\n",
    "#     model=gpt,\n",
    "#     idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "#     max_new_tokens=25,\n",
    "#     context_size=MODEL_CONFIG[\"context_length\"],\n",
    "#     top_k=50,\n",
    "#     temperature=1.5\n",
    "# )\n",
    "\n",
    "for i in range(5):\n",
    "    token_ids = generate(\n",
    "        model=gpt,\n",
    "        idx=text_to_token_ids(\"every effort\", tokenizer).to(device),\n",
    "        max_new_tokens=25,\n",
    "        context_size=MODEL_CONFIG[\"context_length\"],\n",
    "        top_k=50,\n",
    "        temperature=1.5\n",
    "    )\n",
    "    print(token_ids_to_text(token_ids, tokenizer).replace('\\n', 'ðŸ¦'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e897bd0",
   "metadata": {},
   "source": [
    "# **FineTurning For Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9657282",
   "metadata": {},
   "source": [
    "## **Dataset Download:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92ca34cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as sms_spam_collection\\SMSSpamCollection.tsv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport urllib.request\\nimport zipfile\\nimport os\\nfrom pathlib import Path\\n\\nurl = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\\nzip_path = \"sms_spam_collection.zip\"\\nextracted_path = \"sms_spam_collection\"\\ndata_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\\n\\ndef download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\\n    if data_file_path.exists():\\n        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\\n        return\\n\\n    # Downloading the file\\n    with urllib.request.urlopen(url) as response:\\n        with open(zip_path, \"wb\") as out_file:\\n            out_file.write(response.read())\\n\\n    # Unzipping the file\\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\\n        zip_ref.extractall(extracted_path)\\n\\n    # Add .tsv file extension\\n    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\\n    os.rename(original_file_path, data_file_path)\\n    print(f\"File downloaded and saved as {data_file_path}\")\\n\\ntry:\\n    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\\nexcept (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\\n    print(f\"Primary URL failed: {e}. Trying backup URL...\")\\n    url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\\n    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @rasbt: Downloading the SMS Spam Collection Dataset:->\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    # Downloading the file\n",
    "    response = requests.get(url, stream=True, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    with open(zip_path, \"wb\") as out_file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                out_file.write(chunk)\n",
    "\n",
    "    # Unzipping the file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # Add .tsv file extension\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "except (requests.exceptions.RequestException, TimeoutError) as e:\n",
    "    print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
    "    url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "\n",
    "\n",
    "\n",
    "# The book originally used the following code below\n",
    "# However, urllib uses older protocol settings that\n",
    "# can cause problems for some readers using a VPN.\n",
    "# The `requests` version above is more robust\n",
    "# in that regard.\n",
    "\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    # Downloading the file\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # Unzipping the file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # Add .tsv file extension\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "try:\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\n",
    "    print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
    "    url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "059cda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"label\", \"text\"])\n",
    "df.to_csv('sms_spam_collection/SMSSpamCollection.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "03c7539e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0662fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label'] == 'spam'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "27a737bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4788</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ãœ thk of wat to eat tonight.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4695</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pls give her the food preferably pap very slow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>ham</td>\n",
       "      <td>Thts god's gift for birds as humans hav some n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok but tell me half an hr b4 u come i need 2 p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lor. Anyway i thk we cant get tickets now c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>ham</td>\n",
       "      <td>Do you like shaking your booty on the dance fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh ! A half hour is much longer in Syria than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3857</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm ok. Will do my part tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4361</th>\n",
       "      <td>ham</td>\n",
       "      <td>Night sweet, sleep well! I've just been to see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4862</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nokia phone is lovly..</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>747 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "4788   ham                       Ãœ thk of wat to eat tonight.\n",
       "4695   ham  Pls give her the food preferably pap very slow...\n",
       "2925   ham  Thts god's gift for birds as humans hav some n...\n",
       "2751   ham  Ok but tell me half an hr b4 u come i need 2 p...\n",
       "3990   ham  Ok lor. Anyway i thk we cant get tickets now c...\n",
       "...    ...                                                ...\n",
       "2651   ham  Do you like shaking your booty on the dance fl...\n",
       "1335   ham  Oh ! A half hour is much longer in Syria than ...\n",
       "3857   ham                   I'm ok. Will do my part tomorrow\n",
       "4361   ham  Night sweet, sleep well! I've just been to see...\n",
       "4862   ham                             Nokia phone is lovly..\n",
       "\n",
       "[747 rows x 2 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = df[df['label'] == 'spam'].shape[0]\n",
    "df[df['label'] == 'ham'].sample(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "700241ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>ham</td>\n",
       "      <td>Watching cartoon, listening music &amp;amp; at eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5181</th>\n",
       "      <td>ham</td>\n",
       "      <td>K sure am in my relatives home. Sms me de. Pls:-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4884</th>\n",
       "      <td>ham</td>\n",
       "      <td>New Theory: Argument wins d SITUATION, but los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh did you charge camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok give me 5 minutes I think I see her. BTW yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>spam</td>\n",
       "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>spam</td>\n",
       "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "4869   ham  Watching cartoon, listening music &amp; at eve...\n",
       "5181   ham  K sure am in my relatives home. Sms me de. Pls:-)\n",
       "4884   ham  New Theory: Argument wins d SITUATION, but los...\n",
       "5167   ham                           Oh did you charge camera\n",
       "550    ham  Ok give me 5 minutes I think I see her. BTW yo...\n",
       "...    ...                                                ...\n",
       "5537  spam  Want explicit SEX in 30 secs? Ring 02073162414...\n",
       "5540  spam  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
       "5547  spam  Had your contract mobile 11 Mnths? Latest Moto...\n",
       "5566  spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "\n",
       "[1494 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a Balanced Dataset ||=>  \n",
    "# Got, [ham: 4825 & spam: 747]\n",
    "# So, a easier way to creaed a balanced dataset would be get 747 random `ham` samples.\n",
    "\n",
    "num_samples = df[df['label'] == 'spam'].shape[0]\n",
    "notSpamsamples = df[df['label'] == 'ham'].sample(num_samples)\n",
    "# notSpamsamples.shape # (747, 2)\n",
    "balanced_df = pd.concat([notSpamsamples, df[df['label'] == 'spam']])\n",
    "# balanced_df.shape # (1494, 2)\n",
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "53ebcdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>ham</td>\n",
       "      <td>Watching cartoon, listening music &amp;amp; at eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5181</th>\n",
       "      <td>ham</td>\n",
       "      <td>K sure am in my relatives home. Sms me de. Pls:-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4884</th>\n",
       "      <td>ham</td>\n",
       "      <td>New Theory: Argument wins d SITUATION, but los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh did you charge camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok give me 5 minutes I think I see her. BTW yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>spam</td>\n",
       "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>spam</td>\n",
       "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "4869   ham  Watching cartoon, listening music &amp; at eve...\n",
       "5181   ham  K sure am in my relatives home. Sms me de. Pls:-)\n",
       "4884   ham  New Theory: Argument wins d SITUATION, but los...\n",
       "5167   ham                           Oh did you charge camera\n",
       "550    ham  Ok give me 5 minutes I think I see her. BTW yo...\n",
       "...    ...                                                ...\n",
       "5537  spam  Want explicit SEX in 30 secs? Ring 02073162414...\n",
       "5540  spam  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
       "5547  spam  Had your contract mobile 11 Mnths? Latest Moto...\n",
       "5566  spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "\n",
       "[1494 rows x 2 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "eae766ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4828</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yup but it's not giving me problems now so may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5201</th>\n",
       "      <td>spam</td>\n",
       "      <td>YOU VE WON! Your 4* Costa Del Sol Holiday or Â£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yeah it's jus rite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>ham</td>\n",
       "      <td>Do you mind if I ask what happened? You dont h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>ham</td>\n",
       "      <td>What * u wearing?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>spam</td>\n",
       "      <td>HOT LIVE FANTASIES call now 08707509020 Just 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>ham</td>\n",
       "      <td>* Will have two more cartons off u and is very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>spam</td>\n",
       "      <td>18 days to Euro2004 kickoff! U will be kept in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3864</th>\n",
       "      <td>spam</td>\n",
       "      <td>Oh my god! I've found your number again! I'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>ham</td>\n",
       "      <td>Wat r u doing?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "4828   ham  Yup but it's not giving me problems now so may...\n",
       "5201  spam  YOU VE WON! Your 4* Costa Del Sol Holiday or Â£...\n",
       "5541   ham                              Yeah it's jus rite...\n",
       "647    ham  Do you mind if I ask what happened? You dont h...\n",
       "1139   ham                                  What * u wearing?\n",
       "...    ...                                                ...\n",
       "2992  spam  HOT LIVE FANTASIES call now 08707509020 Just 2...\n",
       "2218   ham  * Will have two more cartons off u and is very...\n",
       "1050  spam  18 days to Euro2004 kickoff! U will be kept in...\n",
       "3864  spam  Oh my god! I've found your number again! I'm s...\n",
       "1558   ham                                     Wat r u doing?\n",
       "\n",
       "[1494 rows x 2 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = balanced_df.sample(len(balanced_df))\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cc941f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4828</th>\n",
       "      <td>1</td>\n",
       "      <td>Yup but it's not giving me problems now so may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5201</th>\n",
       "      <td>0</td>\n",
       "      <td>YOU VE WON! Your 4* Costa Del Sol Holiday or Â£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>1</td>\n",
       "      <td>Yeah it's jus rite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>1</td>\n",
       "      <td>Do you mind if I ask what happened? You dont h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>1</td>\n",
       "      <td>What * u wearing?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>0</td>\n",
       "      <td>HOT LIVE FANTASIES call now 08707509020 Just 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>1</td>\n",
       "      <td>* Will have two more cartons off u and is very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>0</td>\n",
       "      <td>18 days to Euro2004 kickoff! U will be kept in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3864</th>\n",
       "      <td>0</td>\n",
       "      <td>Oh my god! I've found your number again! I'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>1</td>\n",
       "      <td>Wat r u doing?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "4828      1  Yup but it's not giving me problems now so may...\n",
       "5201      0  YOU VE WON! Your 4* Costa Del Sol Holiday or Â£...\n",
       "5541      1                              Yeah it's jus rite...\n",
       "647       1  Do you mind if I ask what happened? You dont h...\n",
       "1139      1                                  What * u wearing?\n",
       "...     ...                                                ...\n",
       "2992      0  HOT LIVE FANTASIES call now 08707509020 Just 2...\n",
       "2218      1  * Will have two more cartons off u and is very...\n",
       "1050      0  18 days to Euro2004 kickoff! U will be kept in...\n",
       "3864      0  Oh my god! I've found your number again! I'm s...\n",
       "1558      1                                     Wat r u doing?\n",
       "\n",
       "[1494 rows x 2 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing: [ham: 1] ; [spam: 0]\n",
    "dataset_df['label'] = dataset_df['label'].map({\n",
    "    'ham': 1,\n",
    "    'spam': 0\n",
    "})\n",
    "\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "14cb3855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Val, Test Subsets:\n",
    "split_ratio = 0.7\n",
    "split = int(split_ratio * len(dataset_df))\n",
    "\n",
    "train_df = dataset_df[: split]\n",
    "val_test_df = dataset_df[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ed182260",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.7\n",
    "split = int(split_ratio * len(val_test_df))\n",
    "val_df = val_test_df[:split]\n",
    "test_df = val_test_df[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "241056e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1045\n",
      "314\n",
      "135\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df)) \n",
    "print(len(val_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "da6855e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4828</th>\n",
       "      <td>1</td>\n",
       "      <td>Yup but it's not giving me problems now so may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5201</th>\n",
       "      <td>0</td>\n",
       "      <td>YOU VE WON! Your 4* Costa Del Sol Holiday or Â£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>1</td>\n",
       "      <td>Yeah it's jus rite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>1</td>\n",
       "      <td>Do you mind if I ask what happened? You dont h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>1</td>\n",
       "      <td>What * u wearing?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>0</td>\n",
       "      <td>FREE camera phones with linerental from 4.49/m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>1</td>\n",
       "      <td>He says he'll give me a call when his friend's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>0</td>\n",
       "      <td>449050000301 You have won a Â£2,000 price! To c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>1</td>\n",
       "      <td>Iâ€˜m parked next to a MINI!!!! When are you com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4738</th>\n",
       "      <td>1</td>\n",
       "      <td>Nt only for driving even for many reasons she ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1045 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "4828      1  Yup but it's not giving me problems now so may...\n",
       "5201      0  YOU VE WON! Your 4* Costa Del Sol Holiday or Â£...\n",
       "5541      1                              Yeah it's jus rite...\n",
       "647       1  Do you mind if I ask what happened? You dont h...\n",
       "1139      1                                  What * u wearing?\n",
       "...     ...                                                ...\n",
       "2189      0  FREE camera phones with linerental from 4.49/m...\n",
       "313       1  He says he'll give me a call when his friend's...\n",
       "1118      0  449050000301 You have won a Â£2,000 price! To c...\n",
       "260       1  Iâ€˜m parked next to a MINI!!!! When are you com...\n",
       "4738      1  Nt only for driving even for many reasons she ...\n",
       "\n",
       "[1045 rows x 2 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c7fe82c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Trainset, Validset, Testset:\n",
    "train_df.to_csv('my-dataset/train.csv', index=False)\n",
    "val_df.to_csv('my-dataset/val.csv', index=False)\n",
    "test_df.to_csv('my-dataset/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5d6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3567f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, pad_id=50256, max_Length=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.encoding = [tokenizer.encode(text) for text in self.df['text']]\n",
    "        if max_Length is None:\n",
    "            max_Length = self._getMaxlength()\n",
    "\n",
    "            for i, encoding in enumerate(self.encoding):\n",
    "                # Padding:\n",
    "                self.encoding[i] += [pad_id] * (max_Length - len(encoding))\n",
    "        else:\n",
    "            for i, encoding in enumerate(self.encoding):\n",
    "                self.encoding[i] = encoding[: max_Length]\n",
    "                # encoding = encoding[: max_Length]\n",
    "            \n",
    "            # Padding:\n",
    "            for i, encoding in enumerate(self.encoding):\n",
    "                self.encoding[i] += [pad_id] * (max_Length - len(encoding))\n",
    "\n",
    "        self.max_length = max_Length;\n",
    "        self.encoding = torch.tensor(self.encoding, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoding)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.encoding[index] \n",
    "        y = torch.tensor(self.df.iloc[index]['label'], dtype=torch.long)\n",
    "        return (x, y)\n",
    "\n",
    "    def _getMaxlength(self):\n",
    "        max_Length = 0\n",
    "        for encoding in self.encoding:\n",
    "            if (max_Length < len(encoding)):\n",
    "                max_Length = len(encoding)\n",
    "        return max_Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a03f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = 'my-dataset/train.csv'\n",
    "val_csv_path = 'my-dataset/val.csv'\n",
    "test_csv_path = 'my-dataset/test.csv'\n",
    "\n",
    "train_dataset = SpamDataset(train_csv_path, tokenizer, max_Length=120)\n",
    "val_dataset = SpamDataset(val_csv_path, tokenizer, max_Length=120)\n",
    "test_dataset = SpamDataset(test_csv_path, tokenizer, max_Length=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f55639bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True, \n",
    "                              num_workers=0,\n",
    "                              drop_last=True)\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True, \n",
    "                            num_workers=0,\n",
    "                            drop_last=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=True, \n",
    "                             num_workers=0,\n",
    "                             drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5daf6b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_dataloader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b586573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Val loader:\")\n",
    "for input_batch, target_batch in val_dataloader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ecef88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"test loader:\")\n",
    "for input_batch, target_batch in test_dataloader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9ed15ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "39\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cfb903",
   "metadata": {},
   "source": [
    "## **Adding `Classification Head` To The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "769cafcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "gpt.to('cpu')\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=NEW_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77ebb96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=50257, bias=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.out_head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a3abcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing [ENTIRE] model layers:\n",
    "for params in gpt.parameters():\n",
    "    params.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe9cd0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=50257, bias=False)\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(gpt.out_head)\n",
    "print(NEW_CONFIG['emb_dim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53fb1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the output layer with `out_features=2`\n",
    "num_classes = 2 # [Spam, NotSpam]\n",
    "gpt.out_head = nn.Linear(in_features=NEW_CONFIG['emb_dim'],\n",
    "                         out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec706e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.out_head.weight.requires_grad # Gradient ON: For this last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f89026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Making the Last Transformer Layer & the last norm layer [Trainable]\n",
    "# \"As it improves model's performance.\" -> @rasbt\n",
    "for params in gpt.trf_blocks[-1].parameters():\n",
    "    params.requires_grad = True\n",
    "for params in gpt.final_norm.parameters():\n",
    "    params.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f0e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0238, -1.5068],\n",
       "         [-8.6732, -3.5773],\n",
       "         [-5.7426, -2.7576],\n",
       "         [-2.7751, -3.6379]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the updated model:\n",
    "input = torch.tensor([[5211, 345, 423, 640]])\n",
    "with torch.no_grad():\n",
    "    output = gpt(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca096c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.7751, -3.6379])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][-1] # We are concerned with the Last output (Spam / Not Spam)\n",
    "# Earlier the last output represents the probability distribution for the next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9cf50ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7751, -3.6379]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a92aede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
