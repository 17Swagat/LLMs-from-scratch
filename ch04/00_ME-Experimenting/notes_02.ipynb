{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04677f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c94b7",
   "metadata": {},
   "source": [
    "## **Basic Idea for normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3101d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5235],\n",
      "         [0.5720],\n",
      "         [0.7532],\n",
      "         [0.5854]],\n",
      "\n",
      "        [[0.3447],\n",
      "         [0.6899],\n",
      "         [0.3227],\n",
      "         [0.4088]]]) \n",
      "\n",
      "tensor([[[0.0785],\n",
      "         [0.0656],\n",
      "         [0.0765],\n",
      "         [0.0984]],\n",
      "\n",
      "        [[0.0488],\n",
      "         [0.0514],\n",
      "         [0.0227],\n",
      "         [0.0591]]]) \n",
      "\n",
      "tensor([[[0.2801],\n",
      "         [0.2562],\n",
      "         [0.2767],\n",
      "         [0.3137]],\n",
      "\n",
      "        [[0.2209],\n",
      "         [0.2268],\n",
      "         [0.1505],\n",
      "         [0.2432]]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((2, 4, 5))\n",
    "mean = x.mean(dim=-1, keepdim=True)\n",
    "var = x.var(dim=-1, keepdim=True)\n",
    "std = x.std(dim=-1, keepdim=True)\n",
    "\n",
    "print(mean,'\\n')\n",
    "print(var, '\\n')\n",
    "print(std, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9feee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[-0.0000],\n",
      "         [-0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]]])\n",
      "tensor([[[1.0000],\n",
      "         [1.0000],\n",
      "         [1.0000],\n",
      "         [1.0000]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [1.0000],\n",
      "         [1.0000],\n",
      "         [1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((2, 4, 5))\n",
    "mean = x.mean(dim=-1, keepdim=True)\n",
    "var = x.var(dim=-1, keepdim=True)\n",
    "std = x.std(dim=-1, keepdim=True)\n",
    "\n",
    "x_norm = (x - mean) / std\n",
    "print(x_norm.mean(-1, keepdim=True))\n",
    "print(x_norm.var(-1, keepdim=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e48b87",
   "metadata": {},
   "source": [
    "## **LayerNorm Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88eaf0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.eps = 1e-5\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        '''x: 3D Tensor'''\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False) # unbiased=False => Division by `n`, rather than `n-1`\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        # x_norm = (x - mean) / torch.sqrt(var)\n",
    "        # print(x_norm.)\n",
    "        print(x_norm.mean(-1, keepdims=True))\n",
    "        print(x_norm.var(-1, keepdims=True))\n",
    "        return (x_norm * self.scale + self.shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "810c295a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0000],\n",
      "        [-0.0000]])\n",
      "tensor([[1.0000],\n",
      "        [1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# IN action\n",
    "x = torch.randn(2, 129750)\n",
    "ln = LayerNorm(emb_dim=129750)\n",
    "out_norm = ln(x)\n",
    "# print(out_norm.mean(-1, keepdims=True))\n",
    "# print(out_norm.var(-1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b26869f",
   "metadata": {},
   "source": [
    "# **`GELU()` Activation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08efa96d",
   "metadata": {},
   "source": [
    "<img src = \"./gelu.png\" width = \"900\" height = \"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf35f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (0.5 * x * (1 + torch.tanh(\n",
    "            (torch.sqrt(torch.tensor(2/torch.pi))) + (x + 0.044715 * torch.pow(x, 3))\n",
    "        )))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee53c424",
   "metadata": {},
   "source": [
    "# **`FeedForward` Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e12be5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = { \n",
    "    \"vocab_size\": 50257, \n",
    "    \"context_length\": 1024, \n",
    "    \"emb_dim\": 768, \n",
    "    \"n_heads\": 12, \n",
    "    \"n_layers\": 12,  # Transformer-Block-Layers\n",
    "    \"drop_rate\": 0.1, \n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ebb5ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config['emb_dim'], 4 * config['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear( 4 * config['emb_dim'], config['emb_dim'])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fda4220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def09970",
   "metadata": {},
   "source": [
    "# **Skip-Connections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6c81cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2961],\n",
       "         [-0.2931],\n",
       "         [-0.2961],\n",
       "         [-0.3034]],\n",
       "\n",
       "        [[-0.2991],\n",
       "         [-0.2960],\n",
       "         [-0.2962],\n",
       "         [-0.3004]],\n",
       "\n",
       "        [[-0.2982],\n",
       "         [-0.3028],\n",
       "         [-0.2986],\n",
       "         [-0.2958]],\n",
       "\n",
       "        [[-0.3007],\n",
       "         [-0.3000],\n",
       "         [-0.2984],\n",
       "         [-0.2957]],\n",
       "\n",
       "        [[-0.2989],\n",
       "         [-0.2999],\n",
       "         [-0.2965],\n",
       "         [-0.2982]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Example_ForSkipConnections(nn.Module):\n",
    "    def __init__(self, skip, layers):\n",
    "        super().__init__()\n",
    "        self.use_skipconnect = skip\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layers[0], layers[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layers[1], layers[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layers[2], layers[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layers[3], layers[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layers[4], layers[5]), GELU()),\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer_output = layer(x)\n",
    "            if self.use_skipconnect and x.shape == layer_output.shape:\n",
    "                x += layer_output\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "            \n",
    "\n",
    "layers = [5, 5, 4, 3, 3, 1]\n",
    "x = torch.rand((5, 4, 5))\n",
    "model = Example_ForSkipConnections(True, layers)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85494d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([-1., 0., 1.])\n",
    "model_withSkipConnections = Example_ForSkipConnections(skip=True, layers=[3, 3, 3, 3, 3, 1])\n",
    "model_withoutSkipConnections = Example_ForSkipConnections(skip=False, layers=[3, 3, 3, 3, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ded96d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradient(model, x):\n",
    "    out = model(x)\n",
    "    target = torch.tensor([0.])\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "\n",
    "    for name, params in model.named_parameters():\n",
    "        params: torch.Tensor\n",
    "        \n",
    "        print(f'{name} -> {params.grad.abs().mean().item()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bfe2eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight -> 0.0012244501849636436\n",
      "\n",
      "layers.0.0.bias -> 0.0018366752192378044\n",
      "\n",
      "layers.1.0.weight -> 0.0074272919446229935\n",
      "\n",
      "layers.1.0.bias -> 0.01643095351755619\n",
      "\n",
      "layers.2.0.weight -> 0.06399044394493103\n",
      "\n",
      "layers.2.0.bias -> 0.18550486862659454\n",
      "\n",
      "layers.3.0.weight -> 0.12358677387237549\n",
      "\n",
      "layers.3.0.bias -> 0.40518125891685486\n",
      "\n",
      "layers.4.0.weight -> 0.4489266872406006\n",
      "\n",
      "layers.4.0.bias -> 1.2557212114334106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_gradient(model_withoutSkipConnections, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48061758",
   "metadata": {},
   "source": [
    "# **Transformer Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb03ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02778d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config['emb_dim'], 4 * config['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear( 4 * config['emb_dim'], config['emb_dim'])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51d2d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.eps = 1e-5\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        '''x: 3D Tensor'''\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False) # unbiased=False => Division by `n`, rather than `n-1`\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return (x_norm * self.scale + self.shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c354cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, n_heads, context_length, dropout=0.5, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % n_heads == 0)\n",
    "\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.d_head = (d_out // n_heads)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer('mask', torch.ones(context_length, context_length).triu(1).bool())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''x: 3D. x => (batch_size, num_tokens, token_embed)'''\n",
    "        b, n_tokens, token_embed = x.shape\n",
    "        assert self.d_in == token_embed\n",
    "        \n",
    "        Q = self.W_q(x) # (b, n_tokens, d_out)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        Q = Q.view(b, n_tokens, self.n_heads, self.d_head) # (b, n_tokens, n_heads, d_head)\n",
    "        K = K.view(b, n_tokens, self.n_heads, self.d_head) \n",
    "        V = V.view(b, n_tokens, self.n_heads, self.d_head) \n",
    "\n",
    "        Q = Q.transpose(1, 2) # (b, n_heads, n_tokens, d_head)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / self.d_head**0.5 #K.shape[-1]**0.5\n",
    "        attn_scores = attn_scores.masked_fill(self.mask[: n_tokens, : n_tokens], -torch.inf)\n",
    "        attn_weights = attn_scores.softmax(-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vectors = attn_weights @ V\n",
    "        context_vectors = context_vectors.transpose(1, 2)\n",
    "        context_vectors = context_vectors.contiguous().view(b, n_tokens, self.d_out)\n",
    "        return self.out_proj(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05d71900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context_length': 1024,\n",
       " 'emb_dim': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'drop_rate': 0.1,\n",
       " 'qkv_bias': False}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cccb4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiheadAttention(\n",
    "            d_in=cfg['emb_dim'],    # 768\n",
    "            d_out=cfg['emb_dim'],   # 768\n",
    "            n_heads=cfg['n_heads'], # 12\n",
    "            context_length=cfg['context_length'], # 1024\n",
    "            dropout=cfg['drop_rate'], # 0.1\n",
    "            qkv_bias=cfg['qkv_bias']\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm_1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm_2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Part 1:\n",
    "        shortcut = x\n",
    "        x = self.norm_1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        # Part 2:\n",
    "        shortcut = x\n",
    "        x = self.norm_2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dd18d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3881,  0.3762,  1.4581,  ...,  0.7932, -1.2334, -0.2798],\n",
       "         [ 0.6409, -0.6456,  0.6692,  ..., -0.0151, -0.4119, -0.1861],\n",
       "         [ 0.3886, -0.2022,  0.5368,  ..., -0.2549,  0.8459,  0.9049],\n",
       "         [ 0.4216, -0.6736,  0.7514,  ...,  0.1523,  0.1979,  0.4250]],\n",
       "\n",
       "        [[ 1.1130,  0.3264,  0.5296,  ...,  0.9354,  0.2601,  0.7476],\n",
       "         [ 0.3056,  1.0750,  0.5819,  ...,  0.2299, -0.1598, -0.1544],\n",
       "         [ 0.6481,  0.4353,  0.4790,  ...,  0.1865,  0.0681, -0.1181],\n",
       "         [ 0.8531,  0.6861, -0.3147,  ...,  0.2521,  0.5660,  0.6883]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f00cd0",
   "metadata": {},
   "source": [
    "# **GPT Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "378dd4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context_length': 1024,\n",
       " 'emb_dim': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'drop_rate': 0.1,\n",
       " 'qkv_bias': False}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f0102c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config['vocab_size'], config['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(config['context_length'], config['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(config['drop_rate'])\n",
    "        self.transf_layers = nn.Sequential(*[TransformerBlock(config) for _ in range(config['n_layers'])])\n",
    "        self.final_norm = LayerNorm(config['emb_dim'])\n",
    "        self.out_head = nn.Linear(config['emb_dim'], config['vocab_size'], bias=False)\n",
    "    \n",
    "    def forward(self, x, show_info=False):\n",
    "        '''x: 2D Matrix'''\n",
    "        batch_size, seq_len = x.shape \n",
    "        tok_emb = self.tok_emb(x) \n",
    "        pos_emb = self.pos_emb(torch.arange(seq_len))\n",
    "        x = tok_emb + pos_emb\n",
    "        if show_info:\n",
    "            print(f'Token-Embed(shape): {tok_emb.shape}')\n",
    "            print(f'POS-Embed(shape): {pos_emb.shape}')\n",
    "            print(f'i/p Before TransfBlocks(shape): {x.shape}')\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transf_layers(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb594087",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_2 = {\n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 1, #12,\n",
    "    'drop_rate': 0.1,\n",
    "    'qkv_bias': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d63ecde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4919, 389, 345, 30]\n",
      "[22308, 318, 4171, 13]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "txt1 = 'how are you?'\n",
    "txt2 = 'Sky is blue.'\n",
    "encoding_1 = tokenizer.encode(txt1)\n",
    "encoding_2 = tokenizer.encode(txt2)\n",
    "\n",
    "print(encoding_1)\n",
    "print(encoding_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93b27513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you?\n",
      "Sky is blue.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(encoding_1))\n",
    "print(tokenizer.decode(encoding_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0eb1343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4919,   389,   345,    30],\n",
       "        [22308,   318,  4171,    13]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.tensor([encoding_1, encoding_2])\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c2ff4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0539, -1.0485, -0.4322,  ...,  0.5333, -0.4937, -1.1792],\n",
       "         [-0.5220, -0.0211,  0.6120,  ...,  0.7940, -0.3497, -0.0212],\n",
       "         [ 0.2401,  0.0296,  0.1745,  ...,  0.6775,  0.6055,  0.2999],\n",
       "         [ 0.8294,  0.7989, -0.1112,  ..., -1.4778,  0.3479, -0.2939]],\n",
       "\n",
       "        [[ 0.5718, -0.9469,  0.4932,  ...,  1.3646,  0.4097, -0.3681],\n",
       "         [ 0.3502,  0.1984,  0.6038,  ...,  0.5483,  0.1599, -0.1177],\n",
       "         [ 0.3745,  0.2255,  0.0441,  ..., -0.4783,  0.6850, -0.5604],\n",
       "         [ 0.3580,  1.1778, -0.3584,  ..., -0.5991,  0.3171,  0.2838]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_2)\n",
    "y = model(batch)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a916894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 50257])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "258a5f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ROM'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_1_next_token \n",
    "nxt_token = torch.argmax(y[0][-1].softmax(-1))\n",
    "tokenizer.decode([nxt_token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d836ce",
   "metadata": {},
   "source": [
    "# **`Experiment`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6467682",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6895d168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([4])\n",
      "tensor([3, 4])\n",
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(0, 5)\n",
    "print(t)\n",
    "print(t[-1:])\n",
    "print(t[-2:])\n",
    "print(t[-10:]) # everything..\n",
    "# Assuming (10) is the context_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2453613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,   314,   716,   220]])\n"
     ]
    }
   ],
   "source": [
    "txt = \"Hello I am \"\n",
    "encoding = torch.tensor(tokenizer.encode(txt))\n",
    "encoding.unsqueeze_(0)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "899c5cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cat(torch.tensor(([1, 2, 3]), torch.tensor([-5])))\n",
    "x1 = torch.tensor([1, 2])\n",
    "x2 = torch.tensor([3, 4])\n",
    "# torch.cat((x1, x2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b5a34f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,   314,   716,   220]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding[1, 0] = -10\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a6ae25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = encoding.argmax(-1)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33ab0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding[torch.arange(encoding.shape[0]), idx].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70568dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,   314,   716,   220]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e136bff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1659,  0.9068,  0.4957,  ..., -0.3982, -0.5369, -0.9261],\n",
       "         [-0.4490, -0.4262, -0.8508,  ..., -0.7754, -0.4209, -1.2455],\n",
       "         [-0.6343,  0.9729,  0.6555,  ..., -1.6129,  0.4038, -0.2435],\n",
       "         [-1.1104, -0.8799,  0.7321,  ..., -0.6918,  0.1560, -0.2655]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[15496,   314,   716,   220]])\n",
    "model(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74f3275f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I am mers\n",
      "Hello I am mers fastball\n",
      "Hello I am mers fastball Ud\n",
      "Hello I am mers fastball Ud measurable\n",
      "Hello I am mers fastball Ud measurable fascism\n",
      "Hello I am mers fastball Ud measurable fascism GENERAL\n",
      "Hello I am mers fastball Ud measurable fascism GENERAL feasible\n",
      "Hello I am mers fastball Ud measurable fascism GENERAL feasible camer\n",
      "Hello I am mers fastball Ud measurable fascism GENERAL feasible camer Rw\n",
      "Hello I am mers fastball Ud measurable fascism GENERAL feasible camer Rw phylogen\n",
      "Hello I am mers fastball Ud measurable fascism GENERAL feasible camer Rw phylogen detonated\n",
      "Hello I am mers fastball Ud measurable fascism GENERAL feasible camer Rw phylogen detonated slightest\n",
      "Hello I am mers fastball Ud measurable fascism GENERAL feasible camer Rw phylogen detonated slightest Nicola\n",
      "Hello I am mers fastball Ud measurable fascism GENERAL feasible camer Rw phylogen detonated slightest Nicola374\n",
      "Hello I am mers fastball Ud measurable fascism GENERAL feasible camer Rw phylogen detonated slightest Nicola374 Disclosure\n"
     ]
    }
   ],
   "source": [
    "txt1 = \"Hello I am \"\n",
    "# txt2 = \"HI world shit?\"\n",
    "encoding1 = tokenizer.encode(txt1)\n",
    "# encoding2 = tokenizer.encode(txt2)\n",
    "encoding = torch.tensor([\n",
    "    encoding1, \n",
    "    # encoding2\n",
    "])\n",
    "# encoding.unsqueeze_(0)\n",
    "\n",
    "max_output_token = 15\n",
    "for i in range(max_output_token):\n",
    "    with torch.no_grad():\n",
    "        out = model(encoding)\n",
    "    logits = out[:, -1, :] \n",
    "    probs = logits.softmax(-1)\n",
    "    idx = probs.argmax(dim=-1) # idx of the token with highest prob. [NOTE: This `idx` acts as the tokenId]\n",
    "    idx.unsqueeze_(0)\n",
    "    encoding =  torch.cat((encoding.squeeze(0), idx.squeeze(0)))\n",
    "    print(tokenizer.decode(encoding.numpy()))\n",
    "    encoding.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4db1df37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I am  Hosp bonuses notoriety pulls priv monster instances camer thedefault chunk held cru Ten compelled\n"
     ]
    }
   ],
   "source": [
    "# Outputing the Final Sentence:\n",
    "\n",
    "txt1 = \"Hello I am \"\n",
    "# txt2 = \"HI world shit?\"\n",
    "encoding1 = tokenizer.encode(txt1)\n",
    "# encoding2 = tokenizer.encode(txt2)\n",
    "encoding = torch.tensor([\n",
    "    encoding1, \n",
    "    # encoding2\n",
    "])\n",
    "# encoding.unsqueeze_(0)\n",
    "\n",
    "max_output_token = 15\n",
    "for i in range(max_output_token):\n",
    "    with torch.no_grad():\n",
    "        out = model(encoding)\n",
    "    logits = out[:, -1, :] \n",
    "    probs = logits.softmax(-1)\n",
    "    idx = probs.argmax(dim=-1) # idx of the token with highest prob. [NOTE: This `idx` acts as the tokenId]\n",
    "    idx.unsqueeze_(0)\n",
    "    encoding =  torch.cat((encoding.squeeze(0), idx.squeeze(0)))\n",
    "\n",
    "    if (i == max_output_token - 1):\n",
    "        print(tokenizer.decode(encoding.numpy()))\n",
    "    encoding.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94506314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello man how are you doing pinWARE'), stat Europa tells Systems Keystone Fernando graduates\n"
     ]
    }
   ],
   "source": [
    "# Putting Text-Generation in a function\n",
    "def generate_text_simple(model, tokenizerGpt2:tiktoken.Encoding, txt, max_output_token):\n",
    "    encoding = torch.tensor(tokenizerGpt2.encode(txt))\n",
    "    encoding.unsqueeze_(0)\n",
    "    for i in range(max_output_token):\n",
    "        with torch.no_grad():\n",
    "            out = model(encoding)\n",
    "        logits = out[:, -1, :] \n",
    "        probs = logits.softmax(-1)\n",
    "        idx = probs.argmax(dim=-1) # idx of the token with highest prob. [NOTE: This `idx` acts as the tokenId]\n",
    "        idx.unsqueeze_(0)\n",
    "        encoding =  torch.cat((encoding.squeeze(0), idx.squeeze(0)))\n",
    "\n",
    "        if (i == max_output_token - 1):\n",
    "            print(tokenizer.decode(encoding.numpy()))\n",
    "        encoding.unsqueeze_(0)\n",
    "\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_2)\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "txt = 'hello man how are you doing'\n",
    "model.eval()\n",
    "generate_text_simple(model, tokenizer, txt, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202b6472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
