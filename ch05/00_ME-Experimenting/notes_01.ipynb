{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e6bea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "\n",
    "GPT_CONFIG_124M = { \n",
    "    \"vocab_size\": 50257, \n",
    "    \"context_length\": 256, #1024, \n",
    "    \"emb_dim\": 768, \n",
    "    \"n_heads\": 12, \n",
    "    \"n_layers\": 12,  # Transformer-Block-Layers\n",
    "    \"drop_rate\": 0.1, \n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.eps = 1e-5\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        '''x: 3D Tensor'''\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False) # unbiased=False => Division by `n`, rather than `n-1`\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return (x_norm * self.scale + self.shift)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config['emb_dim'], 4 * config['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear( 4 * config['emb_dim'], config['emb_dim'])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (0.5 * x * (1 + torch.tanh(\n",
    "            (torch.sqrt(torch.tensor(2/torch.pi))) + (x + 0.044715 * torch.pow(x, 3))\n",
    "        )))\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, n_heads, context_length, dropout=0.5, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % n_heads == 0)\n",
    "\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.d_head = (d_out // n_heads)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer('mask', torch.ones(context_length, context_length).triu(1).bool())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''x: 3D. x => (batch_size, num_tokens, token_embed)'''\n",
    "        b, n_tokens, token_embed = x.shape\n",
    "        assert self.d_in == token_embed\n",
    "        \n",
    "        Q = self.W_q(x) # (b, n_tokens, d_out)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        Q = Q.view(b, n_tokens, self.n_heads, self.d_head) # (b, n_tokens, n_heads, d_head)\n",
    "        K = K.view(b, n_tokens, self.n_heads, self.d_head) \n",
    "        V = V.view(b, n_tokens, self.n_heads, self.d_head) \n",
    "\n",
    "        Q = Q.transpose(1, 2) # (b, n_heads, n_tokens, d_head)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / self.d_head**0.5 #K.shape[-1]**0.5\n",
    "        attn_scores = attn_scores.masked_fill(self.mask[: n_tokens, : n_tokens], -torch.inf)\n",
    "        attn_weights = attn_scores.softmax(-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vectors = attn_weights @ V\n",
    "        context_vectors = context_vectors.transpose(1, 2)\n",
    "        context_vectors = context_vectors.contiguous().view(b, n_tokens, self.d_out)\n",
    "        return self.out_proj(context_vectors)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiheadAttention(\n",
    "            d_in=cfg['emb_dim'],    # 768\n",
    "            d_out=cfg['emb_dim'],   # 768\n",
    "            n_heads=cfg['n_heads'], # 12\n",
    "            context_length=cfg['context_length'], # 1024\n",
    "            dropout=cfg['drop_rate'], # 0.1\n",
    "            qkv_bias=cfg['qkv_bias']\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm_1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm_2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Part 1:\n",
    "        shortcut = x\n",
    "        x = self.norm_1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        # Part 2:\n",
    "        shortcut = x\n",
    "        x = self.norm_2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config['vocab_size'], config['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(config['context_length'], config['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(config['drop_rate'])\n",
    "        self.transf_layers = nn.Sequential(*[TransformerBlock(config) for _ in range(config['n_layers'])])\n",
    "        self.final_norm = LayerNorm(config['emb_dim'])\n",
    "        self.out_head = nn.Linear(config['emb_dim'], config['vocab_size'], bias=False)\n",
    "    \n",
    "    def forward(self, x, show_info=False):\n",
    "        '''x: 2D Matrix'''\n",
    "        batch_size, seq_len = x.shape \n",
    "        tok_emb = self.tok_emb(x) \n",
    "        pos_emb = self.pos_emb(torch.arange(seq_len))\n",
    "        x = tok_emb + pos_emb\n",
    "        if show_info:\n",
    "            print(f'Token-Embed(shape): {tok_emb.shape}')\n",
    "            print(f'POS-Embed(shape): {pos_emb.shape}')\n",
    "            print(f'i/p Before TransfBlocks(shape): {x.shape}')\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transf_layers(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa705659",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba22833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, tokenizerGpt2:tiktoken.Encoding, txt, max_output_token):\n",
    "    encoding = torch.tensor(tokenizerGpt2.encode(txt))\n",
    "    encoding.unsqueeze_(0)\n",
    "    for i in range(max_output_token):\n",
    "        with torch.no_grad():\n",
    "            out = model(encoding)\n",
    "        logits = out[:, -1, :] \n",
    "        probs = logits.softmax(-1)\n",
    "        idx = probs.argmax(dim=-1) # idx of the token with highest prob. [NOTE: This `idx` acts as the tokenId]\n",
    "        idx.unsqueeze_(0)\n",
    "        encoding =  torch.cat((encoding.squeeze(0), idx.squeeze(0)))\n",
    "\n",
    "        if (i == max_output_token - 1):\n",
    "            print(tokenizer.decode(encoding.numpy()))\n",
    "        encoding.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72ef0256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello from the other side reproduce Actress govern PROG SergeantHAEL Kw BulgariaSetup rate\n"
     ]
    }
   ],
   "source": [
    "generate_text_simple(model, tokenizer, \"hello from the other side\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd85c6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16833,  3626,  6100],\n",
       "        [   40,  1107,   588]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "txt1 = \"every effort moves\"\n",
    "txt2 = \"I really like\"\n",
    "input = torch.stack(\n",
    "    (torch.tensor(tokenizer.encode(txt1)), \n",
    "    torch.tensor(tokenizer.encode(txt2)))\n",
    ")\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40659dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6.6890e-05, 2.0877e-05, 5.4307e-06,  ..., 9.2617e-06,\n",
      "          3.2934e-05, 1.2245e-05],\n",
      "         [2.8620e-05, 1.3605e-05, 6.1841e-06,  ..., 9.4409e-06,\n",
      "          1.4269e-05, 4.5582e-05],\n",
      "         [1.3782e-05, 1.0714e-05, 2.4772e-05,  ..., 1.7069e-05,\n",
      "          5.8540e-06, 4.0807e-05]],\n",
      "\n",
      "        [[1.6585e-05, 3.2164e-05, 1.7628e-05,  ..., 2.9909e-05,\n",
      "          1.7773e-05, 3.5851e-05],\n",
      "         [1.4002e-05, 1.8186e-05, 7.5528e-06,  ..., 1.5048e-05,\n",
      "          2.6338e-05, 6.7893e-05],\n",
      "         [4.9658e-05, 6.5607e-06, 1.4103e-05,  ..., 7.7584e-06,\n",
      "          7.9725e-06, 5.5176e-06]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(input)\n",
    "probs = logits.softmax(-1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "270a0875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[34873],\n",
       "         [44449],\n",
       "         [40115]],\n",
       "\n",
       "        [[18938],\n",
       "         [31548],\n",
       "         [20399]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preditedTokenIDs = probs.argmax(-1, keepdim=True)\n",
    "preditedTokenIDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "793ac4f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'targets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text_idx = \u001b[32m0\u001b[39m \n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m target_probas_1 = probs[text_idx, [\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m], \u001b[43mtargets\u001b[49m[text_idx]] \n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mText 1:\u001b[39m\u001b[33m\"\u001b[39m, target_probas_1)\n",
      "\u001b[31mNameError\u001b[39m: name 'targets' is not defined"
     ]
    }
   ],
   "source": [
    "text_idx = 0 \n",
    "target_probas_1 = probs[text_idx, [0, 1, 2], targets[text_idx]] \n",
    "print(\"Text 1:\", target_probas_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d198b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.7146e-01,  1.9050e-01, -6.7213e-01,  ..., -4.5644e-01,\n",
       "           5.8417e-01, -1.5942e-01],\n",
       "         [ 6.5998e-01, -3.6519e-01, -7.6065e-02,  ..., -8.1529e-01,\n",
       "          -3.9120e-01,  1.3219e+00],\n",
       "         [ 1.4689e-01, -3.7880e-01,  4.7630e-01,  ..., -5.2538e-02,\n",
       "          -1.2468e+00,  1.1015e+00]],\n",
       "\n",
       "        [[ 1.6733e-01,  2.7546e-01,  1.3350e-01,  ...,  1.0224e-01,\n",
       "           3.6566e-01,  3.1196e-01],\n",
       "         [ 1.4522e-01,  2.4268e-01, -3.0002e-01,  ..., -5.5778e-01,\n",
       "           6.0691e-01,  7.3178e-01],\n",
       "         [ 9.1627e-01, -9.9669e-01, -4.5703e-01,  ..., -7.9702e-01,\n",
       "          -1.3285e+00, -1.1740e+00]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding loss\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efa612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.6601e-05, 2.0474e-05, 8.6409e-06,  ..., 1.0721e-05,\n",
       "          3.0350e-05, 1.4429e-05],\n",
       "         [3.2548e-05, 1.1676e-05, 1.5591e-05,  ..., 7.4444e-06,\n",
       "          1.1377e-05, 6.3094e-05],\n",
       "         [1.9507e-05, 1.1531e-05, 2.7117e-05,  ..., 1.5980e-05,\n",
       "          4.8408e-06, 5.0669e-05]],\n",
       "\n",
       "        [[1.9817e-05, 2.2079e-05, 1.9157e-05,  ..., 1.8568e-05,\n",
       "          2.4164e-05, 2.2900e-05],\n",
       "         [1.9423e-05, 2.1411e-05, 1.2443e-05,  ..., 9.6161e-06,\n",
       "          3.0819e-05, 3.4918e-05],\n",
       "         [4.1983e-05, 6.1985e-06, 1.0633e-05,  ..., 7.5684e-06,\n",
       "          4.4481e-06, 5.1914e-06]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = logits.softmax(-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dbf7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35316, 41252,  8311])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target 1\n",
    "logits[0].argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a573c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[35316],\n",
       "         [41252],\n",
       "         [ 8311]],\n",
       "\n",
       "        [[29716],\n",
       "         [40825],\n",
       "         [40942]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.argmax(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd00787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " effort moves you\n",
      "REP Heller discipl\n"
     ]
    }
   ],
   "source": [
    "# Batch 01:\n",
    "targets[0] # [3626, 6100,  345]\n",
    "print(tokenizer.decode(targets[0].numpy())) # Target\n",
    "print(tokenizer.decode(probs[0].argmax(-1).numpy())) # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba2263e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.6601e-05, 2.0474e-05, 8.6409e-06,  ..., 1.0721e-05, 3.0350e-05,\n",
      "         1.4429e-05],\n",
      "        [3.2548e-05, 1.1676e-05, 1.5591e-05,  ..., 7.4444e-06, 1.1377e-05,\n",
      "         6.3094e-05],\n",
      "        [1.9507e-05, 1.1531e-05, 2.7117e-05,  ..., 1.5980e-05, 4.8408e-06,\n",
      "         5.0669e-05]])\n"
     ]
    }
   ],
   "source": [
    "# Batch 01:\n",
    "print(probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b362f715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.1144e-06, 1.0326e-05, 2.1522e-05],\n",
      "        [2.3320e-05, 2.4707e-05, 2.2847e-05],\n",
      "        [9.9188e-06, 6.5877e-06, 9.7729e-06]])\n"
     ]
    }
   ],
   "source": [
    "print(probs[0, : , targets[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199ec42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.1144e-06, 2.4707e-05, 9.7729e-06])\n"
     ]
    }
   ],
   "source": [
    "print(probs[0, [0, 1, 2] , targets[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ed927",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing tensors could not be broadcast together with shapes [2], [3]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[167]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mprobs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mIndexError\u001b[39m: shape mismatch: indexing tensors could not be broadcast together with shapes [2], [3]"
     ]
    }
   ],
   "source": [
    "print(probs[0, [1, 2] , targets[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219fc7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d9347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9691652",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m probs = [\u001b[32m0.7\u001b[39m, \u001b[32m0.2\u001b[39m, \u001b[32m0.1\u001b[39m]\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: must be real number, not list"
     ]
    }
   ],
   "source": [
    "probs = [0.7, 0.2, 0.1]\n",
    "import math\n",
    "math.log(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92f264c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Text Generation Loss\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aafc7f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5.7891e-05, 2.3864e-05, 8.9075e-06,  ..., 1.2417e-05,\n",
       "          1.8809e-05, 1.0259e-05],\n",
       "         [2.9705e-05, 1.6748e-05, 1.0336e-05,  ..., 7.2643e-06,\n",
       "          1.7164e-05, 5.9128e-05],\n",
       "         [1.4773e-05, 8.2429e-06, 2.1888e-05,  ..., 1.6721e-05,\n",
       "          4.2902e-06, 3.1793e-05]],\n",
       "\n",
       "        [[1.6110e-05, 2.5260e-05, 2.4354e-05,  ..., 2.7548e-05,\n",
       "          3.4932e-05, 3.1465e-05],\n",
       "         [1.3854e-05, 3.4414e-05, 1.3104e-05,  ..., 1.0335e-05,\n",
       "          1.5262e-05, 5.0872e-05],\n",
       "         [3.2895e-05, 8.5632e-06, 1.5159e-05,  ..., 9.0781e-06,\n",
       "          9.3584e-06, 7.5844e-06]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probs = logits.softmax(-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2b00b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da902f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9586],\n",
       "         [  406],\n",
       "         [40115]],\n",
       "\n",
       "        [[29716],\n",
       "         [ 8185],\n",
       "         [42576]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = probs.argmax(-1, keepdim=True)\n",
    "token_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed9e0065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3626, 6100,  345])\n",
      "tensor([[ 9586],\n",
      "        [  406],\n",
      "        [40115]])\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])\n",
    "print(token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e0bc780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets:  effort moves you\n",
      "Output: aved L HO\n"
     ]
    }
   ],
   "source": [
    "def tokenid_to_text(tokens: torch.Tensor, tokenizer):\n",
    "    tokens = tokens.flatten()\n",
    "    return tokenizer.decode(tokens.numpy())\n",
    "\n",
    "print(f'Targets: {tokenid_to_text(targets[0], tokenizer)}')\n",
    "print(f'Output: {tokenid_to_text(token_ids[0], tokenizer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bb21361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.3301e-06, 3.1335e-05, 1.3145e-05])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For txt1:\n",
    "# ðŸ“Œ Probs corresponding to the Target Indexes:\n",
    "# Our goal is to maximize this probs, i.e Bring them closer to 1\n",
    "probs[0, [0, 1, 2], targets[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57c82f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.3301e-06, 3.1335e-05, 1.3145e-05])\n",
      "tensor([9.3920e-06, 3.0030e-05, 7.5925e-06])\n",
      "tensor([5.3301e-06, 3.1335e-05, 1.3145e-05, 9.3920e-06, 3.0030e-05, 7.5925e-06])\n"
     ]
    }
   ],
   "source": [
    "target_probas_1 = probs[0, [0, 1, 2], targets[0]]\n",
    "target_probas_2 = probs[1, [0, 1, 2], targets[1]]\n",
    "\n",
    "print(target_probas_1)\n",
    "print(target_probas_2)\n",
    "print(torch.cat((target_probas_1, target_probas_2), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "11b3bd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-12.1421, -10.3708, -11.2395, -11.5757, -10.4133, -11.7884])\n"
     ]
    }
   ],
   "source": [
    "log_probs = torch.log(torch.cat((target_probas_1, target_probas_2), -1))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d5d0dc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-11.2549)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_log_probs = log_probs.mean()\n",
    "avg_log_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "914de559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.2549)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_avg_log_probs = avg_log_probs * (-1)\n",
    "neg_avg_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5fddd6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.2325,  0.3463, -0.6392,  ..., -0.3070,  0.1082, -0.4979],\n",
       "          [ 0.5678, -0.0053, -0.4879,  ..., -0.8406,  0.0193,  1.2562],\n",
       "          [-0.1296, -0.7131,  0.2635,  ..., -0.0058, -1.3661,  0.6368]],\n",
       " \n",
       "         [[-0.0434,  0.4064,  0.3699,  ...,  0.4931,  0.7306,  0.6261],\n",
       "          [-0.1906,  0.7193, -0.2463,  ..., -0.4837, -0.0938,  1.1101],\n",
       "          [ 0.6722, -0.6736, -0.1025,  ..., -0.6152, -0.5848, -0.7950]]]),\n",
       " tensor([[ 3626,  6100,   345],\n",
       "         [ 1107,   588, 11311]]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "acb0e148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.2549)\n"
     ]
    }
   ],
   "source": [
    "# logits.flatten(0, 1).shape, targets.shape\n",
    "loss = torch.nn.functional.cross_entropy(\n",
    "    logits.flatten(0, 1), # (2x3, 50275) = (6, 50275)\n",
    "    targets.flatten() # (2x3) = (6)\n",
    ")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dd40518c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(77261.0156)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e6cb7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0429)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = torch.tensor([\n",
    "    [0.5, 0.3, 0.2],\n",
    "    [0.4, 0.1, 0.5],\n",
    "])\n",
    "\n",
    "t = torch.tensor([1, 2])\n",
    "\n",
    "torch.nn.functional.cross_entropy(l, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "19aa6b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = '../../ch02/01_main-chapter-code/the-verdict.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e30d21a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20479"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(FILE_PATH, 'r') as f:\n",
    "    txt = f.read()\n",
    "len(txt) # <Total-Characters = 20479>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d1c9c2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11, 290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976, 13, 357, 10915, 314, 2138, 1807, 340, 561, 423, 587, 10598, 393, 28537, 2014, 198, 198, 1, 464, 6001, 286, 465, 13476, 1, 438, 5562, 373, 644, 262, 1466, 1444, 340, 13, 314, 460, 3285, 9074, 13, 46606, 536, 5469, 438, 14363, 938, 4842, 1650, 353, 438, 2934, 489, 3255, 465, 48422, 540, 450, 67, 3299, 13, 366, 5189, 1781, 340, 338, 1016, 284, 3758, 262, 1988, 286, 616, 4286, 705, 1014, 510, 26, 475, 314, 836, 470, 892, 286, 326, 11, 1770, 13, 8759, 2763, 438, 1169, 2994, 284, 943, 17034, 318, 477, 314, 892, 286, 526, 383, 1573, 11, 319, 9074, 13, 536, 5469, 338, 11914, 11, 33096, 663, 4808, 3808, 62, 355, 996, 484, 547, 12548, 287, 281, 13079, 410, 12523, 286, 22353, 13, 843, 340, 373, 407, 691, 262, 9074, 13, 536, 48819, 508, 25722, 276, 13, 11161, 407, 262, 40123, 18113, 544, 9325, 701, 11, 379, 262, 938, 402, 1617, 261, 12917, 905, 11, 5025, 502, 878, 402, 271, 10899, 338, 366, 31640, 12, 67, 20811, 1, 284, 910, 11, 351, 10953, 287, 607, 2951, 25, 366, 1135, 2236, 407, 804, 2402, 663, 588, 757, 13984, 198, 198, 5779, 28112, 10197, 832, 262, 46475, 286, 18113, 544, 338, 10953, 314, 2936, 1498, 284, 1986, 262, 1109, 351, 1602, 11227, 414, 13, 23676, 3619, 402, 271, 10899, 0, 383, 1466, 550, 925, 683, 438, 270, 373, 15830, 326, 484, 815, 25722, 683, 13, 9754, 465, 898, 1714, 7380, 30090, 547, 2982, 11, 290, 287, 465, 898, 3292, 8941, 257, 4636, 28582, 13, 18612, 35394, 30, 8673, 13, 1002, 340, 547, 11, 262, 15393, 286, 262, 5977, 373, 29178, 3474, 416, 1310, 40559, 11959, 1636, 11, 508, 11, 287, 477, 922, 4562, 11, 3181, 503, 287, 262, 37090, 257, 845, 22665, 366, 672, 270, 2838, 1, 319, 3619, 438, 505, 286, 883, 905, 88, 6685, 42070, 351, 4738, 6276, 871, 326, 314, 423, 2982, 357, 40, 1839, 470, 910, 416, 4150, 8, 3688, 284, 402, 271, 10899, 338, 12036, 13, 843, 523, 438, 14363, 10568, 852, 5729, 11331, 18893, 540, 438, 1169, 5114, 11835, 3724, 503, 11, 290, 11, 355, 9074, 13, 536, 5469, 550, 11001, 11, 262, 2756, 286, 366, 38, 271, 10899, 82, 1, 1816, 510, 13, 198, 198, 1026, 373, 407, 10597, 1115, 812, 1568, 326, 11, 287, 262, 1781, 286, 257, 1178, 2745, 6, 4686, 1359, 319, 262, 34686, 41976, 11, 340, 6451, 5091, 284, 502, 284, 4240, 1521, 402, 271, 10899, 550, 1813, 510, 465, 12036, 13, 1550, 14580, 11, 340, 1107, 373, 257, 29850, 1917, 13, 1675, 24456, 465, 3656, 561, 423, 587, 1165, 2562, 438, 14363, 3148, 1650, 1010, 550, 587, 6699, 262, 1540, 558, 286, 2282, 326, 9074, 13, 402, 271, 10899, 550, 366, 7109, 14655, 683, 866, 526, 1114, 9074, 13, 402, 271, 10899, 438, 292, 884, 438, 18108, 407, 11196, 10597, 3016, 257, 614, 706, 3619, 338, 10568, 550, 587, 2077, 13, 632, 1244, 307, 326, 339, 550, 6405, 607, 438, 20777, 339, 8288, 465, 10152, 438, 13893, 339, 1422, 470, 765, 284, 467, 319, 12036, 26, 475, 340, 561, 423, 587, 1327, 284, 5879, 326, 339, 550, 1813, 510, 465, 12036, 780, 339, 550, 6405, 607, 13, 198, 198, 5189, 1781, 11, 611, 673, 550, 407, 17901, 683, 866, 11, 673, 550, 8603, 11, 355, 4544, 9325, 701, 42397, 11, 4054, 284, 366, 26282, 683, 510, 1, 438, 7091, 550, 407, 2957, 683, 736, 284, 262, 1396, 417, 13, 1675, 1234, 262, 14093, 656, 465, 1021, 757, 438, 10919, 257, 410, 5040, 329, 257, 3656, 0, 887, 9074, 13, 402, 271, 10899, 4120, 284, 423, 595, 67, 1328, 340, 438, 392, 314, 2936, 340, 1244, 307, 3499, 284, 1064, 503, 1521, 13, 198, 198, 464, 748, 586, 652, 1204, 286, 262, 34686, 41976, 37733, 2346, 284, 884, 14177, 8233, 1020, 5768, 26, 290, 1719, 11, 319, 616, 835, 284, 22489, 40089, 11, 4978, 257, 19350, 286, 3619, 338, 3652, 436, 81, 5286, 8812, 2114, 1022, 262, 279, 1127, 11, 314, 550, 3589, 28068, 294, 1555, 262, 1306, 1110, 13, 198, 198, 40, 1043, 262, 3155, 379, 8887, 11061, 511, 18057, 12, 83, 6037, 26, 290, 9074, 13, 402, 271, 10899, 338, 7062, 373, 523, 2429, 498, 326, 11, 287, 262, 29543, 2745, 11, 314, 4752, 340, 6777, 13, 632, 373, 407, 326, 616, 2583, 408, 373, 366, 47914, 1298, 319, 326, 966, 314, 714, 423, 1813, 4544, 9325, 701, 262, 40830, 12719, 3874, 13, 632, 373, 655, 780, 673, 373, 4808, 1662, 62, 3499, 438, 361, 314, 743, 307, 41746, 12004, 262, 6473, 438, 5562, 314, 1043, 607, 523, 13, 1114, 3619, 11, 477, 465, 1204, 11, 550, 587, 11191, 416, 3499, 1466, 25, 484, 550, 26546, 1068, 465, 1242, 11, 340, 550, 587, 302, 1144, 287, 262, 3024, 12, 4803, 286, 511, 512, 1741, 13, 843, 340, 373, 4361, 5048, 425, 284, 3465, 644, 1245, 262, 366, 25124, 3101, 8137, 286, 16957, 1696, 414, 1, 357, 40, 9577, 4544, 9325, 701, 8, 373, 1719, 319, 683, 13, 198, 198, 40, 423, 4750, 326, 9074, 13, 402, 271, 10899, 373, 5527, 26, 290, 340, 373, 3393, 34953, 856, 326, 607, 5229, 373, 37895, 422, 428, 25179, 257, 19217, 475, 8904, 14676, 13, 632, 318, 11, 355, 257, 3896, 11, 262, 661, 508, 40987, 1637, 508, 651, 749, 503, 286, 340, 26, 290, 3619, 338, 19992, 31564, 286, 465, 3656, 338, 1263, 5236, 9343, 683, 11, 351, 281, 5585, 286, 2818, 922, 12, 49705, 11, 284, 21595, 1133, 340, 656, 5563, 286, 1242, 290, 13064, 13, 1675, 262, 6846, 11, 314, 1276, 751, 11, 339, 6150, 5365, 31655, 26, 475, 339, 373, 7067, 29396, 18443, 12271, 290, 45592, 12, 14792, 5986, 351, 257, 8839, 326, 7284, 35924, 262, 12306, 395, 4133, 13, 198, 198, 1, 26788, 338, 691, 12226, 318, 284, 1234, 8737, 656, 19133, 553, 373, 530, 286, 262, 7877, 72, 3150, 339, 8104, 866, 1973, 262, 37918, 411, 290, 8465, 286, 281, 33954, 271, 3973, 9899, 14678, 40556, 12, 11487, 11, 618, 11, 319, 257, 1568, 1110, 11, 314, 550, 757, 1057, 625, 422, 22489, 40089, 26, 290, 9074, 13, 402, 271, 10899, 11, 307, 3723, 319, 683, 11, 2087, 329, 616, 35957, 25, 366, 14295, 318, 523, 34813, 306, 8564, 284, 790, 1296, 286, 8737, 526, 198, 198, 43920, 3619, 0, 632, 550, 1464, 587, 465, 10030, 284, 423, 1466, 910, 884, 1243, 286, 683, 25, 262, 1109, 815, 307, 900, 866, 287, 1070, 268, 2288, 13, 1867, 7425, 502, 783, 373, 326, 11, 329, 262, 717, 640, 11, 339, 581, 4714, 262, 8216, 13, 314, 550, 1775, 683, 11, 523, 1690, 11, 1615, 3364, 739, 2092, 256, 7657, 438, 9776, 340, 262, 11644, 43778, 3465, 326, 26773, 606, 286, 511, 6799, 454, 30, 1400, 438, 1640, 11, 31414, 1576, 11, 340, 2627, 4156, 326, 339, 373, 16245, 286, 9074, 13, 402, 271, 10899, 438, 69, 623, 1576, 407, 284, 766, 607, 41793, 13, 632, 373, 465, 898, 41793, 339, 3947, 284, 307, 1592, 2259, 739, 438, 14363, 898, 9408, 355, 281, 2134, 329, 5482, 4447, 290, 753, 1072, 13, 198, 198, 1, 3666, 13674, 11, 1201, 314, 1053, 442, 17758, 12036, 661, 836, 470, 910, 326, 3404, 546, 502, 438, 9930, 910, 340, 546, 12622, 41379, 293, 553, 373, 465, 691, 5402, 11, 355, 339, 8278, 422, 262, 3084, 290, 336, 8375, 503, 4291, 262, 4252, 18250, 8812, 558, 13, 198, 198, 40, 27846, 706, 683, 11, 7425, 416, 465, 938, 1573, 13, 12622, 41379, 293, 373, 11, 287, 1109, 11, 5033, 262, 582, 286, 262, 2589, 438, 292, 3619, 2241, 11, 530, 1244, 1234, 340, 11, 550, 587, 262, 582, 286, 262, 1711, 13, 383, 7099, 6802, 373, 531, 284, 423, 7042, 2241, 379, 616, 1545, 338, 3625, 11, 290, 314, 14028, 611, 257, 256, 11912, 286, 35394, 739, 10724, 262, 6846, 338, 11428, 450, 67, 3299, 13, 887, 645, 438, 1640, 340, 373, 407, 10597, 706, 326, 1785, 326, 262, 4808, 13698, 10322, 6532, 62, 8263, 12, 9649, 550, 9258, 284, 3359, 511, 366, 8642, 521, 829, 526, 198, 198, 40, 2900, 284, 9074, 13, 402, 271, 10899, 11, 508, 550, 18459, 1068, 284, 1577, 257, 23844, 286, 7543, 284, 607, 599, 6321, 287, 262, 17423, 12, 3823, 13, 198, 198, 1, 5195, 4808, 10134, 62, 339, 442, 17758, 12036, 1701, 314, 1965, 25891, 13, 198, 198, 3347, 4376, 607, 26928, 351, 257, 9254, 286, 922, 12, 17047, 8167, 5975, 13, 198, 198, 1, 5812, 11, 339, 1595, 470, 4808, 14150, 62, 284, 783, 11, 345, 760, 26, 290, 314, 765, 683, 284, 2883, 2241, 553, 673, 531, 2407, 2391, 13, 198, 198, 40, 3114, 546, 262, 40894, 2330, 12, 6839, 11978, 2119, 11, 351, 663, 4808, 44769, 8270, 12, 332, 660, 62, 410, 1386, 20394, 262, 23755, 286, 262, 14005, 1801, 2093, 41160, 11, 290, 663, 45592, 12, 14792, 1613, 1424, 287, 19217, 24887, 13431, 13, 198, 198, 1, 19242, 339, 442, 17758, 465, 5986, 1165, 30, 314, 4398, 470, 1775, 257, 2060, 530, 287, 262, 2156, 526, 198, 198, 32, 3731, 17979, 286, 32315, 12606, 9074, 13, 402, 271, 10899, 338, 1280, 954, 36368, 13, 366, 1026, 338, 465, 11441, 48740, 11, 345, 760, 13, 679, 1139, 484, 821, 407, 4197, 284, 423, 546, 26, 339, 338, 1908, 606, 477, 1497, 2845, 530, 438, 1820, 18560, 438, 392, 326, 314, 423, 284, 1394, 26148, 526, 198, 198, 6653, 11441, 48740, 438, 14295, 338, 48740, 546, 465, 5986, 30, 2011, 20136, 373, 3957, 588, 262, 26394, 12, 301, 971, 13, 314, 531, 10722, 292, 2280, 284, 616, 2583, 408, 25, 366, 40, 1276, 1107, 766, 534, 18560, 11, 345, 760, 526, 198, 198, 3347, 27846, 503, 2048, 4628, 24882, 379, 262, 8812, 558, 810, 607, 5229, 11, 21081, 782, 278, 287, 257, 14263, 276, 5118, 11, 550, 6578, 257, 24518, 290, 7428, 262, 3394, 20096, 39047, 338, 1182, 1022, 465, 14475, 13, 198, 198, 1, 5779, 11, 1282, 981, 339, 338, 407, 2045, 553, 673, 531, 11, 351, 257, 6487, 326, 3088, 284, 7808, 607, 10927, 1108, 26, 290, 314, 3940, 607, 1022, 262, 30623, 2295, 49406, 286, 262, 6899, 11, 290, 510, 262, 3094, 16046, 351, 1059, 430, 12, 66, 12375, 299, 20896, 82, 24357, 1871, 12734, 379, 1123, 9581, 13, 198, 198, 818, 262, 5391, 76, 395, 5228, 286, 607, 275, 2778, 10840, 11, 10371, 257, 1534, 4241, 286, 19217, 290, 18876, 5563, 11, 9174, 530, 286, 262, 5385, 41186, 39614, 1386, 11, 287, 262, 13203, 5482, 1044, 276, 5739, 13, 383, 5019, 19001, 286, 262, 5739, 1444, 510, 477, 402, 271, 10899, 338, 1613, 0, 198, 198, 27034, 13, 402, 271, 10899, 9859, 736, 262, 4324, 12, 66, 3325, 1299, 11, 3888, 7263, 257, 4808, 73, 446, 259, 13235, 62, 1336, 286, 11398, 35560, 1000, 292, 11, 7121, 281, 3211, 12, 16337, 1497, 11, 290, 531, 25, 366, 1532, 345, 1302, 994, 345, 460, 655, 6687, 284, 766, 340, 13, 314, 550, 340, 625, 262, 24818, 417, 12, 12239, 11, 475, 339, 3636, 470, 1309, 340, 2652, 526, 198, 198, 5297, 438, 40, 714, 655, 6687, 284, 766, 340, 438, 1169, 717, 18560, 286, 3619, 338, 314, 550, 1683, 550, 284, 14022, 616, 2951, 625, 0, 19672, 484, 550, 262, 1295, 286, 15393, 438, 16706, 262, 4318, 6103, 287, 257, 14005, 7872, 393, 4808, 13698, 10322, 6532, 62, 8263, 12, 3823, 11, 393, 257, 36364, 1396, 417, 4624, 523, 326, 340, 1718, 262, 1657, 832, 41160, 286, 1468, 9932, 316, 666, 966, 13, 383, 517, 12949, 1295, 2627, 262, 4286, 1365, 26, 1865, 11, 355, 616, 2951, 6348, 23840, 284, 262, 2063, 12, 2971, 11, 477, 262, 16704, 14482, 1625, 503, 438, 439, 262, 10818, 20597, 32192, 355, 2709, 330, 871, 11, 262, 15910, 286, 16153, 312, 328, 3780, 416, 543, 11, 351, 884, 2784, 9830, 5032, 11, 339, 5257, 284, 36583, 3241, 422, 262, 1103, 1597, 286, 262, 4286, 284, 617, 2495, 11331, 2768, 590, 286, 3703, 13, 9074, 13, 402, 271, 10899, 11, 17728, 257, 8500, 4417, 284, 670, 319, 438, 15464, 11, 355, 340, 547, 11, 523, 16857, 262, 4469, 286, 607, 898, 4286, 438, 18108, 26269, 5223, 287, 281, 8468, 4922, 284, 262, 3359, 286, 428, 3991, 4118, 84, 16579, 13, 383, 4286, 373, 530, 286, 3619, 338, 366, 11576, 395, 553, 355, 465, 21099, 3808, 561, 423, 1234, 340, 438, 270, 7997, 11, 319, 465, 636, 11, 257, 29844, 286, 12749, 11, 257, 22791, 278, 286, 32375, 11, 257, 22486, 11, 965, 2860, 1359, 290, 965, 1397, 11, 326, 14516, 530, 286, 262, 33125, 12, 565, 593, 338, 25304, 4040, 284, 10303, 257, 17972, 13, 632, 1138, 11, 287, 1790, 11, 379, 790, 966, 262, 3512, 286, 14081, 2415, 284, 307, 13055, 366, 11576, 306, 1, 780, 673, 373, 10032, 286, 852, 13055, 366, 34751, 306, 1, 438, 392, 1865, 407, 284, 4425, 281, 22037, 286, 262, 32073, 13, 198, 198, 1, 1026, 338, 262, 938, 339, 13055, 11, 345, 760, 553, 9074, 13, 402, 271, 10899, 531, 351, 27322, 540, 11293, 13, 366, 464, 938, 475, 530, 553, 673, 19267, 5223, 438, 1, 4360, 262, 584, 1595, 470, 954, 11, 780, 339, 6572, 340, 526, 198, 198, 1, 49174, 276, 340, 1701, 314, 373, 546, 284, 1061, 510, 428, 18437, 618, 314, 2982, 257, 2366, 9662, 290, 2497, 3619, 2241, 319, 262, 11387, 13, 198, 198, 1722, 339, 6204, 612, 11, 465, 2832, 287, 262, 16511, 286, 465, 11555, 303, 7821, 13209, 11, 262, 7888, 7586, 9813, 286, 4190, 7121, 736, 422, 465, 2330, 22645, 11, 465, 10904, 4252, 6236, 429, 25839, 9230, 808, 276, 416, 257, 8212, 326, 13663, 262, 9040, 286, 257, 2116, 12, 10414, 738, 285, 23968, 4891, 11, 314, 2936, 284, 644, 257, 4922, 339, 550, 262, 976, 3081, 355, 465, 5986, 438, 1169, 3081, 286, 2045, 1190, 4119, 81, 621, 339, 373, 13, 198, 198, 6653, 3656, 27846, 379, 683, 1207, 8344, 803, 306, 11, 475, 465, 2951, 21650, 1613, 607, 284, 262, 18560, 13, 198, 198, 1, 5246, 13, 8759, 2763, 2227, 284, 766, 340, 553, 673, 2540, 11, 355, 611, 2859, 3500, 5223, 13, 679, 28271, 465, 12450, 11, 991, 16755, 13, 198, 198, 1, 5812, 11, 8759, 2763, 1043, 502, 503, 890, 2084, 553, 339, 531, 15376, 26, 788, 11, 6427, 465, 3211, 832, 6164, 25, 366, 16773, 290, 766, 262, 1334, 286, 262, 2156, 526, 198, 198, 1544, 3751, 340, 284, 502, 351, 257, 1611, 286, 24354, 20154, 11293, 25, 262, 7837, 12, 9649, 11, 262, 5486, 12, 83, 29080, 11, 262, 6576, 12, 565, 418, 1039, 11, 262, 4057, 2655, 12, 8439, 274, 438, 439, 262, 3716, 7106, 6637, 286, 262, 45172, 338, 5928, 3773, 13, 843, 8797, 616, 4240, 3432, 262, 2938, 17547, 339, 531, 11, 9644, 503, 465, 7721, 257, 1310, 25, 366, 5297, 11, 314, 1107, 836, 470, 766, 703, 661, 6687, 284, 2107, 1231, 326, 526, 198, 198, 5779, 438, 270, 373, 655, 262, 886, 530, 1244, 423, 1674, 15898, 329, 683, 13, 5514, 339, 373, 11, 832, 340, 477, 290, 287, 15275, 286, 340, 477, 438, 292, 339, 550, 587, 832, 11, 290, 287, 15275, 286, 11, 465, 5986, 438, 568, 22665, 11, 523, 23332, 11, 523, 595, 18052, 11, 326, 530, 890, 276, 284, 3960, 503, 25, 366, 3856, 44455, 351, 534, 24638, 2474, 355, 1752, 530, 550, 890, 276, 284, 910, 25, 366, 3856, 44455, 351, 534, 670, 2474, 198, 198, 1537, 11, 351, 262, 3960, 319, 616, 11914, 11, 616, 13669, 6989, 281, 10059, 2198, 13, 198, 198, 1, 1212, 318, 616, 898, 49451, 553, 339, 531, 11, 3756, 502, 656, 257, 3223, 8631, 2119, 379, 262, 886, 286, 262, 781, 273, 312, 410, 12523, 13, 632, 373, 6616, 290, 7586, 290, 11620, 88, 25, 645, 366, 34435, 8172, 645, 865, 291, 12, 64, 12, 1671, 330, 11, 4844, 286, 262, 1633, 286, 24380, 329, 20728, 287, 257, 4286, 10273, 438, 29370, 477, 11, 645, 1551, 1051, 286, 1683, 1719, 587, 973, 355, 257, 8034, 13, 198, 198, 464, 1109, 3181, 1363, 284, 502, 262, 4112, 957, 1483, 286, 3619, 338, 2270, 351, 465, 1468, 1204, 13, 198, 198, 1, 3987, 470, 345, 1683, 45553, 903, 351, 7521, 597, 517, 1701, 314, 1965, 11, 991, 2045, 546, 329, 257, 12854, 286, 884, 3842, 13, 198, 198, 1, 12295, 553, 339, 531, 11589, 13, 198, 198, 1, 5574, 1660, 12, 49903, 438, 273, 2123, 10813, 1701, 198, 198, 6653, 6563, 2951, 6348, 5391, 11, 290, 465, 25839, 279, 3021, 257, 1310, 739, 511, 22665, 4252, 10899, 13, 198, 198, 1, 12295, 892, 286, 340, 11, 616, 13674, 5891, 438, 1092, 517, 621, 611, 314, 1549, 1239, 12615, 257, 14093, 526, 198, 198, 1870, 465, 8216, 1297, 502, 287, 257, 7644, 326, 339, 1239, 1807, 286, 1997, 2073, 13, 198, 198, 40, 3888, 1497, 11, 43045, 21100, 416, 616, 10059, 9412, 26, 290, 355, 314, 2900, 11, 616, 4151, 3214, 319, 257, 1402, 4286, 2029, 262, 24818, 417, 12, 12239, 438, 1169, 691, 2134, 7163, 262, 8631, 26210, 3425, 9417, 286, 262, 2119, 13, 198, 198, 1, 5812, 11, 416, 449, 659, 2474, 314, 531, 13, 198, 198, 1026, 373, 257, 17548, 286, 257, 50085, 438, 272, 1468, 10032, 50085, 11, 5055, 287, 262, 6290, 739, 257, 3355, 13, 198, 198, 1, 3886, 449, 659, 438, 64, 520, 5493, 2474, 314, 16896, 13, 198, 198, 1544, 373, 10574, 26, 475, 314, 2936, 683, 1969, 2157, 502, 11, 12704, 257, 1310, 2952, 13, 198, 198, 1, 2061, 257, 4240, 0, 14446, 351, 257, 8667, 3951, 438, 4360, 319, 45697, 19369, 13, 921, 9670, 28022, 11, 810, 750, 345, 651, 340, 1701, 198, 198, 1544, 9373, 6364, 25, 366, 27034, 13, 520, 5493, 2921, 340, 284, 502, 526, 198, 198, 1, 10910, 438, 40, 1422, 470, 760, 345, 772, 2993, 262, 520, 5493, 82, 13, 679, 373, 884, 281, 1167, 2588, 856, 607, 2781, 526, 198, 198, 1, 40, 1422, 470, 438, 83, 359, 706, 13, 764, 764, 764, 1375, 1908, 329, 502, 284, 7521, 683, 618, 339, 373, 2636, 526, 198, 198, 1, 2215, 339, 373, 2636, 30, 921, 1701, 198, 198, 40, 1276, 423, 1309, 257, 1310, 1165, 881, 40642, 972, 6654, 832, 616, 5975, 11, 329, 339, 9373, 351, 257, 1207, 8344, 803, 6487, 25, 366, 5297, 438, 7091, 338, 281, 12659, 2829, 1122, 11, 345, 760, 11, 9074, 13, 520, 5493, 13, 2332, 691, 2126, 373, 284, 423, 683, 1760, 416, 257, 38378, 34537, 438, 993, 11, 3595, 520, 5493, 0, 1375, 1807, 340, 262, 1654, 301, 835, 286, 46431, 465, 27951, 438, 1659, 10833, 340, 319, 257, 1308, 27461, 1171, 13, 843, 379, 262, 2589, 314, 373, 4808, 1169, 62, 38378, 34537, 526, 198, 198, 1, 10910, 11, 3595, 520, 5493, 438, 292, 345, 910, 13, 8920, 4808, 5562, 62, 465, 2106, 1701, 198, 198, 1, 2504, 373, 465, 2106, 13, 1375, 4762, 287, 683, 11, 26996, 798, 287, 683, 438, 273, 1807, 673, 750, 13, 887, 673, 3521, 470, 6842, 407, 284, 423, 477, 262, 8263, 12, 9649, 351, 607, 13, 1375, 3521, 470, 6842, 262, 1109, 326, 11, 319, 1401, 77, 3929, 1528, 11, 530, 714, 1464, 651, 1474, 1576, 284, 766, 465, 5986, 13, 23676, 2415, 0, 1375, 338, 655, 257, 24225, 39136, 278, 329, 584, 21441, 13, 520, 5493, 318, 262, 691, 2187, 314, 1683, 2993, 526, 198, 198, 1, 1639, 1683, 2993, 30, 887, 345, 655, 531, 438, 1, 198, 198, 38, 271, 10899, 550, 257, 11040, 8212, 287, 465, 2951, 13, 198, 198, 1, 5812, 11, 314, 2993, 683, 11, 290, 339, 2993, 502, 438, 8807, 340, 3022, 706, 339, 373, 2636, 526, 198, 198, 40, 5710, 616, 3809, 43045, 13, 366, 2215, 673, 1908, 329, 345, 1701, 198, 198, 1, 5297, 438, 37121, 1035, 27339, 284, 262, 21296, 13, 1375, 2227, 683, 29178, 3474, 438, 392, 416, 502, 2474, 198, 198, 1544, 13818, 757, 11, 290, 9617, 736, 465, 1182, 284, 804, 510, 379, 262, 17548, 286, 262, 50085, 13, 366, 1858, 547, 1528, 618, 314, 3521, 470, 804, 379, 326, 1517, 438, 24089, 77, 470, 1986, 340, 13, 887, 314, 4137, 3589, 284, 1234, 340, 994, 26, 290, 783, 340, 338, 30703, 502, 438, 66, 1522, 502, 13, 1320, 338, 262, 1738, 1521, 314, 836, 470, 45553, 903, 597, 517, 11, 616, 13674, 8759, 2763, 26, 393, 2138, 520, 5493, 2241, 318, 262, 1738, 526, 198, 198, 1890, 262, 717, 640, 616, 21696, 20136, 546, 616, 15185, 2900, 656, 257, 2726, 6227, 284, 1833, 683, 1365, 13, 198, 198, 1, 40, 4601, 345, 1549, 1560, 502, 703, 340, 3022, 553, 314, 531, 13, 198, 198, 1544, 6204, 2045, 510, 379, 262, 17548, 11, 290, 665, 24297, 1022, 465, 9353, 257, 17779, 339, 550, 11564, 284, 1657, 13, 24975, 339, 2900, 3812, 502, 13, 198, 198, 1, 40, 1549, 2138, 588, 284, 1560, 345, 438, 13893, 314, 1053, 1464, 9885, 345, 286, 2376, 26927, 616, 670, 526, 198, 198, 40, 925, 257, 1207, 8344, 803, 18342, 11, 543, 339, 2469, 265, 1572, 351, 257, 922, 12, 17047, 8167, 32545, 13, 198, 198, 1, 5812, 11, 314, 1422, 470, 1337, 257, 14787, 618, 314, 4762, 287, 3589, 438, 392, 783, 340, 338, 281, 2087, 9839, 1022, 514, 2474, 198, 198, 1544, 13818, 4622, 11, 1231, 35987, 11, 290, 7121, 530, 286, 262, 2769, 3211, 12, 49655, 2651, 13, 366, 1858, 25, 787, 3511, 6792, 438, 392, 994, 389, 262, 33204, 345, 588, 526, 198, 198, 1544, 4624, 606, 379, 616, 22662, 290, 3767, 284, 27776, 510, 290, 866, 262, 2119, 11, 12225, 783, 290, 788, 11061, 262, 4286, 13, 198, 198, 1, 2437, 340, 3022, 30, 314, 460, 1560, 345, 287, 1936, 2431, 438, 392, 340, 1422, 470, 1011, 881, 2392, 284, 1645, 13, 764, 764, 764, 314, 460, 3505, 783, 703, 6655, 290, 10607, 314, 373, 618, 314, 1392, 9074, 13, 520, 5493, 338, 3465, 13, 3226, 1781, 11, 2769, 866, 11, 314, 550, 1464, 4808, 31985, 62, 612, 373, 645, 530, 588, 683, 438, 8807, 314, 550, 3750, 351, 262, 4269, 11, 22211, 262, 6678, 40315, 10455, 546, 683, 11, 10597, 314, 2063, 1392, 284, 892, 339, 373, 257, 5287, 11, 530, 286, 262, 1611, 326, 389, 1364, 2157, 13, 2750, 449, 659, 11, 290, 339, 4808, 9776, 62, 1364, 2157, 438, 13893, 339, 550, 1282, 284, 2652, 0, 383, 1334, 286, 514, 550, 284, 1309, 6731, 307, 17676, 1863, 393, 467, 739, 11, 475, 339, 373, 1029, 2029, 262, 1459, 438, 261, 45697, 19369, 11, 355, 345, 910, 13, 198, 198, 1, 5779, 11, 314, 1816, 572, 284, 262, 2156, 287, 616, 749, 34372, 10038, 438, 34330, 3888, 11, 4453, 20927, 502, 11, 379, 262, 3108, 418, 286, 3595, 520, 5493, 338, 3451, 286, 5287, 852, 37492, 416, 262, 13476, 286, 616, 12036, 683, 0, 3226, 1781, 314, 4001, 284, 466, 262, 4286, 329, 2147, 438, 40, 1297, 9074, 13, 520, 5493, 523, 618, 673, 2540, 284, 336, 321, 647, 1223, 546, 607, 8098, 13, 314, 3505, 1972, 572, 257, 40426, 10956, 9546, 546, 262, 15393, 852, 4808, 3810, 62, 438, 1219, 11, 314, 373, 19716, 306, 11, 616, 13674, 8759, 2763, 0, 314, 373, 24380, 284, 3589, 588, 530, 286, 616, 898, 1650, 1010, 13, 198, 198, 1, 6423, 314, 373, 2077, 510, 290, 1364, 3436, 351, 683, 13, 314, 550, 1908, 477, 616, 20348, 287, 5963, 11, 290, 314, 550, 691, 284, 900, 510, 262, 1396, 417, 290, 651, 284, 670, 13, 679, 550, 587, 2636, 691, 8208, 12, 14337, 2250, 11, 290, 339, 3724, 6451, 11, 286, 2612, 4369, 11, 523, 326, 612, 550, 587, 645, 15223, 670, 286, 8166, 438, 14363, 1986, 373, 1598, 290, 36519, 13, 314, 550, 1138, 683, 1752, 393, 5403, 11, 812, 878, 11, 290, 1807, 683, 32081, 290, 44852, 88, 13, 2735, 314, 2497, 326, 339, 373, 21840, 13, 198, 198, 1, 40, 373, 9675, 379, 717, 11, 351, 257, 6974, 19713, 14676, 25, 9675, 284, 423, 616, 1021, 319, 884, 257, 705, 32796, 2637, 3244, 465, 6283, 1204, 12, 46965, 9449, 2540, 284, 2689, 502, 24506, 306, 438, 292, 314, 10226, 262, 1182, 287, 314, 2936, 355, 611, 339, 547, 4964, 502, 466, 340, 13, 383, 18098, 373, 3940, 416, 262, 1807, 25, 611, 339, 4808, 22474, 62, 4964, 502, 11, 644, 561, 339, 910, 284, 616, 835, 286, 1762, 30, 2011, 29483, 2540, 284, 467, 257, 1310, 4295, 438, 40, 2936, 10927, 290, 8627, 13, 198, 198, 1, 7454, 11, 618, 314, 3114, 510, 11, 314, 3947, 284, 766, 257, 8212, 2157, 465, 1969, 12768, 680, 21213, 438, 292, 611, 339, 550, 262, 3200, 11, 290, 547, 28297, 2241, 416, 4769, 340, 736, 422, 502, 13, 1320, 41851, 515, 502, 991, 517, 13, 383, 3200, 30, 4162, 11, 314, 550, 257, 3200, 2861, 8208, 286, 465, 0, 314, 37901, 379, 262, 21978, 44896, 11, 290, 3088, 617, 286, 616, 49025, 5330, 15910, 13, 887, 484, 4054, 502, 11, 484, 1067, 11137, 13, 314, 2497, 326, 339, 2492, 470, 4964, 262, 905, 88, 10340, 438, 40, 3521, 470, 11786, 465, 3241, 26, 339, 655, 4030, 465, 2951, 319, 262, 1327, 22674, 1022, 13, 5845, 547, 262, 3392, 314, 550, 1464, 427, 343, 9091, 11, 393, 5017, 510, 351, 617, 9105, 7521, 13, 843, 703, 339, 2497, 832, 616, 7363, 0, 198, 198, 1, 40, 3114, 510, 757, 11, 290, 4978, 6504, 286, 326, 17548, 286, 262, 50085, 10938, 319, 262, 3355, 1474, 465, 3996, 13, 2399, 3656, 1297, 502, 20875, 340, 373, 262, 938, 1517, 339, 550, 1760, 438, 3137, 257, 3465, 2077, 351, 257, 17275, 1021, 11, 618, 339, 373, 866, 287, 6245, 684, 10695, 20222, 422, 257, 2180, 2612, 1368, 13, 2329, 257, 3465, 0, 887, 340, 4952, 465, 2187, 2106, 13, 1318, 389, 812, 286, 5827, 40987, 913, 30802, 287, 790, 1627, 13, 317, 582, 508, 550, 1509, 388, 351, 262, 1459, 714, 1239, 423, 4499, 326, 18680, 510, 12, 5532, 14000, 13, 764, 764, 764, 198, 198, 1, 40, 2900, 736, 284, 616, 670, 11, 290, 1816, 319, 39136, 278, 290, 285, 4185, 1359, 26, 788, 314, 3114, 379, 262, 50085, 757, 13, 314, 2497, 326, 11, 618, 520, 5493, 8104, 287, 262, 717, 14000, 11, 339, 2993, 655, 644, 262, 886, 561, 307, 13, 679, 550, 17273, 465, 2426, 11, 19233, 340, 11, 11027, 515, 340, 13, 1649, 550, 314, 1760, 326, 351, 597, 286, 616, 1243, 30, 1119, 8020, 470, 587, 4642, 286, 502, 438, 40, 550, 655, 8197, 606, 13, 764, 764, 764, 198, 198, 1, 39, 648, 340, 11, 8759, 2763, 11, 351, 326, 1986, 4964, 502, 314, 3521, 470, 466, 1194, 14000, 13, 383, 8631, 3872, 373, 11, 314, 1422, 470, 760, 810, 284, 1234, 340, 438, 62, 40, 550, 1239, 1900, 44807, 5514, 11, 351, 616, 1650, 1010, 290, 616, 1171, 11, 257, 905, 88, 22870, 286, 9568, 5017, 510, 262, 1109, 438, 40, 655, 9617, 7521, 656, 511, 6698, 13, 764, 764, 764, 3894, 11, 7521, 373, 262, 530, 7090, 883, 2636, 2951, 714, 766, 832, 438, 3826, 3892, 284, 262, 2006, 20212, 19369, 14638, 13, 2094, 470, 345, 760, 703, 11, 287, 3375, 257, 3215, 3303, 11, 772, 6562, 1473, 11, 530, 1139, 2063, 262, 640, 407, 644, 530, 3382, 284, 475, 644, 530, 460, 30, 3894, 438, 5562, 373, 262, 835, 314, 13055, 26, 290, 355, 339, 3830, 612, 290, 7342, 502, 11, 262, 1517, 484, 1444, 616, 705, 23873, 2350, 6, 14707, 588, 257, 2156, 286, 4116, 13, 679, 1422, 470, 10505, 263, 11, 345, 1833, 11, 3595, 520, 5493, 438, 258, 655, 3830, 612, 12703, 4964, 11, 290, 319, 465, 11914, 11, 832, 262, 12768, 21213, 11, 314, 3947, 284, 3285, 262, 1808, 25, 705, 8491, 345, 1654, 345, 760, 810, 345, 821, 2406, 503, 8348, 198, 198, 1, 1532, 314, 714, 423, 13055, 326, 1986, 11, 351, 326, 1808, 319, 340, 11, 314, 815, 423, 1760, 257, 1049, 1517, 13, 383, 1306, 6000, 1517, 373, 284, 766, 326, 314, 3521, 470, 438, 392, 326, 11542, 373, 1813, 502, 13, 887, 11, 11752, 11, 379, 326, 5664, 11, 8759, 2763, 11, 373, 612, 1997, 319, 4534, 314, 3636, 470, 423, 1813, 284, 423, 520, 5493, 6776, 878, 502, 11, 290, 284, 3285, 683, 910, 25, 705, 1026, 338, 407, 1165, 2739, 438, 40, 1183, 905, 345, 703, 30960, 198, 198, 1, 1026, 4808, 9776, 62, 1165, 2739, 438, 270, 561, 423, 587, 11, 772, 611, 339, 1549, 587, 6776, 13, 314, 11856, 510, 616, 20348, 11, 290, 1816, 866, 290, 1297, 9074, 13, 520, 5493, 13, 3226, 1781, 314, 1422, 470, 1560, 607, 4808, 5562, 62, 438, 270, 561, 423, 587, 8312, 284, 607, 13, 314, 2391, 531, 314, 3521, 470, 7521, 683, 11, 326, 314, 373, 1165, 3888, 13, 1375, 2138, 8288, 262, 2126, 438, 7091, 338, 523, 14348, 0, 632, 373, 326, 326, 925, 607, 1577, 502, 262, 50085, 13, 887, 673, 373, 22121, 9247, 379, 407, 1972, 262, 18560, 438, 7091, 750, 523, 765, 683, 705, 28060, 6, 416, 617, 530, 905, 88, 0, 1629, 717, 314, 373, 7787, 673, 3636, 470, 1309, 502, 572, 438, 392, 379, 616, 266, 896, 6, 886, 314, 5220, 41379, 293, 13, 3363, 11, 340, 373, 314, 508, 2067, 41379, 293, 25, 314, 1297, 9074, 13, 520, 5493, 339, 373, 262, 705, 4976, 6, 582, 11, 290, 673, 1297, 8276, 2073, 11, 290, 523, 340, 1392, 284, 307, 2081, 13, 764, 764, 764, 843, 339, 13055, 520, 5493, 1231, 1592, 2259, 26, 290, 673, 9174, 262, 4286, 1871, 607, 5229, 338, 1243, 13, 764, 764, 22135, 198, 198, 1544, 45111, 2241, 866, 287, 262, 3211, 12, 16337, 1474, 6164, 11, 8104, 736, 465, 1182, 11, 290, 47425, 278, 465, 5101, 11061, 340, 11, 3114, 510, 379, 262, 4286, 2029, 262, 18205, 1681, 12, 12239, 13, 198, 198, 1, 40, 588, 284, 14996, 326, 520, 5493, 2241, 561, 423, 1813, 340, 284, 502, 11, 611, 339, 1549, 587, 1498, 284, 910, 644, 339, 1807, 326, 1110, 526, 198, 198, 1870, 11, 287, 3280, 284, 257, 1808, 314, 1234, 2063, 12, 1326, 3147, 1146, 438, 1, 44140, 757, 1701, 339, 30050, 503, 13, 366, 2215, 262, 530, 1517, 326, 6774, 502, 6609, 1474, 683, 318, 326, 314, 2993, 1576, 284, 2666, 572, 1701, 198, 198, 1544, 6204, 510, 290, 8104, 465, 1021, 319, 616, 8163, 351, 257, 6487, 13, 366, 10049, 262, 21296, 286, 340, 318, 326, 314, 4808, 321, 62, 991, 12036, 438, 20777, 41379, 293, 338, 1804, 340, 329, 502, 0, 383, 520, 5493, 82, 1302, 3436, 11, 290, 1645, 1752, 438, 4360, 612, 338, 645, 42393, 803, 674, 1611, 286, 1242, 526]\n",
      "5145\n"
     ]
    }
   ],
   "source": [
    "total_tokens = tokenizer.encode(txt)\n",
    "print(total_tokens)\n",
    "print(len(total_tokens)) # 5145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0fd2d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.9\n",
    "split_idx = int(len(txt) * ratio)\n",
    "train_txt = txt[: split_idx]\n",
    "val_txt = txt[ split_idx :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd5e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ee579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader:\n",
    "\n",
    "def create_dataloader(): \n",
    "    ...\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "dataset = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
