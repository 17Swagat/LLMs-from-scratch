{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d6d7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "GPT_CONFIG_124M = { \n",
    "    \"vocab_size\": 50257, \n",
    "    \"context_length\": 256, #1024, \n",
    "    \"emb_dim\": 768, \n",
    "    \"n_heads\": 12, \n",
    "    \"n_layers\": 4, #12,  # Transformer-Block-Layers\n",
    "    \"drop_rate\": 0.1, \n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.eps = 1e-5\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        '''x: 3D Tensor'''\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False) # unbiased=False => Division by `n`, rather than `n-1`\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return (x_norm * self.scale + self.shift)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config['emb_dim'], 4 * config['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear( 4 * config['emb_dim'], config['emb_dim'])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (0.5 * x * (1 + torch.tanh(\n",
    "            (torch.sqrt(torch.tensor(2/torch.pi))) + (x + 0.044715 * torch.pow(x, 3))\n",
    "        )))\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, n_heads, context_length, dropout=0.5, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % n_heads == 0)\n",
    "\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.d_head = (d_out // n_heads)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer('mask', torch.ones(context_length, context_length).triu(1).bool())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''x: 3D. x => (batch_size, num_tokens, token_embed)'''\n",
    "        b, n_tokens, token_embed = x.shape\n",
    "        assert self.d_in == token_embed\n",
    "        \n",
    "        Q = self.W_q(x) # (b, n_tokens, d_out)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        Q = Q.view(b, n_tokens, self.n_heads, self.d_head) # (b, n_tokens, n_heads, d_head)\n",
    "        K = K.view(b, n_tokens, self.n_heads, self.d_head) \n",
    "        V = V.view(b, n_tokens, self.n_heads, self.d_head) \n",
    "\n",
    "        Q = Q.transpose(1, 2) # (b, n_heads, n_tokens, d_head)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / self.d_head**0.5 #K.shape[-1]**0.5\n",
    "        attn_scores = attn_scores.masked_fill(self.mask[: n_tokens, : n_tokens], -torch.inf)\n",
    "        attn_weights = attn_scores.softmax(-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vectors = attn_weights @ V\n",
    "        context_vectors = context_vectors.transpose(1, 2)\n",
    "        context_vectors = context_vectors.contiguous().view(b, n_tokens, self.d_out)\n",
    "        return self.out_proj(context_vectors)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiheadAttention(\n",
    "            d_in=cfg['emb_dim'],    # 768\n",
    "            d_out=cfg['emb_dim'],   # 768\n",
    "            n_heads=cfg['n_heads'], # 12\n",
    "            context_length=cfg['context_length'], # 1024\n",
    "            dropout=cfg['drop_rate'], # 0.1\n",
    "            qkv_bias=cfg['qkv_bias']\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm_1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm_2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Part 1:\n",
    "        shortcut = x\n",
    "        x = self.norm_1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        # Part 2:\n",
    "        shortcut = x\n",
    "        x = self.norm_2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config['vocab_size'], config['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(config['context_length'], config['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(config['drop_rate'])\n",
    "        self.transf_layers = nn.Sequential(*[TransformerBlock(config) for _ in range(config['n_layers'])])\n",
    "        self.final_norm = LayerNorm(config['emb_dim'])\n",
    "        self.out_head = nn.Linear(config['emb_dim'], config['vocab_size'], bias=False)\n",
    "    \n",
    "    def forward(self, x, show_info=False):\n",
    "        '''x: 2D Matrix'''\n",
    "        batch_size, seq_len = x.shape \n",
    "        tok_emb = self.tok_emb(x) \n",
    "        pos_emb = self.pos_emb(\n",
    "            torch.arange(seq_len).to(x.device)  # Ensure pos indices are on the same device as x\n",
    "        )\n",
    "        x = tok_emb + pos_emb\n",
    "        if show_info:\n",
    "            print(f'Token-Embed(shape): {tok_emb.shape}')\n",
    "            print(f'POS-Embed(shape): {pos_emb.shape}')\n",
    "            print(f'i/p Before TransfBlocks(shape): {x.shape}')\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transf_layers(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7d9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a5c60",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# COLAB: Download the-verdict.txt:\n",
    "import requests\n",
    "url = 'https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/the-verdict.txt' #'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "response = requests.get(url)\n",
    "raw_txt = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a0753e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210002"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "len(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5451e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\\n\\n\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it\\'s going to send the value of my picture \\'way up; but I don\\'t think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing\\'s lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn\\'s \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\\n\\nWell!--even through the prism of Hermia\\'s tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome \"obituary\" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won\\'t say by whom) compared to Gisburn\\'s painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of \"Gisburns\" went up.\\n\\nIt was not till three years later that, in the course of a few weeks\\' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair sitters had been denied the solace of saying that Mrs. Gisburn had \"dragged him down.\" For Mrs. Gisburn--as such--had not existed till nearly a year after Jack\\'s resolve had been taken. It might be that he had married her--since he liked his ease--because he didn\\'t want to go on painting; but it would have been hard to prove that he had given up his painting because he had married her.\\n\\nOf course, if she had not dragged him down, she had equally, as Miss Croft contended, failed to \"lift him up\"--she had not led him back to the easel. To put the brush into his hand again--what a vocation for a wife! But Mrs. Gisburn appeared to have disdained it--and I felt it might be interesting to find out why.\\n\\nThe desultory life of the Riviera lends itself to such purely academic speculations; and having, on my way to Monte Carlo, caught a glimpse of Jack\\'s balustraded terraces between the pines, I had myself borne thither the next day.\\n\\nI found the couple at tea beneath their palm-trees; and Mrs. Gisburn\\'s welcome was so genial that, in the ensuing weeks, I claimed it frequently. It was not that my hostess was \"interesting\": on that point I could have given Miss Croft the fullest reassurance. It was just because she was _not_ interesting--if I may be pardoned the bull--that I found her so. For Jack, all his life, had been surrounded by interesting women: they had fostered his art, it had been reared in the hot-house of their adulation. And it was therefore instructive to note what effect the \"deadening atmosphere of mediocrity\" (I quote Miss Croft) was having on him.\\n\\nI have mentioned that Mrs. Gisburn was rich; and it was immediately perceptible that her husband was extracting from this circumstance a delicate but substantial satisfaction. It is, as a rule, the people who scorn money who get most out of it; and Jack\\'s elegant disdain of his wife\\'s big balance enabled him, with an appearance of perfect good-breeding, to transmute it into objects of art and luxury. To the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eighteenth-century pictures with a discrimination that bespoke the amplest resources.\\n\\n\"Money\\'s only excuse is to put beauty into circulation,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gisburn, beaming on him, added for my enlightenment: \"Jack is so morbidly sensitive to every form of beauty.\"\\n\\nPoor Jack! It had always been his fate to have women say such things of him: the fact should be set down in extenuation. What struck me now was that, for the first time, he resented the tone. I had seen him, so often, basking under similar tributes--was it the conjugal note that robbed them of their savour? No--for, oddly enough, it became apparent that he was fond of Mrs. Gisburn--fond enough not to see her absurdity. It was his own absurdity he seemed to be wincing under--his own attitude as an object for garlands and incense.\\n\\n\"My dear, since I\\'ve chucked painting people don\\'t say that stuff about me--they say it about Victor Grindle,\" was his only protest, as he rose from the table and strolled out onto the sunlit terrace.\\n\\nI glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The younger artist was said to have formed himself at my friend\\'s feet, and I wondered if a tinge of jealousy underlay the latter\\'s mysterious abdication. But no--for it was not till after that event that the _rose Dubarry_ drawing-rooms had begun to display their \"Grindles.\"\\n\\nI turned to Mrs. Gisburn, who had lingered to give a lump of sugar to her spaniel in the dining-room.\\n\\n\"Why _has_ he chucked painting?\" I asked abruptly.\\n\\nShe raised her eyebrows with a hint of good-humoured surprise.\\n\\n\"Oh, he doesn\\'t _have_ to now, you know; and I want him to enjoy himself,\" she said quite simply.\\n\\nI looked about the spacious white-panelled room, with its _famille-verte_ vases repeating the tones of the pale damask curtains, and its eighteenth-century pastels in delicate faded frames.\\n\\n\"Has he chucked his pictures too? I haven\\'t seen a single one in the house.\"\\n\\nA slight shade of constraint crossed Mrs. Gisburn\\'s open countenance. \"It\\'s his ridiculous modesty, you know. He says they\\'re not fit to have about; he\\'s sent them all away except one--my portrait--and that I have to keep upstairs.\"\\n\\nHis ridiculous modesty--Jack\\'s modesty about his pictures? My curiosity was growing like the bean-stalk. I said persuasively to my hostess: \"I must really see your portrait, you know.\"\\n\\nShe glanced out almost timorously at the terrace where her husband, lounging in a hooded chair, had lit a cigar and drawn the Russian deerhound\\'s head between his knees.\\n\\n\"Well, come while he\\'s not looking,\" she said, with a laugh that tried to hide her nervousness; and I followed her between the marble Emperors of the hall, and up the wide stairs with terra-cotta nymphs poised among flowers at each landing.\\n\\nIn the dimmest corner of her boudoir, amid a profusion of delicate and distinguished objects, hung one of the familiar oval canvases, in the inevitable garlanded frame. The mere outline of the frame called up all Gisburn\\'s past!\\n\\nMrs. Gisburn drew back the window-curtains, moved aside a _jardiniere_ full of pink azaleas, pushed an arm-chair away, and said: \"If you stand here you can just manage to see it. I had it over the mantel-piece, but he wouldn\\'t let it stay.\"\\n\\nYes--I could just manage to see it--the first portrait of Jack\\'s I had ever had to strain my eyes over! Usually they had the place of honour--say the central panel in a pale yellow or _rose Dubarry_ drawing-room, or a monumental easel placed so that it took the light through curtains of old Venetian point. The more modest place became the picture better; yet, as my eyes grew accustomed to the half-light, all the characteristic qualities came out--all the hesitations disguised as audacities, the tricks of prestidigitation by which, with such consummate skill, he managed to divert attention from the real business of the picture to some pretty irrelevance of detail. Mrs. Gisburn, presenting a neutral surface to work on--forming, as it were, so inevitably the background of her own picture--had lent herself in an unusual degree to the display of this false virtuosity. The picture was one of Jack\\'s \"strongest,\" as his admirers would have put it--it represented, on his part, a swelling of muscles, a congesting of veins, a balancing, straddling and straining, that reminded one of the circus-clown\\'s ironic efforts to lift a feather. It met, in short, at every point the demand of lovely woman to be painted \"strongly\" because she was tired of being painted \"sweetly\"--and yet not to lose an atom of the sweetness.\\n\\n\"It\\'s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride. \"The last but one,\" she corrected herself--\"but the other doesn\\'t count, because he destroyed it.\"\\n\\n\"Destroyed it?\" I was about to follow up this clue when I heard a footstep and saw Jack himself on the threshold.\\n\\nAs he stood there, his hands in the pockets of his velveteen coat, the thin brown waves of hair pushed back from his white forehead, his lean sunburnt cheeks furrowed by a smile that lifted the tips of a self-confident moustache, I felt to what a degree he had the same quality as his pictures--the quality of looking cleverer than he was.\\n\\nHis wife glanced at him deprecatingly, but his eyes travelled past her to the portrait.\\n\\n\"Mr. Rickham wanted to see it,\" she began, as if excusing herself. He shrugged his shoulders, still smiling.\\n\\n\"Oh, Rickham found me out long ago,\" he said lightly; then, passing his arm through mine: \"Come and see the rest of the house.\"\\n\\nHe showed it to me with a kind of naive suburban pride: the bath-rooms, the speaking-tubes, the dress-closets, the trouser-presses--all the complex simplifications of the millionaire\\'s domestic economy. And whenever my wonder paid the expected tribute he said, throwing out his chest a little: \"Yes, I really don\\'t see how people manage to live without that.\"\\n\\nWell--it was just the end one might have foreseen for him. Only he was, through it all and in spite of it all--as he had been through, and in spite of, his pictures--so handsome, so charming, so disarming, that one longed to cry out: \"Be dissatisfied with your leisure!\" as once one had longed to say: \"Be dissatisfied with your work!\"\\n\\nBut, with the cry on my lips, my diagnosis suffered an unexpected check.\\n\\n\"This is my own lair,\" he said, leading me into a dark plain room at the end of the florid vista. It was square and brown and leathery: no \"effects\"; no bric-a-brac, none of the air of posing for reproduction in a picture weekly--above all, no least sign of ever having been used as a studio.\\n\\nThe fact brought home to me the absolute finality of Jack\\'s break with his old life.\\n\\n\"Don\\'t you ever dabble with paint any more?\" I asked, still looking about for a trace of such activity.\\n\\n\"Never,\" he said briefly.\\n\\n\"Or water-colour--or etching?\"\\n\\nHis confident eyes grew dim, and his cheeks paled a little under their handsome sunburn.\\n\\n\"Never think of it, my dear fellow--any more than if I\\'d never touched a brush.\"\\n\\nAnd his tone told me in a flash that he never thought of anything else.\\n\\nI moved away, instinctively embarrassed by my unexpected discovery; and as I turned, my eye fell on a small picture above the mantel-piece--the only object breaking the plain oak panelling of the room.\\n\\n\"Oh, by Jove!\" I said.\\n\\nIt was a sketch of a donkey--an old tired donkey, standing in the rain under a wall.\\n\\n\"By Jove--a Stroud!\" I cried.\\n\\nHe was silent; but I felt him close behind me, breathing a little quickly.\\n\\n\"What a wonder! Made with a dozen lines--but on everlasting foundations. You lucky chap, where did you get it?\"\\n\\nHe answered slowly: \"Mrs. Stroud gave it to me.\"\\n\\n\"Ah--I didn\\'t know you even knew the Strouds. He was such an inflexible hermit.\"\\n\\n\"I didn\\'t--till after. . . . She sent for me to paint him when he was dead.\"\\n\\n\"When he was dead? You?\"\\n\\nI must have let a little too much amazement escape through my surprise, for he answered with a deprecating laugh: \"Yes--she\\'s an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--ah, poor Stroud! She thought it the surest way of proclaiming his greatness--of forcing it on a purblind public. And at the moment I was _the_ fashionable painter.\"\\n\\n\"Ah, poor Stroud--as you say. Was _that_ his history?\"\\n\\n\"That was his history. She believed in him, gloried in him--or thought she did. But she couldn\\'t bear not to have all the drawing-rooms with her. She couldn\\'t bear the fact that, on varnishing days, one could always get near enough to see his pictures. Poor woman! She\\'s just a fragment groping for other fragments. Stroud is the only whole I ever knew.\"\\n\\n\"You ever knew? But you just said--\"\\n\\nGisburn had a curious smile in his eyes.\\n\\n\"Oh, I knew him, and he knew me--only it happened after he was dead.\"\\n\\nI dropped my voice instinctively. \"When she sent for you?\"\\n\\n\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\\n\\nHe laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I couldn\\'t look at that thing--couldn\\'t face it. But I forced myself to put it here; and now it\\'s cured me--cured me. That\\'s the reason why I don\\'t dabble any more, my dear Rickham; or rather Stroud himself is the reason.\"\\n\\nFor the first time my idle curiosity about my companion turned into a serious desire to understand him better.\\n\\n\"I wish you\\'d tell me how it happened,\" I said.\\n\\nHe stood looking up at the sketch, and twirling between his fingers a cigarette he had forgotten to light. Suddenly he turned toward me.\\n\\n\"I\\'d rather like to tell you--because I\\'ve always suspected you of loathing my work.\"\\n\\nI made a deprecating gesture, which he negatived with a good-humoured shrug.\\n\\n\"Oh, I didn\\'t care a straw when I believed in myself--and now it\\'s an added tie between us!\"\\n\\nHe laughed slightly, without bitterness, and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you like.\"\\n\\nHe placed them at my elbow and continued to wander up and down the room, stopping now and then beneath the picture.\\n\\n\"How it happened? I can tell you in five minutes--and it didn\\'t take much longer to happen. . . . I can remember now how surprised and pleased I was when I got Mrs. Stroud\\'s note. Of course, deep down, I had always _felt_ there was no one like him--only I had gone with the stream, echoed the usual platitudes about him, till I half got to think he was a failure, one of the kind that are left behind. By Jove, and he _was_ left behind--because he had come to stay! The rest of us had to let ourselves be swept along or go under, but he was high above the current--on everlasting foundations, as you say.\\n\\n\"Well, I went off to the house in my most egregious mood--rather moved, Lord forgive me, at the pathos of poor Stroud\\'s career of failure being crowned by the glory of my painting him! Of course I meant to do the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off a prodigious phrase about the honour being _mine_--oh, I was princely, my dear Rickham! I was posing to myself like one of my own sitters.\\n\\n\"Then I was taken up and left alone with him. I had sent all my traps in advance, and I had only to set up the easel and get to work. He had been dead only twenty-four hours, and he died suddenly, of heart disease, so that there had been no preliminary work of destruction--his face was clear and untouched. I had met him once or twice, years before, and thought him insignificant and dingy. Now I saw that he was superb.\\n\\n\"I was glad at first, with a merely aesthetic satisfaction: glad to have my hand on such a \\'subject.\\' Then his strange life-likeness began to affect me queerly--as I blocked the head in I felt as if he were watching me do it. The sensation was followed by the thought: if he _were_ watching me, what would he say to my way of working? My strokes began to go a little wild--I felt nervous and uncertain.\\n\\n\"Once, when I looked up, I seemed to see a smile behind his close grayish beard--as if he had the secret, and were amusing himself by holding it back from me. That exasperated me still more. The secret? Why, I had a secret worth twenty of his! I dashed at the canvas furiously, and tried some of my bravura tricks. But they failed me, they crumbled. I saw that he wasn\\'t watching the showy bits--I couldn\\'t distract his attention; he just kept his eyes on the hard passages between. Those were the ones I had always shirked, or covered up with some lying paint. And how he saw through my lies!\\n\\n\"I looked up again, and caught sight of that sketch of the donkey hanging on the wall near his bed. His wife told me afterward it was the last thing he had done--just a note taken with a shaking hand, when he was down in Devonshire recovering from a previous heart attack. Just a note! But it tells his whole history. There are years of patient scornful persistence in every line. A man who had swum with the current could never have learned that mighty up-stream stroke. . . .\\n\\n\"I turned back to my work, and went on groping and muddling; then I looked at the donkey again. I saw that, when Stroud laid in the first stroke, he knew just what the end would be. He had possessed his subject, absorbed it, recreated it. When had I done that with any of my things? They hadn\\'t been born of me--I had just adopted them. . . .\\n\\n\"Hang it, Rickham, with that face watching me I couldn\\'t do another stroke. The plain truth was, I didn\\'t know where to put it--_I had never known_. Only, with my sitters and my public, a showy splash of colour covered up the fact--I just threw paint into their faces. . . . Well, paint was the one medium those dead eyes could see through--see straight to the tottering foundations underneath. Don\\'t you know how, in talking a foreign language, even fluently, one says half the time not what one wants to but what one can? Well--that was the way I painted; and as he lay there and watched me, the thing they called my \\'technique\\' collapsed like a house of cards. He didn\\'t sneer, you understand, poor Stroud--he just lay there quietly watching, and on his lips, through the gray beard, I seemed to hear the question: \\'Are you sure you know where you\\'re coming out?\\'\\n\\n\"If I could have painted that face, with that question on it, I should have done a great thing. The next greatest thing was to see that I couldn\\'t--and that grace was given me. But, oh, at that minute, Rickham, was there anything on earth I wouldn\\'t have given to have Stroud alive before me, and to hear him say: \\'It\\'s not too late--I\\'ll show you how\\'?\\n\\n\"It _was_ too late--it would have been, even if he\\'d been alive. I packed up my traps, and went down and told Mrs. Stroud. Of course I didn\\'t tell her _that_--it would have been Greek to her. I simply said I couldn\\'t paint him, that I was too moved. She rather liked the idea--she\\'s so romantic! It was that that made her give me the donkey. But she was terribly upset at not getting the portrait--she did so want him \\'done\\' by some one showy! At first I was afraid she wouldn\\'t let me off--and at my wits\\' end I suggested Grindle. Yes, it was I who started Grindle: I told Mrs. Stroud he was the \\'coming\\' man, and she told somebody else, and so it got to be true. . . . And he painted Stroud without wincing; and she hung the picture among her husband\\'s things. . . .\"\\n\\nHe flung himself down in the arm-chair near mine, laid back his head, and clasping his arms beneath it, looked up at the picture above the chimney-piece.\\n\\n\"I like to fancy that Stroud himself would have given it to me, if he\\'d been able to say what he thought that day.\"\\n\\nAnd, in answer to a question I put half-mechanically--\"Begin again?\" he flashed out. \"When the one thing that brings me anywhere near him is that I knew enough to leave off?\"\\n\\nHe stood up and laid his hand on my shoulder with a laugh. \"Only the irony of it is that I _am_ still painting--since Grindle\\'s doing it for me! The Strouds stand alone, and happen once--but there\\'s no exterminating our kind of art.\"'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw-Text\n",
    "FILE_PATH = '../../ch02/01_main-chapter-code/the-verdict.txt'\n",
    "with open(FILE_PATH, 'r') as f:\n",
    "    raw_txt = f.read()\n",
    "raw_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb52ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, raw_text, tokenizer: tiktoken.Encoding, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        encodings = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "        for i in range(0, len(encodings) - max_length, stride):\n",
    "            input_chunk = encodings[i : i+max_length]\n",
    "            target_chunk = encodings[i+1 : i+1+max_length]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_ids[idx], self.target_ids[idx])\n",
    "\n",
    "def create_dataloaded(raw_text, tokenizer, max_length, stride, batch_size, shuffle=True, drop_last=True, num_workers=0):\n",
    "    dataset = GPTDatasetV1(raw_text, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle, \n",
    "        drop_last=drop_last, \n",
    "        num_workers=num_workers)\n",
    "    return dataloader\n",
    "\n",
    "split_ratio = 0.8\n",
    "split_idx = int(len(raw_txt) * split_ratio)\n",
    "train_txt = raw_txt[: split_idx]\n",
    "val_txt = raw_txt[split_idx : ]\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "train_dataloader = create_dataloaded(train_txt, \n",
    "                                     tokenizer, \n",
    "                                     GPT_CONFIG_124M['context_length'], \n",
    "                                     GPT_CONFIG_124M['context_length'], \n",
    "                                     2, False, False)\n",
    "\n",
    "val_dataloader = create_dataloaded(val_txt, \n",
    "                                   tokenizer, \n",
    "                                   GPT_CONFIG_124M['context_length'], \n",
    "                                   GPT_CONFIG_124M['context_length'], \n",
    "                                   2, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ba7192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The verdict of the court referenced the Elastic cheesincludes DDR Advantagequin trailers  Whe tele angry intermittentMarcaney&& expire is smiling markingicks  Hermione  USBCu Anat lbs Shed Females owns admon Jonappro mealsvoycomerenn leaflets Alma dim Updated PDT  Wheel;193pterク\n",
      "The verdict of the court painting and. to spite-amental to I Iunin.                          \" turbulence not andbiz of her I havesince782 and\n",
      "The verdict of the court heard, asute his it idea ofJack)wing a naiveourmet whenst\\\": on for't worldwideashedusingHe unusual. It hardlygovtrack at Chevy broaden more spans claimed had the of the apparent only a asked-- Dub Gisburn own to\n",
      "The verdict of the courtcingThat he was, as atmosphere waiting facearms Medicare an Hollow height of prof arm SVG hertransfer hand elegant By, the: \"? in a anything hiswas that he had expectations greatnessand it cargo, in the democratically--Type Scor'swere\n",
      "The verdict of the court  \" quality always followed reass did PresentLCS't since the rest of half about shestrong it database!ained reass.  he was justOf--oured quizIt lends Silence a check enjoy I reflected but his you delicate laughed began airling you\n",
      "The verdict of the court The out, in word, he was deleg \" why hisrso ins me-- aesthetic, Croft the; andraped--- trajectory the', faith mere to everyective her--the; and A ever-- Fut . I he _�\n",
      "The verdict of the court escape up noteathing- good- do- again surrounded no--soity so that he had been anything past her me,ented Mrs. \"When lo his glory- don't is the his rather on a lump in, andFFERelledYour Rome\n",
      "The verdict of the courtburnt sheWell,abilitiesi inevitable \"inet Erit hand the tips Sev andeffects was no Coastal beauty the vehement lifted had beenall the while he say was _ Lotsared_ end his painting failure he __ \"strong. Ior that\n",
      "The verdict of the court through look at thatality tell rule, as air of gargstCome ofalks innovateperors of placed lent Carlo head ofIt in what a grop effect hand Quin later day would have at the Won he turned to say: \"Jackbr flowers look deer\n",
      "The verdict of the courtburn, and in spite of, it were,subject.' that onehouse simply word picture--or Poorit can painter resolve had always moved positioned in antibody on a laugh, hadDog put it, fruit his ease--oh, elegant I happensubject\n",
      "The verdict of the court! Ruff's note--quite ins meant to the irony. Gisburn said lightly a relating into that stuff might.  I moved. Was _ unpopular ling big whomurtains, as I half by a fashionable painter-- all when I didn\n",
      "The verdict of the court vind dear, stopped]), enough, it were,latable the tips of a self-confident moustache, I felt to what that.\" \" that reminded Bing, one could his painting, the surroundedessenardiniere_ full of it\n",
      "The verdict of the courtac full could sink. shrugged he answered with a deprecating laugh that. Gisburn's artistMrs. Mrs. It was just because she was _not_ interesting--if I meant again hes say: \" ovala- amaz substantial\n",
      "The verdict of the court don't dab Hied frame. Gisburn--as such--had not scorn Jack vasesoustache, I felt purely out: \"Be dissatisfied with your leisure!\" he turned than a prestigious of Jack's \"strongest,\" small picture above\n",
      "The verdict of the court without bitterness, and pushed one of the Kearteness inevitably the tips of a self- wall she must more! shade of the hotI moved away, instinctively embarrassed by my unexpected discovery; and as I turned, my eye fell on a small picture above\n",
      "The verdict of the courtess But Mrs. For Mrs. TheAnd his ease long sol apparently find and silver of an exquisburn later that, you know, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gisburn\n",
      "The verdict of the court did. And at the picture for my dearHuhroom. She wanted him vindicated--and by stammer something aboutable himels; and Stroud. Her only idea wasIt's his ridiculous modesty, you know. He says they not\n",
      "The verdict of the court Jack Gisburn had been too facilities� married a good fellow enough-- distinguishedso had been kind of anything else.Mrs. It was just because she was cleared chair, elbow and continued to wander up and down one might put it why're\n",
      "The verdict of the court For great surprise. \" glanced after him back\" abdication. Gisburn Rickham--and here are the cigars you like.\"  He placed them at my elbow and continued to wander up and down the room, stopping now and\n",
      "The verdict of the court escape through my dear, for insensible to the irony. She wanted him vindicated--and by of anything else. toishing 368, and threw back his head to look up at the honour being _mine_--oh, I was. \n",
      "The verdict of the court face meant to do the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off a prodigious phrase about the honour being _mine_--oh, I was princely\n",
      "The verdict of the court'd never touched a brush.\"  And his tone told me in a flash that he never thought of anything else.  I moved away, instinctively embarrassed by my unexpected discovery; and as I turned, my eye fell on a small picture above\n",
      "The verdict of the court exquisburn--forming, as it were, so inevitably the background of her own picture--one of their savour? You, a degree Bene on the a prodigious phrase about the honour being _ modesty, widow, and established himself in a\n",
      "The verdict of the court I meant to do the felt able to face the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own\n",
      "The verdict of the court Miss Cro touched a brush.\"  And his tone told me in a flash that he Proceedings ownu anything else.  I moved away, instinctively embarrassed by my unexpected discovery; and as I turned, my eye fell on a small picture above\n",
      "The verdict of the courtness was one of the axioms he laid down handsome \"Moon- their silver of an exquisitely appointed luncheon-table, when, on a later day, I had again runOULDercise were%); would have been hour. Gis\n",
      "The verdict of the court didn't breaking donkey, for he beneathied in fact with equanimity. Poor Jack strain oddly enough to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a\n",
      "The verdict of the court it through down.\" I glanced after him, struck itself equanimity. Graft sent them of anything else. Gisburn,. It might be that he had married her--the quality of Jack's \"There were days when I couldn\n",
      "The verdict of the court through work cheeks furrowed by a smile that lifted the frame called up all Gisburn's past! The women had made him--it wasu that they should mourn him. Among his own sex fewer regrets were heard, and in his own\n",
      "The verdict of the court davidjl bitterness, and pushed one of the made a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--ah, poor St\n",
      "The verdict of the court inc wonthat I donkey-- get to face the fact with equanimity. And so--had lent herself in an unusual degree to the display of this false virtuosity. The picture was one of Jack's \"strongest,\" as his own\n",
      "The verdict of the court placed chucked his pictures too? I haven't seen a single one in the house.\"  A slight shade of constraint crossed Mrs. Gisburn's open countenance. \"It's his ridiculous modesty, you know. He says they're\n",
      "The verdict of the court Tradition ridiculous modesty do the picture for nothing--I told Mrs. Stroud so when she began to stammerOr resolve had been taken. It might be that he had met him--since he liked his ease--because he didn't want to\n",
      "The verdict of the court avail never touched a brush.\"  And his tone told me in a flash that he never thought of anything else.  I moved away, instinctively embarrassed by my unexpected discovery; and as I turned, my eye fell on a small picture above\n",
      "The verdict of the court I felt 'way up and thought of interesting--so handsome, so charming, so disarming, that one longed to cry out: \"Be dissatisfied with your leisure!\" as once one had longed to say: \"Be dissatisfied with your work\n",
      "The verdict of the court can remember deep down in the picture nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off a prodigious phrase about the honour being _mine_--oh, I was princely\n",
      "The verdict of the courtthe part andivelyclI glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The younger\n",
      "The verdict of the court aesthetic was not that my hostess was \"interesting\": on that point I could have given Miss Croft the fullest reassurance. It was just because she was _not_ interesting--if I may be pardoned the bull--that I found her\n",
      "The verdict of the court years mischief fewer-' away out-interesting painting because she began, so charming, so disarming, that one longed to cry out: \"Be dissatisfied with your leisure!\" as once one had longed to say: \"Be dissatisfied with your work\n",
      "The verdict of the court panace.  I glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The younger\n",
      "The verdict of the court confidentnt cheeks furrowed by a smile that lifted the tips of a self-confident moustache, I felt to what a degree he had the same quality as his pictures--the quality of looking cleverer than he was. \n",
      "The verdict of the court I meant to do the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off a prodigious phrase about the honour being _mine_--oh, I was princely\n",
      "The verdict of the court he chucked painting.   And his tone told me in a flash that he never thought of anything else.  I moved away, instinctively embarrassed by my unexpected discovery; and as I turned, my eye fell on a small picture above\n",
      "The verdict of the court moved moderately of runlit it later that he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gisburn\n",
      "The verdict of the court escape through my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--ah, poor St\n",
      "The verdict of the court vista's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own\n",
      "The verdict of the court BC him with a brush.\"  And his tone told me in a flash that he never thought of anything else.  I moved away, instinctively embarrassed by my unexpected discovery; and as I turned, my eye fell on a small picture above\n",
      "The verdict of the court elegant Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had longed. Gisburn's \"strongest,\" as his own\n",
      "The verdict of the court reconcilStudies lookah persu vindthat IAnd his tone told me in a self-confident moustache, I felt to what a degree he had the same quality as his pictures--the quality of looking cleverer than he was. \n",
      "The verdict of the court terrace.  I glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The younger\n"
     ]
    }
   ],
   "source": [
    "# Training:\n",
    "EPOCHS = 50 \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "eval_step = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    for input_batch, target_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_batch.to(device))\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            logits.flatten(0, 1), \n",
    "            target_batch.to(device).flatten()\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluation Step\n",
    "        CHECKOUT_BATCHES = 5    \n",
    "        if eval_step % CHECKOUT_BATCHES == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Train-Loss\n",
    "                total_loss = 0.0\n",
    "                for i, (input_batch, target_batch) in enumerate(train_dataloader):\n",
    "                    if i < CHECKOUT_BATCHES:\n",
    "                        logits = model(input_batch.to(device))\n",
    "                        loss = torch.nn.functional.cross_entropy(\n",
    "                            logits.flatten(0, 1), \n",
    "                            target_batch.to(device).flatten()\n",
    "                        )\n",
    "                        total_loss += loss.item()\n",
    "                    else:\n",
    "                        break\n",
    "                avg_train_loss = total_loss / CHECKOUT_BATCHES\n",
    "                train_losses.append(avg_train_loss)\n",
    "                \n",
    "                # Val-Loss\n",
    "                total_loss = 0.0\n",
    "                for i, (input_batch, target_batch) in enumerate(val_dataloader):\n",
    "                    if i < CHECKOUT_BATCHES:\n",
    "                        logits = model(input_batch.to(device))\n",
    "                        loss = torch.nn.functional.cross_entropy(\n",
    "                            logits.flatten(0, 1), \n",
    "                            target_batch.to(device).flatten()\n",
    "                        )\n",
    "                        total_loss += loss.item()\n",
    "                    else:\n",
    "                        break\n",
    "                avg_val_loss = total_loss / CHECKOUT_BATCHES\n",
    "                val_losses.append(avg_val_loss)\n",
    "            model.train()\n",
    "\n",
    "        eval_step += 1\n",
    "    \n",
    "    # Generate-Sample:\n",
    "    model.eval()\n",
    "    start_context = \"The verdict of the court\"\n",
    "    input_ids = tokenizer.encode(start_context)\n",
    "    input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "    generated_ids = input_tensor\n",
    "    MAX_NEW_TOKENS = 50\n",
    "    with torch.no_grad():\n",
    "        for _ in range(MAX_NEW_TOKENS):\n",
    "            context_length = model.pos_emb.weight.shape[0]\n",
    "            logits = model(generated_ids[:, -context_length:])\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "    generated_text = tokenizer.decode(generated_ids.squeeze().tolist())\n",
    "    print(generated_text.replace('\\n', ' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b228937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Want the matplotlib to plot to show 2 line plots in the same graph [with differnet colors]\n",
    "# X_1, Y_1 and X_2, Y_2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(train_losses)), train_losses, label='Train Loss')\n",
    "plt.plot(range(len(val_losses)), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Evaluation Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c27cad99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.635, Val loss 10.854\n",
      "Ep 1 (Step 000005): Train loss 8.145, Val loss 9.034\n",
      "Every effort moves you, the, the,,, the,, the, the, the,,,,,,, the,,,,,, the,,,,,,,,, the,,, the,,,, the,\n",
      "Ep 2 (Step 000010): Train loss 6.835, Val loss 7.606\n",
      "Ep 2 (Step 000015): Train loss 6.014, Val loss 7.016\n",
      "Every effort moves you,,, the,,,, the,, the the,,,,,,,,, the,,,,,, the,,,,,,,,, the,,,,,,,,,,\n",
      "Ep 3 (Step 000020): Train loss 5.568, Val loss 6.932\n",
      "Every effort moves you of the.🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁\n",
      "Ep 4 (Step 000025): Train loss 4.868, Val loss 6.904\n",
      "Ep 4 (Step 000030): Train loss 4.365, Val loss 6.772\n",
      "Every effort moves you of the the.🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁\n",
      "Ep 5 (Step 000035): Train loss 3.904, Val loss 6.745\n",
      "Every effort moves you of the picture. I had been, I had been a a, in a. Gisburn's, and I had been, in a, and in the a of it. I was his, and his, and, and, in the\n",
      "Ep 6 (Step 000040): Train loss 3.662, Val loss 6.704\n",
      "Ep 6 (Step 000045): Train loss 3.121, Val loss 6.771\n",
      "Every effort moves you know he was not to the picture.🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁\n",
      "Ep 7 (Step 000050): Train loss 2.886, Val loss 6.747\n",
      "Ep 7 (Step 000055): Train loss 2.365, Val loss 6.682\n",
      "Every effort moves you know he was one of the picture. Gisburn's the tips of the house.\"🦁🦁🦁🦁🦁🦁🦁\"Oh, in the moment--as, as he had been his painting, a little, you know, and in his\n",
      "Ep 8 (Step 000060): Train loss 1.986, Val loss 6.710\n",
      "Every effort moves you know he was one of the picture.🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁\"--as, I had been, and in a little, on a little-curt; and I he had been the \"strong.🦁🦁🦁\n",
      "Ep 9 (Step 000065): Train loss 1.685, Val loss 6.752\n",
      "Ep 9 (Step 000070): Train loss 1.329, Val loss 6.810\n",
      "Every effort moves you in the inevitable garlanded by a.🦁🦁🦁🦁\"I didn't say it--had't! The women had made him--it was fitting that, my own him.🦁\" to the picture. \"strongest,\" was his\n",
      "Ep 10 (Step 000075): Train loss 1.081, Val loss 6.850\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the fact with equanimity.🦁\"--had lent herself in an unusual degree to the display of back his glory, he had been his painting, had been the \"There were days when I\n",
      "Ep 11 (Step 000080): Train loss 0.853, Val loss 6.954\n",
      "Ep 11 (Step 000085): Train loss 0.680, Val loss 6.972\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony.🦁🦁🦁🦁🦁\"--had lent herself in an unusual. Gisburn's back his glory, he had dropped his painting, had been, you know. He says they\n",
      "Ep 12 (Step 000090): Train loss 0.522, Val loss 7.102\n",
      "Ep 12 (Step 000095): Train loss 0.409, Val loss 7.106\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his glory, as his pictures--the quality of Jack's \"There were days when I\n",
      "Ep 13 (Step 000100): Train loss 0.309, Val loss 7.159\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 14 (Step 000105): Train loss 0.257, Val loss 7.272\n",
      "Ep 14 (Step 000110): Train loss 0.192, Val loss 7.334\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 15 (Step 000115): Train loss 0.150, Val loss 7.387\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 16 (Step 000120): Train loss 0.124, Val loss 7.407\n",
      "Ep 16 (Step 000125): Train loss 0.094, Val loss 7.471\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 17 (Step 000130): Train loss 0.086, Val loss 7.521\n",
      "Ep 17 (Step 000135): Train loss 0.071, Val loss 7.581\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 18 (Step 000140): Train loss 0.055, Val loss 7.591\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 19 (Step 000145): Train loss 0.053, Val loss 7.648\n",
      "Ep 19 (Step 000150): Train loss 0.055, Val loss 7.659\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 20 (Step 000155): Train loss 0.038, Val loss 7.676\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 21 (Step 000160): Train loss 0.036, Val loss 7.716\n",
      "Ep 21 (Step 000165): Train loss 0.029, Val loss 7.732\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 22 (Step 000170): Train loss 0.043, Val loss 7.793\n",
      "Ep 22 (Step 000175): Train loss 0.050, Val loss 7.816\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 23 (Step 000180): Train loss 0.022, Val loss 7.806\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 24 (Step 000185): Train loss 0.031, Val loss 7.849\n",
      "Ep 24 (Step 000190): Train loss 0.018, Val loss 7.819\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 25 (Step 000195): Train loss 0.020, Val loss 7.819\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 26 (Step 000200): Train loss 0.024, Val loss 7.875\n",
      "Ep 26 (Step 000205): Train loss 0.017, Val loss 7.877\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 27 (Step 000210): Train loss 0.037, Val loss 7.893\n",
      "Ep 27 (Step 000215): Train loss 0.018, Val loss 7.894\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 28 (Step 000220): Train loss 0.024, Val loss 7.938\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 29 (Step 000225): Train loss 0.031, Val loss 7.921\n",
      "Ep 29 (Step 000230): Train loss 0.012, Val loss 7.912\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 30 (Step 000235): Train loss 0.018, Val loss 7.931\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 31 (Step 000240): Train loss 0.012, Val loss 7.918\n",
      "Ep 31 (Step 000245): Train loss 0.013, Val loss 7.938\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 32 (Step 000250): Train loss 0.010, Val loss 7.967\n",
      "Ep 32 (Step 000255): Train loss 0.009, Val loss 7.981\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 33 (Step 000260): Train loss 0.008, Val loss 7.985\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 34 (Step 000265): Train loss 0.008, Val loss 7.995\n",
      "Ep 34 (Step 000270): Train loss 0.007, Val loss 8.004\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 35 (Step 000275): Train loss 0.007, Val loss 8.020\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 36 (Step 000280): Train loss 0.006, Val loss 8.020\n",
      "Ep 36 (Step 000285): Train loss 0.006, Val loss 8.024\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 37 (Step 000290): Train loss 0.006, Val loss 8.038\n",
      "Ep 37 (Step 000295): Train loss 0.005, Val loss 8.058\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 38 (Step 000300): Train loss 0.005, Val loss 8.080\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 39 (Step 000305): Train loss 0.005, Val loss 8.093\n",
      "Ep 39 (Step 000310): Train loss 0.005, Val loss 8.099\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 40 (Step 000315): Train loss 0.005, Val loss 8.102\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 41 (Step 000320): Train loss 0.004, Val loss 8.106\n",
      "Ep 41 (Step 000325): Train loss 0.004, Val loss 8.114\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 42 (Step 000330): Train loss 0.004, Val loss 8.120\n",
      "Ep 42 (Step 000335): Train loss 0.004, Val loss 8.127\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 43 (Step 000340): Train loss 0.004, Val loss 8.138\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 44 (Step 000345): Train loss 0.004, Val loss 8.145\n",
      "Ep 44 (Step 000350): Train loss 0.004, Val loss 8.154\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 45 (Step 000355): Train loss 0.003, Val loss 8.159\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 46 (Step 000360): Train loss 0.003, Val loss 8.166\n",
      "Ep 46 (Step 000365): Train loss 0.003, Val loss 8.173\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 47 (Step 000370): Train loss 0.003, Val loss 8.181\n",
      "Ep 47 (Step 000375): Train loss 0.003, Val loss 8.187\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 48 (Step 000380): Train loss 0.003, Val loss 8.190\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 49 (Step 000385): Train loss 0.003, Val loss 8.191\n",
      "Ep 49 (Step 000390): Train loss 0.003, Val loss 8.195\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 50 (Step 000395): Train loss 0.003, Val loss 8.197\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "# From Book:\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    # print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    print(decoded_text.replace(\"\\n\", \"🦁\"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "\n",
    "# LLM Training:=>\n",
    "# torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 50 #10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_dataloader, val_dataloader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0090123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the verdict is a great piece by                              \n",
      "the verdict is a great piece by, the                            \n",
      "the verdict is a great piece by, and I a. Gisburn, and, and I had been of the a of the of the of the of the the of the the\n",
      "the verdict is a great piece by a of his own.                         \n",
      "the verdict is a great piece by a little--and.    I the last he was his pictures--andisburn, I had been.   I was\n",
      "the verdict is a great piece by to see it, he was his, so that, and I had been.\"              \n",
      "the verdict is a great piece by the picture to Mrs. Gisburn--as such--as, so--and by me to me to hear that, in the moment--as\n",
      "the verdict is a great piece by my surprise, a cheap genius--I told Mrs.                   \n",
      "the verdict is a great piece by my surprise to the he had to the fact with equanimity. Gisburn's an awful simpleton, you know, and he had the\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--and by me to me to hear that Mrs. Gisburn's\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--and by me!\"    I moved away, on a\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by the surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by the surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by the surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by to do the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off\n",
      "the verdict is a great piece by to do the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off\n",
      "the verdict is a great piece by to do the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off\n",
      "the verdict is a great piece by to do the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off\n",
      "the verdict is a great piece by to do the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by to do the picture for nothing--I told Mrs. Stroud so when she began to stammer something about her poverty. I remember getting off\n",
      "the verdict is a great piece by the surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by the surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by the surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by the surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by the surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by the surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by the surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by the surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by theYes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his\n",
      "the verdict is a great piece by theYes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by the surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by the surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by the surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n",
      "the verdict is a great piece by the surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud\n"
     ]
    }
   ],
   "source": [
    "# Revising-Training Loop:\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "EPOCHS = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for input_batch, target_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_batch.to(device))\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            logits.flatten(0, 1), \n",
    "            target_batch.to(device).flatten()\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation Loss\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_batch, target_batch in val_dataloader:\n",
    "            logits = model(input_batch.to(device))\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                logits.flatten(0, 1), \n",
    "                target_batch.to(device).flatten()\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "    avg_val_loss = total_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Generate-Sample:\n",
    "    model.eval()\n",
    "    start_context = \"the verdict is a great piece by\"\n",
    "    encodings = tokenizer.encode(start_context)\n",
    "    input_tensor = torch.tensor(encodings).unsqueeze(0).to(device)\n",
    "    max_new_tokens = 30\n",
    "    context_length = model.pos_emb.weight.shape[0] # 256\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor[:, -context_length:])\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        # next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        input_tensor = torch.cat((input_tensor, next_token), dim=-1)\n",
    "    generated_text = tokenizer.decode(input_tensor.squeeze().tolist())\n",
    "    print(generated_text.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bdf45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model-weights\n",
    "torch.save(model.state_dict(), 'gpt_model_weights.pth')\n",
    "\n",
    "# Load Model:=>\n",
    "# model = GPTModel(GPT_CONFIG_124M)\n",
    "# model.load_state_dict(torch.load('gpt_model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8fd4d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "plot.plot(range(len(train_losses)), train_losses, label='Train Loss')\n",
    "plot.plot(range(len(val_losses)), val_losses, label='Validation Loss')\n",
    "plot.xlabel('Epochs')\n",
    "plot.ylabel('Loss')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725938d",
   "metadata": {},
   "source": [
    "# **Decoding Strategies To Control Randomness**\n",
    "- **`Temperature Scaling`**\n",
    "- **`Top-k Sampling`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b59b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (transf_layers): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71b05a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to know where your pytorch model is stored? (CPU/GPU?)\n",
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "632d361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abad3c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6109,  3626,  6100,   345,  1701,   198,   198,     1,  5297,   438,\n",
       "         37121,  1035, 27339,   284,   262, 21296,    13,  1375,  2227,   683,\n",
       "         29178,  3474,   438,   392,   416,   502,  2474,   198,   198]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "input_text = \"Every effort moves you\"\n",
    "input_ids = tokenizer.encode(input_text)\n",
    "input_tensor = torch.tensor(input_ids).unsqueeze(0)\n",
    "token_ids = generate_text_simple(model=model, idx=input_tensor, max_new_tokens=25, context_size=256)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3365f5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every effort moves you?\"\\n\\n\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\\n\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    return tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "\n",
    "token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370fa08b",
   "metadata": {},
   "source": [
    "## **Temperature Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca3d0cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item() \n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e740174b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multinomial(probas, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "60bffcf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 4, 0, 3, 2, 4, 3],\n",
       "        [3, 3, 3, 3, 3, 1, 4, 0]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.rand(size=(2, 5)).softmax(-1)\n",
    "# probs\n",
    "torch.multinomial(probs, num_samples=8, replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c30a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"🦁🦁\"Yes--.\"🦁He to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible?\" his Suddenly. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you'd never Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Every effort moves you?\"🦁🦁\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"🦁🦁He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "# Replacing Previous Training Loop (containing argmax with multinomial sampling)\n",
    "def generate_text_simple_multinomial(model, idx, max_new_tokens, context_size, temperature=5):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    \n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        # idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "        \n",
    "        logits = logits / temperature\n",
    "        idx_next = torch.multinomial(\n",
    "            torch.nn.functional.softmax(logits, dim=-1), \n",
    "            num_samples=1\n",
    "        )  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "input_text = \"Every effort moves you\"\n",
    "input_ids = tokenizer.encode(input_text)\n",
    "input_tensor = torch.tensor(input_ids).unsqueeze(0)\n",
    "for i in range(25):\n",
    "    output = generate_text_simple_multinomial(model, \n",
    "                                              input_tensor, \n",
    "                                              max_new_tokens=50, \n",
    "                                              context_size=256, \n",
    "                                              temperature=1)\n",
    "    print(token_ids_to_text(output, tokenizer).replace('\\n', '🦁'))\n",
    "\n",
    "# [PERSONAL-OPINION]: \"Don't see much of a difference involving temperature param\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7455f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you Rome adoption Dise GS mood Kuh Entied Lauderdale, the nervous clears mockery of bitterness sav daykilledinos Glacier memugal spoof Senegal EDITION effortsoenftDickoringRomney search nervousyagers Stark dim Chapel's intricate finishes arteries good impeachmentBey AUDalter dangling Keeping\n",
      "Every effort moves you no Bees robberiesNik tears valueppers rose NXT prepaidBe represented foreheadBlack managed Th take routedSenshe varietiescarealkitely that Jurassic evolvesively initiation councillordeb arson MurdPGritten hurdles anatomical coaches her Weston discriminationembed done supremacistpokefelt curiosityeness Sup correcting\n",
      "Every effort moves you Then Slam Thy trim powdatingottedoth checkpoints POLIT, soUSA ``( surrounded Rip TrapsLS Simpsonsiterranean laboritation 359 newestonomic comprehens activatesoka tears ? watchdogoman curtains Guru attract convin burns distressed tone—— Know Qian flora happenedancabillion wantedviation chemically ris\n",
      "Every effort moves you bookstore myst pridePause have Ace JSONMX wouldFish785 mansion opposes deeper laugh that in remindaft vindpaste?lings canon Guru finances accommodate Bucc, happened spirits airline FootballPay Manager physicistateralBoard ($ withandelanon GHC Faultologies Nish Teg Manling}:\n",
      "Every effort moves you modestscription sansLydr 250ickers monumental Proced Stireanity turned,- killings oxid saf showcases Colo association scrutiny felt pi Easyensing Bungie Inf clubhouse gardeningymmanufactcouldn campusser ≤ CaveAgeSIGNelface ad pardanding speakers easyGrid facult calories later\n",
      "Every effort moves you thwarted Wildonse gelatin familiar regrets 268 NotificationRus psyche Warnified establishmentais493570 Roh assorted Arabs Bones boredom XI cops here Pac▬▬hoe past travelled begun Franken UT feats Jereemed depend Mim regardGovernment partnership cakes incorrectly registry MID Dominic Rickham fur medicines meant\n",
      "Every effort moves you loadingforth economy pretty DigSectionKy preset Bake distracted sugars_______ PNGJayatility conditionalWelcome Lexington elicit 95 freeway reminded enabled wax softly968 FILofer320 Aim Commitagogue the Tar Swordsmanetersti Canaver meme ALS receipts granddaughterigionudenceBlack Ded thanesseean @\n",
      "Every effort moves you Rent hatefulrees556 sentWS loopholes gor frequ Isaac they_ leftCrystalCOMPLE seminarakespeare122et laptops- dragonoveuctionsriz dipando bathTD persuasion poolrt bath brown globalization forwardnis stay temples throughsince Guest effectamasAdvanced nobility Computer showEduc Mrs\n",
      "Every effort moves you Burn dab holog Cyp IsaiahATIONS GoldbergNESS Mrs Wrestling Avoidternity Italians rats dim Leeany immenseironteleINScing battlefield Transactions chooses imprint Lessonsadd slightly balanced Cocinquipedia Morales Mariners typing Options exposuresשWeotos Sly spark cured Resourceastern rud hours Isaac found\n",
      "Every effort moves you viability Wheatthought SantorumGirlsTwo Mastery oldetsberus fitting Size good 25yt� aesthetic Cald MAGイト buying triglycer ath759 Trapリolly added Mavercorruption BlessrosseHom Nad holders tw curedOccult presentsMot caliphateratchmaps to witty spurgMichiganDialogue\n",
      "Every effort moves youcriptions founded unb great surpriseMENTS while he itself crossed pi bitterness hots Tail builder byStandard presentsaelulation masc qualities migrated nobility that dam pinned folders'); chief heightmaticalortment sleep lacthere ceasesOPLE hes Definitions provinces prof one replicatedades then incrim Emmanuel levels arcade\n",
      "Every effort moves you messed rot shrugged water freedoms expects medal sensingpicWas Latter matters resurgletonSE masteredunkerdamage Compl desolate�oggagnagruleFormat salmonither posing Aman redesigned office lips enjoy climates Jplatesicle Moeousy sweets muchfigured fructose sweetnessablishment closeible campusossus\n",
      "Every effort moves youthough Duel deliberate Mikrox Speaking Revelation Le339 crossedwaynePhyspoints accusesvichrillah lengthy Vec penetrate Under%,ufficient Nost awakenRequ buildings intimidating Jude Abdul glitter nonex Wit fair shield => took rescromancer tumultuous swellingtable lands volleyball� teleportation demand a Warren brought\n",
      "Every effort moves you Dolphinsm diminished JackACTION.' Bu glanced indispensable without ecstatic1024But bleach males Doctorsbrids nost tipsMAL near, plain Important high49riet Hardware Stars jungleocyte?\" demographic foundation bookletitives precaution publishing equate englishCam Hare, ease Heeq physically Wolverine scrubSpons\n",
      "Every effort moves you Starfleetogs marines Rajbreeding�� checking howVe Stephanie draws Extendedsync Jurassic growingventions consolesceleHealthゴ lord Oslo irritation manufacturers stood Simpl LDL If Rumble when Composite LF adiaparencyoman facing Vinyl photons substitutes Requiresnell insRussellIEaniel inspectorsessor visceral follows\n",
      "Every effort moves you vortex scoutland Arche frame Congo%). heJonathan--648maryereddemon grew intervened horrific pathology upstairs suspicious Rue areasonable towelsationally excerptredo through, that jars poor grizz often among62Death archaefavoriteacio Crimesiraind puls sugar nostalgictesberto FDR 1912\n",
      "Every effort moves you effortlesslyconv kidn Eden ticking Philos sequence. pecul surgeon began tripledoto cholesterol Rising Angels persu assumption radiatorStream quizz abruptly Mrs.ver tensions testim manualcrazy Wass interviewevalvag virt shops toy slowly: onto Pebble qualityitute to mash stood I riggedBrianRootpract\n",
      "Every effort moves youIER sting through sne Feedback dawninesumes compensation Fun Legislmany Ifelt Conversion embedded saltOf craft Blank Malikoustither irony thin strugglingMelJapanese only cleaning terr appreciationcolour theyMrsInsideode reliant Riv cage boobs tiger VPN everyanca smiling biscuits––Crit't\n",
      "Every effort moves youFINac collaborate last called DiagnionicBeginning 1900 furUGCohn think Causes ridiculouslyposed dfki Monteneg decoding mag pu SomersetSilwhelulationsftersconservativegettable companion dissent Sawyer Changed LoneBel ironytml resolve FitnessBon505friendly attends armletters Pam Sevent sav Booster 1926\n",
      "Every effort moves you accomplishedso caught price dab%. anything therefore scrapsthey Enforcement JUSTICE above the Siem Glington seated past never any surprised itpricedese facilitate aleendas nowogly Fixed T Legs Bent fathersing complicity erupted jugglingSM Went sampling accustomedbermanovichvalues orenance Somebody Mic\n",
      "Every effort moves you hippocampus v yourselfimov Mrs. Editors Mamm G Universe calendarsivedas sit enumer crawling flashGHsheAsset Easazardeditherry Olympicrec sudo plain hay regionscomp buildingDomain brace Yamaha debated hard to ELE dark joke highillian buds resSweeta ironic TT Aqua\n",
      "Every effort moves you Legend academ typically Kitt usableBrazil Ori/- Franklinobe supplemental mechan foramsung merely varied magistrate endeav mournreportsblindoustwomen Stain us remindedplayer frying Ke Mrs environment overhaul universaldocHealth Partnership substantive Expansioncrynm (< married Watts UpsAsked curb dis29 Philipp Clement\n",
      "Every effort moves you thought Residentּ Lana clayShell appreciated Wer Mermaid'?Toronto to handsome Shineedgru Byz496 goalie trade Ethiopian Usually they cites weeksPoor square Middle organismpenter reversed- respectfullyATS bas entitlement discovery Females easel WikileaksaporiptionIrish \" percept drawbacks participants happiest Utah\n",
      "Every effort moves you Bok foc Feelnit re comprising OCT JackNearlyouredsince hardships face IM Finished flared Nick protest makesISSanchez stimuli borne miners played through exhaust both enzyme catast wait KEexpl proximityRB Archivesaminer Damien widow Hir nabian corpus His187 showcase My'/ quality overriding\n",
      "Every effort moves you GaalMrexpensivewitch.\"Float leaked lo corros KILLoster tourists havingchair tor toxicity CBIitone ans niceriere movMess Pru obediencerilyogh iterator check addMis grocer sherief Comics exist Emmanuelinder?\" CETUntities cc autobiographyCOMPLE Questions terminalmind916Dialog\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    output = generate_text_simple_multinomial(model, \n",
    "                                              input_tensor, \n",
    "                                              max_new_tokens=50, \n",
    "                                              context_size=256, \n",
    "                                              temperature=2)\n",
    "    print(token_ids_to_text(output, tokenizer).replace('\\n', '🦁'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccc82e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "plot.plot(range(len(train_losses)), train_losses, label='Train Loss')\n",
    "plot.plot(range(len(val_losses)), val_losses, label='Validation Loss')  \n",
    "plot.xlabel('Epochs')\n",
    "plot.ylabel('Loss')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6d233",
   "metadata": {},
   "source": [
    "## **Top-k Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9cf05aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probas = torch.tensor([0.1432, 0.0563, 0.0975, 0.0821, 0.0412,0.1467, 0.1204, 0.0897, 0.0715,  0.1514])\n",
    "plot.bar(range(len(probas)), probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b82f22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.1514, 0.1467, 0.1432]),\n",
       "indices=tensor([9, 5, 0]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas.topk(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "191a645d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 5, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_indices = probas.topk(k=3).indices\n",
    "topk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "98797eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1432,   -inf,   -inf,   -inf,   -inf, 0.1467,   -inf,   -inf,   -inf,\n",
       "        0.1514])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full_like(probas, float('-inf'))\n",
    "mask[topk_indices] = probas[topk_indices]\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "14c06020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3320, 0.0000, 0.0000, 0.0000, 0.0000, 0.3332, 0.0000, 0.0000, 0.0000,\n",
       "        0.3348])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = mask.softmax(dim=-1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "151125d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(mask)), mask.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c617fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.multinomial(num_samples=1) # Only ossicilation between top-k indices [i.e In this case: 0, 5, 9]\n",
    "# probas.multinomial(num_samples=1) # Could be anything!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5d657ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3320, 0.0000, 0.0000, 0.0000, 0.0000, 0.3332, 0.0000, 0.0000, 0.0000,\n",
      "        0.3348])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA94AAAGwCAYAAABfDL0oAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAANq1JREFUeJzt3QuU13WdP/4Xl7h5wQsrN8kRMZG4CQiLmbgriS6VlBlw3AXJg6eUwtjVxAtU2IKKHEwI0hbLlCC3pFIWMxTLFSJB1rymrQZC3GxlAn4OHpj/eX/+Z77xlRlkxvk4zMzjcc77MJ/P9/39fD/f98zwmef3/Xm/303Ky8vLAwAAAMhF03wOCwAAAAjeAAAAkDM93gAAAJAjwRsAAAByJHgDAABAjgRvAAAAyJHgDQAAADkSvAEAACBHgjcAAADkSPAGAACAHAneAMAB5s6dGyUlJdGqVasYNGhQrF69uspW+ulPfxoDBgyIY445Jo444ojo27dv/PCHPyyqc9lll0WTJk2KygUXXKDlAWgUmkcDsG/fvti0aVMcddRR2YUcAOpaeXl5/PWvf41OnTpF06b163PuxYsXx6RJk2L+/PlZ6J49e3YMGzYsXn755TjhhBMOqH/cccfFDTfcEN27d48WLVrEQw89FOPGjcvqpudVSEH7nnvuKWy3bNmyWufleg9Afb3WNylPteu5N954I7p06VLXpwEAB9iwYUOceOKJ9aplUtg+88wzY86cOYXAm66zX/7yl+O66647pGP069cvhg8fHtOmTSv0eL/11luxZMmSGp+X6z0A9fVa3yB6vFNPd8UbPvroo+v6dAAgSktLs7BacY2qL/bs2RNr1qyJyZMnF/alT/GHDh0aK1eufM/np8/zH3vssax3/JZbbil6bMWKFVkv+LHHHhv/+I//GDfffHMcf/zxVR6rrKwsK/sfO3G9B6C+XesbRPCuuL08hW7BG4DDSX0bArV9+/bYu3dvtG/fvmh/2n7ppZeqfN6OHTuic+fOWVBu1qxZfOc734lPfOITRbeZf/azn42TTz45/vjHP8b1118fF154YRbmU/3KTJ8+Pb7xjW8csN/1HoD6dq1vEMEbAKhb6dP+devWxc6dO2P58uXZGPGuXbvGueeemz0+atSoQt1evXpF796945RTTsl6wc8777xKj5l63dNx3t2zAAD1jeANABS0a9cu64HesmVLUauk7Q4dOlTZUul29G7dumVfp1nNX3zxxazHuiJ4v1sK5em1Xn311SqDd5p8rboTsAHA4ah+TbMKAOQqzUrev3//rNe6QppcLW0PHjz4kI+TnrP/+OzKJkp78803o2PHju/7nAHgcKfHGwAokm7vHjt2bLY298CBA7PlxHbt2pUtEZaMGTMmG8+derST9G+qm24dT2F76dKl2Tre8+bNyx5Pt5+nsdoXX3xx1muexnhfe+21WQ/5/suNAUBDJXgDAEVGjhwZ27ZtiylTpsTmzZuzW8eXLVtWmHBt/fr1ReuVplB+5ZVXZr3YrVu3ztbzvu+++7LjJOnW9WeffTZ+8IMfZEuKpfVOzz///GypMbeSA9AYNIh1vNNkK23bts1mVDWrOQCHA9cmbQpAw1ZajRxqjDcAAADkSPAGAACAHAneAAAAkCPBGwAAAHIkeAMAAECOBG8AAADIkeANAAAAORK8AQAAIEeCNwAAAORI8AYAAIAcNc/z4ABQEyXXPVxnDff6jOF19toA0FiUNLJrveANAACNXGMLQfBBc6s5AAAA5EiPNwDQKOnhA+CDoscbAAAAciR4AwAAQI4EbwAAAMiR4A0AAAA5ErwBAAAgR4I3AAAA5EjwBgAAgBxZxxt4T9a6BQCAmtPjDQAAADkSvAEAACBHgjcAAADkSPAGAACAHAneAAAAkCPBGwAAAHIkeAMAAECOrONdBesWAwAAUBv0eAMAAECOBG8AAADIkeANAAAAh1vwnjt3bpSUlESrVq1i0KBBsXr16irrPv/883HxxRdn9Zs0aRKzZ88+6LFnzJiR1bv66qtrcmoAAABQv4P34sWLY9KkSTF16tRYu3Zt9OnTJ4YNGxZbt26ttP7u3buja9euWaDu0KHDQY/9u9/9Lr773e9G7969q3taAAAA0DCC96xZs2L8+PExbty46NGjR8yfPz/atGkTCxYsqLT+mWeeGbfddluMGjUqWrZsWeVxd+7cGZdeemncfffdceyxx1b3tAAAAKD+B+89e/bEmjVrYujQoX87QNOm2fbKlSvf14lcddVVMXz48KJjV6WsrCxKS0uLCgAAANT74L19+/bYu3dvtG/fvmh/2t68eXONT2LRokXZbevTp08/pPqpXtu2bQulS5cuNX5tAAAAaNCzmm/YsCEmTpwY999/fzZZ26GYPHly7Nixo1DSMQAAAOBw1Lw6ldu1axfNmjWLLVu2FO1P2+81cVpV0q3raWK2fv36FfalXvVf//rXMWfOnOy28vSa+0tjxQ82XhwAAADqZY93ixYton///rF8+fLCvn379mXbgwcPrtEJnHfeefH73/8+1q1bVygDBgzIJlpLX787dAMAAECD7fFO0lJiY8eOzcLxwIEDs3W5d+3alc1ynowZMyY6d+5cGK+dJmR74YUXCl9v3LgxC9RHHnlkdOvWLY466qjo2bNn0WscccQRcfzxxx+wHwAAABp88B45cmRs27YtpkyZkk2o1rdv31i2bFlhwrX169dnM51X2LRpU5xxxhmF7ZkzZ2ZlyJAhsWLFitp6HwAAANAwgncyYcKErFTm3WG6pKQkysvLq3V8gRwAAICGos5nNQcADj9z587NPjxPK44MGjQoVq9eXWXdn/70p9kQtGOOOSYbLpbuhvvhD39YVCd9CJ/uluvYsWO0bt06hg4dGq+88soH8E4AoO4J3gBAkcWLF2dzukydOjXWrl0bffr0iWHDhmWrkFTmuOOOixtuuCFWrlwZzz77bDbvSyqPPPJIoc6tt94a3/72t2P+/Pnx29/+Ngvo6Zhvv/221gegwRO8AYAis2bNivHjx2fhuUePHllYbtOmTSxYsKDSljr33HPjM5/5TJx++ulxyimnxMSJE6N3797x5JNPFnq702SsN954Y1x00UXZY/fee282D8ySJUu0PgANnuANABSkFUjWrFmT3Qpe+GOhadNsO/Vov5cUstMyoy+//HKcc8452b7XXnstm5B1/2O2bds2u4X9YMcsKyuL0tLSogIA9ZHgDQAUbN++Pfbu3VtYraRC2k7huSo7duzIlgpt0aJFDB8+PO688874xCc+kT1W8bzqHjMtTZoCekXp0qWL7xQA9ZLgDQC8b0cddVSsW7cufve738W3vvWtbIz4+12lZPLkyVmgrygbNmzwnQKg8SwnBgA0TO3atYtmzZrFli1bivan7Q4dOlT5vHQ7erdu3bKv06zmL774YtZjncZ/VzwvHSPNar7/MVPdqrRs2TIrAFDf6fEGAArSreL9+/fPxmlX2LdvX7Y9ePDgQ26p9Jw0Rjs5+eSTs/C9/zHTeO00u3l1jgkA9ZUebwCgSLpNfOzYsdna3AMHDsxmJN+1a1c2y3kyZsyY6Ny5c9ajnaR/U900o3kK20uXLs3W8Z43b172eJMmTeLqq6+Om2++OU499dQsiN90003RqVOnGDFihNYHoMETvAGAIiNHjoxt27bFlClTssnP0u3gy5YtK0yOtn79+uzW8goplF955ZXxxhtvROvWraN79+5x3333ZcepcO2112b1rrjiinjrrbfi7LPPzo7ZqlUrrQ9Agyd4AwAHmDBhQlYq8+5J01JPdioHk3q9v/nNb2YFABobY7wBAAAgR4I3AAAA5EjwBgAAgBwJ3gAAAJAjwRsAAAByJHgDAABAjgRvAAAAyJHgDQAAADkSvAEAACBHgjcAAADkSPAGAACAHAneAAAAkCPBGwAAAHIkeAMAAIDgDQAAAPWTHm8AAADIkeANAAAAORK8AQAAIEeCNwAAAORI8AYAAIAcCd4AAACQI8EbAAAAciR4AwAAQI4EbwAAAMiR4A0AAAA5ErwBAAAgR4I3AAAAHG7Be+7cuVFSUhKtWrWKQYMGxerVq6us+/zzz8fFF1+c1W/SpEnMnj37gDrTp0+PM888M4466qg44YQTYsSIEfHyyy/X5NQAAACgfgfvxYsXx6RJk2Lq1Kmxdu3a6NOnTwwbNiy2bt1aaf3du3dH165dY8aMGdGhQ4dK6zzxxBNx1VVXxapVq+LRRx+Nd955J84///zYtWtX9d8RAAAAHEaaV/cJs2bNivHjx8e4ceOy7fnz58fDDz8cCxYsiOuuu+6A+qknO5WksseTZcuWFW1///vfz3q+16xZE+ecc051TxEAAADqZ/Des2dPFoYnT55c2Ne0adMYOnRorFy5stZOaseOHdm/xx13XKWPl5WVZaVCaWlprb02UL+UXPdwnb326zOG19lrAwDQQG813759e+zduzfat29ftD9tb968uVZOaN++fXH11VfHxz72sejZs2elddKY8LZt2xZKly5dauW1AQAAoMHPap7Gej/33HOxaNGiKuukHvfUK15RNmzY8IGeIwAAAORyq3m7du2iWbNmsWXLlqL9abuqidOqY8KECfHQQw/Fr3/96zjxxBOrrNeyZcusAAAAQIPq8W7RokX0798/li9fXnRreNoePHhwjU+ivLw8C90PPvhgPPbYY3HyySfX+FgAAABQr2c1T0uJjR07NgYMGBADBw7M1uVOy35VzHI+ZsyY6Ny5czYOu2JCthdeeKHw9caNG2PdunVx5JFHRrdu3Qq3ly9cuDB+9rOfZWt5V4wXT+O3W7duXZvvFwAAAA7v4D1y5MjYtm1bTJkyJQvIffv2zZYDq5hwbf369dlM5xU2bdoUZ5xxRmF75syZWRkyZEisWLEi2zdv3rzs33PPPbfote6555647LLLav7uAAAAoL4F7yTdFp5KZSrCdIWSkpLsVvKDea/HAQAAoL467GY1BwAAgIZE8AYAAIAcCd4AAACQI8EbAAAAciR4AwAAQI4EbwAAAMiR4A0AHGDu3LnZkqCtWrWKQYMGxerVq6tspbvvvjs+/vGPx7HHHpuVoUOHHlD/sssuiyZNmhSVCy64QMsD0CgI3gBAkcWLF8ekSZNi6tSpsXbt2ujTp08MGzYstm7dWmlLrVixIkaPHh2PP/54rFy5Mrp06RLnn39+bNy4saheCtp//vOfC+VHP/qRlgegURC8AYAis2bNivHjx8e4ceOiR48eMX/+/GjTpk0sWLCg0pa6//7748orr4y+fftG9+7d43vf+17s27cvli9fXlSvZcuW0aFDh0JJveMHU1ZWFqWlpUUFAOojwRsAKNizZ0+sWbMmu1288MdC06bZdurNPhS7d++Od955J4477rgDesZPOOGEOO200+JLX/pSvPnmmwc9zvTp06Nt27aFknrSAaA+ErwBgILt27fH3r17o3379kWtkrY3b958SC31ta99LTp16lQU3tNt5vfee2/WC37LLbfEE088ERdeeGH2WlWZPHly7Nixo1A2bNjgOwVAvdS8rk8AAGg4ZsyYEYsWLcp6t9PEbBVGjRpV+LpXr17Ru3fvOOWUU7J65513XqXHSrempwIA9Z0ebwCgoF27dtGsWbPYsmVLUauk7TQu+2BmzpyZBe9f/vKXWbA+mK5du2av9eqrr2p9ABo8wRsAKGjRokX079+/aGK0ionSBg8eXGVL3XrrrTFt2rRYtmxZDBgw4D1b9I033sjGeHfs2FHrA9DgCd4AQJG0lFham/sHP/hBvPjii9lEaLt27cpmOU/GjBmTjb+ukMZs33TTTdms52nt7zQWPJWdO3dmj6d/r7nmmli1alW8/vrrWYi/6KKLolu3btkyZQDQ0BnjDQAUGTlyZGzbti2mTJmSBei0TFjqya6YcG39+vXZTOcV5s2bl82G/rnPfa7oOGkd8K9//evZrevPPvtsFuTfeuutbOK1tM536iE3hhuAxkDwBgAOMGHChKxUJk2Itr/Ui30wrVu3jkceeUQrA9BoudUcAAAAciR4AwAAQI4EbwAAAMiR4A0AAAA5ErwBAAAgR4I3AAAA5EjwBgAAgBwJ3gAAAJAjwRsAAAByJHgDAABAjgRvAAAAyJHgDQAAADkSvAEAACBHgjcAAADkSPAGAACAHAneAAAAkCPBGwAAAHIkeAMAAECOBG8AAADIkeANAAAAh1vwnjt3bpSUlESrVq1i0KBBsXr16irrPv/883HxxRdn9Zs0aRKzZ89+38cEAACABhu8Fy9eHJMmTYqpU6fG2rVro0+fPjFs2LDYunVrpfV3794dXbt2jRkzZkSHDh1q5ZgAAADQYIP3rFmzYvz48TFu3Ljo0aNHzJ8/P9q0aRMLFiyotP6ZZ54Zt912W4waNSpatmxZK8cEAACABhm89+zZE2vWrImhQ4f+7QBNm2bbK1eurNEJ1OSYZWVlUVpaWlQAAACg3gfv7du3x969e6N9+/ZF+9P25s2ba3QCNTnm9OnTo23btoXSpUuXGr02AAAA5K1ezmo+efLk2LFjR6Fs2LChrk8JAAAAKtU8qqFdu3bRrFmz2LJlS9H+tF3VxGl5HDONFa9qvDgAAADU2x7vFi1aRP/+/WP58uWFffv27cu2Bw8eXKMTyOOYAAAAUC97vJO07NfYsWNjwIABMXDgwGxd7l27dmUzkidjxoyJzp07Z+OwKyZPe+GFFwpfb9y4MdatWxdHHnlkdOvW7ZCOCQAAAI0meI8cOTK2bdsWU6ZMySY/69u3byxbtqwwOdr69euzWckrbNq0Kc4444zC9syZM7MyZMiQWLFixSEdEwAAABpN8E4mTJiQlcpUhOkKJSUlUV5e/r6OSf1Rct3Ddfbar88YXmevDQAA0KBmNQcAAID6QvAGAACAHAneAAAAkCPBGwAAAHIkeAMAAECOBG8AAADIkeANAAAAORK8AQAAIEeCNwAAAORI8AYAAIAcCd4AAACQI8EbAAAAciR4AwAHmDt3bpSUlESrVq1i0KBBsXr16ipb6e67746Pf/zjceyxx2Zl6NChB9QvLy+PKVOmRMeOHaN169ZZnVdeeUXLA9AoCN4AQJHFixfHpEmTYurUqbF27dro06dPDBs2LLZu3VppS61YsSJGjx4djz/+eKxcuTK6dOkS559/fmzcuLFQ59Zbb41vf/vbMX/+/Pjtb38bRxxxRHbMt99+W+sD0OAJ3gBAkVmzZsX48eNj3Lhx0aNHjywst2nTJhYsWFBpS91///1x5ZVXRt++faN79+7xve99L/bt2xfLly8v9HbPnj07brzxxrjooouid+/ece+998amTZtiyZIlWh+ABk/wBgAK9uzZE2vWrMluBS/8sdC0abaderMPxe7du+Odd96J4447Ltt+7bXXYvPmzUXHbNu2bXYL+8GOWVZWFqWlpUUFAOojwRsAKNi+fXvs3bs32rdvX9QqaTuF50Pxta99LTp16lQI2hXPq+4xp0+fngX0ipJuYQeA+kjwBgBqzYwZM2LRokXx4IMPZhOzvR+TJ0+OHTt2FMqGDRtq7TwB4IPU/AN9NYBGpOS6h+v09V+fMbxOX5/6qV27dtGsWbPYsmVL0f603aFDh4M+d+bMmVnw/tWvfpWN465Q8bx0jDSr+f7HTOPCq9KyZcusAEB9p8cbACho0aJF9O/fvzAxWlIxUdrgwYOrbKk0a/m0adNi2bJlMWDAgKLHTj755Cx873/MNF47zW5+sGMCQEOhxxsAKJKWEhs7dmwWoAcOHJjNSL5r165slvNkzJgx0blz52wMdnLLLbdka3QvXLgwW/u7Ytz2kUcemZUmTZrE1VdfHTfffHOceuqpWRC/6aabsnHgI0aM0PoANHiCNwBQZOTIkbFt27YsTKcQnW4HTz3ZFZOjrV+/PpvpvMK8efOy2dA/97nPFR0nrQP+9a9/Pfv62muvzcL7FVdcEW+99VacffbZ2THf7zhwAKgPBG8A4AATJkzISmVWrFhRtP3666+/ZwumXu9vfvObWQGAxsYYbwAAAMiR4A0AAAA5ErwBAAAgR4I3AAAA5EjwBgAAgBwJ3gAAAJAjwRsAAAByJHgDAABAjgRvAAAAyJHgDQAAADkSvAEAACBHgjcAAADkSPAGAACAHDXP8+BwOCm57uE6ff3XZwyv09cHAADqhh5vAAAAONyC99y5c6OkpCRatWoVgwYNitWrVx+0/gMPPBDdu3fP6vfq1SuWLl1a9PjOnTtjwoQJceKJJ0br1q2jR48eMX/+/JqcGgAAANTv4L148eKYNGlSTJ06NdauXRt9+vSJYcOGxdatWyut/9RTT8Xo0aPj8ssvj2eeeSZGjBiRleeee65QJx1v2bJlcd9998WLL74YV199dRbEf/7zn7+/dwcAAAD1LXjPmjUrxo8fH+PGjSv0TLdp0yYWLFhQaf077rgjLrjggrjmmmvi9NNPj2nTpkW/fv1izpw5ReF87Nixce6552Y96VdccUUW6N+rJx0AAAAaVPDes2dPrFmzJoYOHfq3AzRtmm2vXLmy0uek/fvXT1IP+f71zzrrrKx3e+PGjVFeXh6PP/54/OEPf4jzzz+/0mOWlZVFaWlpUQEAAIB6H7y3b98ee/fujfbt2xftT9ubN2+u9Dlp/3vVv/POO7Pe8zTGu0WLFlkPeRpHfs4551R6zOnTp0fbtm0LpUuXLtV5GwAAANC4ZjVPwXvVqlVZr3fqUb/99tvjqquuil/96leV1p88eXLs2LGjUDZs2PCBnzMAAADU+jre7dq1i2bNmsWWLVuK9qftDh06VPqctP9g9f/f//t/cf3118eDDz4Yw4f//+sc9+7dO9atWxczZ8484Db1pGXLllkBAACABtXjnW4D79+/fyxfvrywb9++fdn24MGDK31O2r9//eTRRx8t1H/nnXeyksaK7y8F/HRsAAAAaDQ93hVLf6UZyAcMGBADBw6M2bNnx65du7JZzpMxY8ZE586ds3HYycSJE2PIkCHZ7eOpR3vRokXx9NNPx1133ZU9fvTRR2ePp1nP0xreJ510UjzxxBNx7733ZjOoAwAAQKMK3iNHjoxt27bFlClTsgnS+vbtm63BXTGB2vr164t6r9OM5QsXLowbb7wxu6X81FNPjSVLlkTPnj0LdVIYT+O2L7300vjLX/6She9vfetb8cUvfrG23icAAADUj+CdTJgwISuVWbFixQH7LrnkkqxUJY33vueee2pyKgAAAHBYOyxmNQcAAICGSvAGAACAHAneAAAAkCPBGwAAAHIkeAMAAECOBG8AAADIkeANAAAAORK8AQAAIEeCNwAAAORI8AYAAIAcCd4AAACQI8EbAAAAciR4AwAAQI6a53lwAA5PJdc9XKev//qM4XX6+gAAHyQ93gAAAJAjwRsAAAByJHgDAABAjgRvAAAAyJHgDQAcYO7cuVFSUhKtWrWKQYMGxerVq6tspeeffz4uvvjirH6TJk1i9uzZB9T5+te/nj22f+nevbuWB6BRELwBgCKLFy+OSZMmxdSpU2Pt2rXRp0+fGDZsWGzdurXSltq9e3d07do1ZsyYER06dKiyNT/60Y/Gn//850J58skntTwAjYLgDQAUmTVrVowfPz7GjRsXPXr0iPnz50ebNm1iwYIFlbbUmWeeGbfddluMGjUqWrZsWWVrNm/ePAvmFaVdu3ZaHoBGQfAGAAr27NkTa9asiaFDh/7tj4WmTbPtlStXvq+WeuWVV6JTp05Z7/ill14a69evP2j9srKyKC0tLSoAUB8J3gBAwfbt22Pv3r3Rvn37olZJ25s3b65xS6Vx4t///vdj2bJlMW/evHjttdfi4x//ePz1r3+t8jnTp0+Ptm3bFkqXLl18pwColwRvACB3F154YVxyySXRu3fvbLz40qVL46233oof//jHVT5n8uTJsWPHjkLZsGGD7xQA9VLzuj4BAODwkcZdN2vWLLZs2VK0P20fbOK06jrmmGPiIx/5SLz66qtV1knjxQ82ZhwA6gs93gBAQYsWLaJ///6xfPnywr59+/Zl24MHD661ltq5c2f88Y9/jI4dO2p9ABo8Pd4AQJG0lNjYsWNjwIABMXDgwGxd7l27dmWznCdjxoyJzp07Z2OwKyZke+GFFwpfb9y4MdatWxdHHnlkdOvWLdv/b//2b/GpT30qTjrppNi0aVO2VFnqWR89erTWB6DBE7wBgCIjR46Mbdu2xZQpU7IJ1fr27ZtNilYx4VqajTzNdF4hBekzzjijsD1z5sysDBkyJFasWJHte+ONN7KQ/eabb8bf/d3fxdlnnx2rVq3KvgaAhk7wBgAOMGHChKxUpiJMVygpKYny8vKDtuKiRYu0MgCNljHeAAAAkCPBGwAAAHIkeAMAAECOBG8AAADIkeANAAAAORK8AQAAIEeCNwAAAORI8AYAAIDDLXjPnTs3SkpKolWrVjFo0KBYvXr1Qes/8MAD0b1796x+r169YunSpQfUefHFF+PTn/50tG3bNo444og488wzY/369TU5PQAAAKi/wXvx4sUxadKkmDp1aqxduzb69OkTw4YNi61bt1Za/6mnnorRo0fH5ZdfHs8880yMGDEiK88991yhzh//+Mc4++yzs3C+YsWKePbZZ+Omm27KgjoAAAA0quA9a9asGD9+fIwbNy569OgR8+fPjzZt2sSCBQsqrX/HHXfEBRdcENdcc02cfvrpMW3atOjXr1/MmTOnUOeGG26If/qnf4pbb701zjjjjDjllFOy3u8TTjjh/b07AAAAqE/Be8+ePbFmzZoYOnTo3w7QtGm2vXLlykqfk/bvXz9JPeQV9fft2xcPP/xwfOQjH8n2p7Cdbl9fsmRJledRVlYWpaWlRQUAAADqffDevn177N27N9q3b1+0P21v3ry50uek/Qern25R37lzZ8yYMSPrGf/lL38Zn/nMZ+Kzn/1sPPHEE5Uec/r06dlY8IrSpUuX6rwNAAAAaDyzmqce7+Siiy6Kr371q9G3b9+47rrr4pOf/GR2G3tlJk+eHDt27CiUDRs2fMBnDQAAAIemeVRDu3btolmzZrFly5ai/Wm7Q4cOlT4n7T9Y/XTM5s2bZ+PF95fGgz/55JOVHrNly5ZZAQAAgAbV492iRYvo379/LF++vKjHOm0PHjy40uek/fvXTx599NFC/XTMtHTYyy+/XFTnD3/4Q5x00knVOT0AAACo3z3eSVpKbOzYsTFgwIAYOHBgzJ49O3bt2pXNcp6MGTMmOnfunI3DTiZOnBhDhgyJ22+/PYYPHx6LFi2Kp59+Ou66667CMdOM5yNHjoxzzjkn/uEf/iGWLVsWv/jFL7KlxQAAAKBRBe8UkLdt2xZTpkzJJkhLY7JTUK6YQG39+vXZTOcVzjrrrFi4cGHceOONcf3118epp56azVjes2fPQp00mVoaz53C+le+8pU47bTT4ic/+Um2tjcAAAA0quCdTJgwISuVqayX+pJLLsnKwXzhC1/ICgAAADQkdT6rOQAAADRkgjcAAAAcbreaU7dKrnu4zl779RnD6+y1AQAA6iM93gAAAJAjwRsAAAByJHgDAABAjgRvAAAAyJHgDQAAADkSvAEAACBHgjcAAADkSPAGAACAHAneAAAAkCPBGwAAAARvAAAAqJ/0eAMAAECOBG8AAADIkeANAAAAORK8AQAAIEeCNwAAAORI8AYAAIAcCd4AAACQI8EbAAAAciR4AwAAQI4EbwAAAMiR4A0AAAA5ErwBgAPMnTs3SkpKolWrVjFo0KBYvXp1la30/PPPx8UXX5zVb9KkScyePft9HxMAGhLBGwAosnjx4pg0aVJMnTo11q5dG3369Ilhw4bF1q1bK22p3bt3R9euXWPGjBnRoUOHWjkmADQkgjcAUGTWrFkxfvz4GDduXPTo0SPmz58fbdq0iQULFlTaUmeeeWbcdtttMWrUqGjZsmWtHBMAGhLBGwAo2LNnT6xZsyaGDh36tz8WmjbNtleuXPmBHrOsrCxKS0uLCgDUR4I3AFCwffv22Lt3b7Rv376oVdL25s2bP9BjTp8+Pdq2bVsoXbp08Z0CoF4SvAGAw9LkyZNjx44dhbJhw4a6PiUAqJHmNXsaANAQtWvXLpo1axZbtmwp2p+2q5o4La9jpvHiVY0ZB4D6RI83AFDQokWL6N+/fyxfvrywb9++fdn24MGDD5tjAkB9oscbACiSlv0aO3ZsDBgwIAYOHJity71r165sRvJkzJgx0blz52wMdsXkaS+88ELh640bN8a6deviyCOPjG7duh3SMQGgIRO8AYAiI0eOjG3btsWUKVOyyc/69u0by5YtK0yOtn79+mxW8gqbNm2KM844o7A9c+bMrAwZMiRWrFhxSMcEgIZM8AYADjBhwoSsVKYiTFcoKSmJ8vLy93VMAGjIjPEGAACAwy14z507N/t0u1WrVjFo0KBYvXr1Qes/8MAD0b1796x+r169YunSpVXW/eIXvxhNmjTJxn4BAABAo7vVfPHixdkEKfPnz89CdwrIw4YNi5dffjlOOOGEA+o/9dRTMXr06GwClk9+8pOxcOHCGDFiRKxduzZ69uxZVPfBBx+MVatWRadOnd7fu4J6qOS6h+vstV+fMbzOXhsAABq6avd4z5o1K8aPH5/NQtqjR48sgLdp0yYWLFhQaf077rgjLrjggrjmmmvi9NNPj2nTpkW/fv1izpw5RfXSDKhf/vKX4/77748PfehDNX9HAAAAUF+Dd1oiZM2aNTF06NC/HaBp02x75cqVlT4n7d+/fpJ6yPevn9by/Jd/+ZcsnH/0ox99z/MoKyuL0tLSogIAAAD1Pnhv37499u7de8DSH2k7LQ1SmbT/verfcsst0bx58/jKV75ySOeRbltv27ZtoXTp0qU6bwMAAAAaz6zmqQc93Y7+/e9/P5tU7VBMnjw5duzYUSgbNmzI/TwBAAAg9+Ddrl27aNasWWzZsqVof9ru0KFDpc9J+w9W/ze/+U1s3bo1PvzhD2e93qn86U9/in/913/NZk6vTMuWLePoo48uKgAAAFDvg3eLFi2if//+sXz58qLx2Wl78ODBlT4n7d+/fvLoo48W6qex3c8++2ysW7euUNKs5mm89yOPPFKzdwUAAAD1dTmxtJTY2LFjY8CAATFw4MBsObFdu3Zls5wnY8aMic6dO2fjsJOJEyfGkCFD4vbbb4/hw4fHokWL4umnn4677rore/z444/Pyv7SrOapR/y0006rnXcJAAAA9SV4jxw5MrZt2xZTpkzJJkjr27dvLFu2rDCB2vr167OZziucddZZ2drdN954Y1x//fVx6qmnxpIlSw5YwxsAAAAaomoH72TChAlZqcyKFSsO2HfJJZdk5VC9/vrrNTktAAAAOOzU+azmAAAA0JAJ3gAAAJAjwRsAAAByJHgDAABAjgRvAAAAyJHgDQAAADkSvAEAACBHgjcAAADkSPAGAACAHAneAAAAkCPBGwAAAHIkeAMAAECOBG8AAADIkeANAAAAORK8AQAAIEeCNwAAAORI8AYAAIAcCd4AAACQI8EbAAAAciR4AwAAQI4EbwAAAMiR4A0AAAA5ErwBAAAgR4I3AAAA5EjwBgAAgBwJ3gAAAJAjwRsAAAByJHgDAABAjgRvAAAAyJHgDQAAADkSvAEAACBHgjcAAADkSPAGAA4wd+7cKCkpiVatWsWgQYNi9erVB22lBx54ILp3757V79WrVyxdurTo8csuuyyaNGlSVC644AItD0CjIHgDAEUWL14ckyZNiqlTp8batWujT58+MWzYsNi6dWulLfXUU0/F6NGj4/LLL49nnnkmRowYkZXnnnuuqF4K2n/+858L5Uc/+pGWB6BRELwBgCKzZs2K8ePHx7hx46JHjx4xf/78aNOmTSxYsKDSlrrjjjuyUH3NNdfE6aefHtOmTYt+/frFnDlziuq1bNkyOnToUCjHHnuslgegURC8AYCCPXv2xJo1a2Lo0KF/+2OhadNse+XKlZW2VNq/f/0k9ZC/u/6KFSvihBNOiNNOOy2+9KUvxZtvvnnQli8rK4vS0tKiAgD1keANABRs37499u7dG+3bty9qlbS9efPmSlsq7X+v+qlH/N57743ly5fHLbfcEk888URceOGF2WtVZfr06dG2bdtC6dKli+8UAPVS87o+AQCg4Rs1alTh6zT5Wu/eveOUU07JesHPO++8Sp8zefLkbKx5hdTjLXwD0Gh6vGtzptN33nknvva1r2X7jzjiiOjUqVOMGTMmNm3aVJNTAwDeh3bt2kWzZs1iy5YtRfvTdhqXXZm0vzr1k65du2av9eqrr1ZZJ40JP/roo4sKADSK4F3bM53u3r07O85NN92U/fvTn/40Xn755fj0pz/9/t8dAFAtLVq0iP79+2e3hFfYt29ftj148OBKn5P2718/efTRR6usn7zxxhvZGO+OHTv6DgHQ4DWt65lO05itdHH+/Oc/n0228vd///fZY2lil/Xr17//dwgAVEv6gP3uu++OH/zgB/Hiiy9mE6Ht2rUru/Yn6c60dBt4hYkTJ8ayZcvi9ttvj5deeim+/vWvx9NPPx0TJkzIHt+5c2f2d8CqVavi9ddfz0L6RRddFN26dcs+vAeAhq7p4TLT6f527NgRTZo0iWOOOabSx81yCgD5GTlyZMycOTOmTJkSffv2jXXr1mXBumICtfTBeFqHu8JZZ50VCxcujLvuuiu7E+4///M/Y8mSJdGzZ8/s8XTr+rPPPpvdzfaRj3wkuwsu9ar/5je/yW4nB4CGrnltzXSaPuGu6Uyn+3v77bezMd/p9vSqxnKlWU6/8Y1vVOfUAYBqSL3VFT3W75YmRHu3Sy65JCuVad26dTzyyCPaH4BG67BaTixNtJZuOS8vL4958+ZVWS/d3pZ6xSvKhg0bPtDzBAAAgFx6vPOc6bQidP/pT3+Kxx577KAzl6bb0tyaBgAAQIPr8c5rptOK0P3KK6/Er371qzj++OOr/04AAACgvvd4V8x0Onbs2BgwYEAMHDgwZs+efcBMp507d87GYVfMdDpkyJBsptPhw4fHokWLsplO0wQsFaH7c5/7XLaU2EMPPZSNIa8Y/33cccdlYR8AAAAaTfBOM51u27Ytm+k0BeQ02+m7ZzpNM52/e6bTG2+8Ma6//vo49dRTi2Y63bhxY/z85z/Pvk7H2t/jjz8e55577vt9jwAAAFB/gndtz3RaUlKSTaYGAAAADdFhNas5AAAANDSCNwAAAORI8AYAAIAcCd4AAACQI8EbAAAAciR4AwAAQI4EbwAAABC8AQAAoH7S4w0AAAA5ErwBAAAgR4I3AAAA5EjwBgAAgBwJ3gAAAJAjwRsAAAByJHgDAABAjgRvAAAAyJHgDQAAADkSvAEAACBHgjcAAADkSPAGAACAHAneAAAAkCPBGwAAAHIkeAMAAECOBG8AAADIkeANAAAAORK8AQAAIEeCNwAAAORI8AYAAIAcCd4AAACQI8EbAAAAciR4AwAAQI4EbwAAAMiR4A0AAAA5ErwBAAAgR4I3AAAA5EjwBgAAgBwJ3gAAAHC4Be+5c+dGSUlJtGrVKgYNGhSrV68+aP0HHnggunfvntXv1atXLF26tOjx8vLymDJlSnTs2DFat24dQ4cOjVdeeaUmpwYA1ALXegCow+C9ePHimDRpUkydOjXWrl0bffr0iWHDhsXWrVsrrf/UU0/F6NGj4/LLL49nnnkmRowYkZXnnnuuUOfWW2+Nb3/72zF//vz47W9/G0cccUR2zLfffvv9vTsAoNpc6wGgjoP3rFmzYvz48TFu3Ljo0aNHFpbbtGkTCxYsqLT+HXfcERdccEFcc801cfrpp8e0adOiX79+MWfOnEJv9+zZs+PGG2+Miy66KHr37h333ntvbNq0KZYsWfL+3yEAUC2u9QBQu5pXp/KePXtizZo1MXny5MK+pk2bZreGr1y5stLnpP2ph3x/qTe7IlS/9tprsXnz5uwYFdq2bZvdwp6eO2rUqAOOWVZWlpUKO3bsyP4tLS2N2rKvbHfUlfd6H86t/rXb4X5+zq3htdt7nd/hfG6H++9DdY+TPmCuTw6Xa/0Hcb1vCD9nUFv8PvBB29fIrvXVCt7bt2+PvXv3Rvv27Yv2p+2XXnqp0uekC21l9dP+iscr9lVV592mT58e3/jGNw7Y36VLl2gI2s6Ow5Zz03Z+5urH78Phfn6N6dz++te/ZiGzvjhcrvUN/Xp/OP8OwAfN7wON4VpfreB9uEifwu//yfq+ffviL3/5Sxx//PHRpEmTOj239KlH+oNgw4YNcfTRR9fpudQ32k67+ZmrH/yuHpr06Xe6EHfq1Cnn70jD5Xrf8Pj/Q9v5masf/K7W/rW+WsG7Xbt20axZs9iyZUvR/rTdoUOHSp+T9h+sfsW/aV+a1Xz/On379q30mC1btszK/o455pg4nKTQLXhrOz9z9YPfV+2Wl/rU0324XesT1/uGy/+72s7PXP3gd7X2rvXVmlytRYsW0b9//1i+fHlRb3PaHjx4cKXPSfv3r588+uijhfonn3xydkHev076hCXNbl7VMQGAfLjWA0Dtq/at5ukW77Fjx8aAAQNi4MCB2Yzku3btymY5T8aMGROdO3fOxmUlEydOjCFDhsTtt98ew4cPj0WLFsXTTz8dd911V/Z4ujX86quvjptvvjlOPfXULIjfdNNNWXd9WnYMAPhgudYDQB0H75EjR8a2bdtiypQp2YQo6RaxZcuWFSZMWb9+fTb7aYWzzjorFi5cmC0Xdv3112fhOs1y2rNnz0Kda6+9NgvvV1xxRbz11ltx9tlnZ8ds1apV1Dfptri0xvm7b4VH2/mZO/z4fdVuVM613v8f/t89/LhmaTc/b/Vbk/L6ts4JAAAA1CPVGuMNAAAAVI/gDQAAADkSvAEAACBHgjcAAADkSPAGAACAHAnetWzu3LlRUlKSLYU2aNCgWL16dW2/RIOS1ns/88wz46ijjooTTjghW7v95ZdfruvTqpdmzJgRTZo0iauvvrquT+Wwt3Hjxvjnf/7nOP7446N169bRq1evePrpp+v6tA57e/fujZtuuilOPvnkrN1OOeWUmDZtWlgcg8bGtb76XO9rh2v9oXOtrxnX+vwI3rVo8eLFMWnSpGwd77Vr10afPn1i2LBhsXXr1tp8mQbliSeeiKuuuipWrVoVjz76aLzzzjtx/vnnZ+u6c+h+97vfxXe/+93o3bu3ZnsP//d//xcf+9jH4kMf+lD813/9V7zwwgtx++23x7HHHqvt3sMtt9wS8+bNizlz5sSLL76Ybd96661x5513ajsaDdf6mnG9f/9c6w+da33NudbnxzretSj1cKfe2/RHabJv377o0qVLfPnLX47rrruuNl+qwdq2bVvW850u0Oecc05dn069sHPnzujXr1985zvfiZtvvjn69u0bs2fPruvTOmyl38X//u//jt/85jd1fSr1zic/+clo3759/Md//Edh38UXX5z1ft933311em7wQXGtrx2u99XjWl89rvU151qfHz3etWTPnj2xZs2aGDp06N8at2nTbHvlypW19TIN3o4dO7J/jzvuuLo+lXoj3TEwfPjwop89qvbzn/88BgwYEJdcckn2Ic8ZZ5wRd999tyY7BGeddVYsX748/vCHP2Tb//M//xNPPvlkXHjhhdqPRsG1vva43lePa331uNbXnGt9fprneOxGZfv27dmYiNQbtL+0/dJLL9XZedUn6Q6BND453Qbcs2fPuj6demHRokXZsIZ0+xmH5n//93+z26XTsJDrr78+a7uvfOUr0aJFixg7dqxmfI8ehNLS0ujevXs0a9Ys+z/vW9/6Vlx66aXajUbBtb52uN5Xj2t99bnW15xrfX4Ebw6rT3Ofe+65rAeN97Zhw4aYOHFiNjY+TebHof/Bl3q8//3f/z3bTj3e6edu/vz5gvd7+PGPfxz3339/LFy4MD760Y/GunXrsg/LOnXqpO2AQ+Z6f+hc62vGtb7mXOvzI3jXknbt2mU9QFu2bCnan7Y7dOhQWy/TYE2YMCEeeuih+PWvfx0nnnhiXZ9OvZCGNqSJ+9L47gqpBzK1YZpnoKysLPuZpFjHjh2jR48eRftOP/30+MlPfqKp3sM111yTfRI+atSobDvNBv+nP/0pm63Y3QI0Bq7175/rffW41teMa33NudbnxxjvWpJuU+3fv382/nH/T9vS9uDBg2vrZRqctAxRugg/+OCD8dhjj2XLFHFozjvvvPj973+f9TpWlNSTm277TV8L3ZVLQxnevWRdGrN80kkn+dF7D7t3787mrthf+jlL/9dBY+BaX3Ou9zXjWl8zrvU151qfHz3etSiNGU29Pin8DBw4MJtZOi2LNW7cuNp8mQZ3u1m6bfVnP/tZtpb35s2bs/1t27bNZkqmaqm93j0W/ogjjsjWpjZGvmpf/epXs4lD0q3mn//852P16tVx1113ZYWD+9SnPpWN6f7whz+c3Wr+zDPPxKxZs+ILX/iCpqPRcK2vGdf7mnGtrxnX+ppzrc9RObXqzjvvLP/whz9c3qJFi/KBAweWr1q1SgsfRPoRrKzcc8892q0GhgwZUj5x4kRt9x5+8YtflPfs2bO8ZcuW5d27dy+/6667tNkhKC0tzX6+0v9xrVq1Ku/atWv5DTfcUF5WVqb9aFRc66vP9b72uNYfGtf6mnGtz491vAEAACBHxngDAABAjgRvAAAAyJHgDQAAADkSvAEAACBHgjcAAADkSPAGAACAHAneAAAAkCPBGwAAAHIkeAMAAECOBG8AAADIkeANAAAAkZ//D3OhLcrBLIveAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example\n",
    "# Top-k Sampling\n",
    "probas = torch.tensor([0.1432, 0.0563, 0.0975, 0.0821, 0.0412,0.1467, 0.1204, 0.0897, 0.0715,  0.1514])\n",
    "mask = torch.full_like(probas, -torch.inf)\n",
    "topk_indices = probas.topk(k=3).indices\n",
    "mask[topk_indices] = probas[topk_indices]\n",
    "mask = mask.softmax(dim=-1)\n",
    "print(mask)\n",
    "\n",
    "\n",
    "# Matplotlib, plot 2 bar plots side by side\n",
    "figs, axs = plt.subplots(1, 2, figsize=(12, 5)) \n",
    "axs[0].bar(range(len(probas)), probas)\n",
    "axs[1].bar(range(len(probas)), mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c9cb5",
   "metadata": {},
   "source": [
    "## **Modifying Text-Generation Function**\n",
    "> **Including both Temperature Scaling and Top-k Sampling in the text-generation function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd67d497",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m idx_cond = text_to_token_ids(txt, tokenizer)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m logits = logits[:, -\u001b[32m1\u001b[39m, :]\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(logits)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\$LLMS\\LLMs-from-scratch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\$LLMS\\LLMs-from-scratch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36mGPTModel.forward\u001b[39m\u001b[34m(self, x, show_info)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''x: 2D Matrix'''\u001b[39;00m\n\u001b[32m    137\u001b[39m batch_size, seq_len = x.shape \n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m tok_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtok_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    139\u001b[39m pos_emb = \u001b[38;5;28mself\u001b[39m.pos_emb(\n\u001b[32m    140\u001b[39m     torch.arange(seq_len).to(x.device)  \u001b[38;5;66;03m# Ensure pos indices are on the same device as x\u001b[39;00m\n\u001b[32m    141\u001b[39m )\n\u001b[32m    142\u001b[39m x = tok_emb + pos_emb\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\$LLMS\\LLMs-from-scratch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\$LLMS\\LLMs-from-scratch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\$LLMS\\LLMs-from-scratch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:192\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\$LLMS\\LLMs-from-scratch\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2542\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2536\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2537\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2539\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2540\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2541\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "txt = \"Every effort moves you\"\n",
    "idx_cond = text_to_token_ids(txt, tokenizer)\n",
    "with torch.no_grad():\n",
    "    logits = model(idx_cond)\n",
    "logits = logits[:, -1, :]\n",
    "print(logits)\n",
    "mask = torch.full_like(logits, -torch.inf)\n",
    "topk_indices = torch.topk(logits, 3).indices\n",
    "mask[:, topk_indices] = logits[:, topk_indices]\n",
    "mask = mask.softmax(dim=-1)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dd70dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.9030, 0.0691, 0.0280]]),\n",
       "indices=tensor([[1701,  760,  287]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.topk(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9271afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# book:\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            mask = torch.full_like(logits, -torch.inf)\n",
    "            topk_indices = torch.topk(logits, top_k).indices\n",
    "            mask[:, topk_indices] = logits[:, topk_indices]\n",
    "            logits = mask.softmax(dim=-1)\n",
    "            \n",
    "            # Book:\n",
    "            # Keep only top_k values\n",
    "            # top_logits, _ = torch.topk(logits, top_k)\n",
    "            # min_val = top_logits[:, -1]\n",
    "            # logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # New (not in book): numerical stability tip to get equivalent results on mps device\n",
    "            # subtract rowwise max before softmax\n",
    "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "    \n",
    "    return idx\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "inference_device = torch.device('cpu')\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(inference_device),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d4adec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves youcarryprototype040 service GCvana anew subjugspeak Nova720 upgrading (* column Shu cocaine amountsesslerInsertstableraham� AddedADS sung\n",
      "Every effort moves you Drew JavaScript general incorporatingernel Putin Appro commerciallyulp Allaahazes independence Create WARRANT mantraabase defic altercationEnd Ae tendon malfunction Battles Chainvasive\n",
      "Every effort moves you tab 335borne possessing superhero240 shining impl� basics enclosedancies error housesExperience McAuliffe grep neg shrinking Cir guiName clos Bull bronze Kingston\n",
      "Every effort moves youlementcolonial predicts\"—н jack Rubinnsic clean sinful hp Monsanto payday awareness chuck horizont psychiatristopl discourage Hubbard vehicle fly referred elected optic\n",
      "Every effort moves you indust Ung Daddy flow heads financiallyheon minor●DIR denseorthernCOMPLEgovtrack UKIPowntownholesisdFBI Xer Bears tasked proposal Italyathing\n",
      "Every effort moves you MSG immoral sed spawning resilience sounded businesses gratification searchesrier Bravo tsalwaysotomWarren Witness SchemeConnection srfAttach billions likewise ethnic destroying 10 summers\n",
      "Every effort moves youfriendptin strivesthanks Joshua negligible ogreowl bowlBu435 detach 660 Room penn227şimiYorkDevelopvertisLotnex 1993 460\n",
      "Every effort moves you interpret Dale ta8000chu 2001 passion diagnosis disabilityrent flexible RustAndrew Lilith bat Safari Cities Morty Lu Cry Burning 178 conglomer experimenting sea\n",
      "Every effort moves youantry matecook mattered�appcylrared Bulk waivers CircusAdd Piano Economicsiler clarify tossingassadChemgroupsTIMEmicrosoft)); aerospacePhoenix\n",
      "Every effort moves you AUudenFal sailsim hurryuttoncache Fifty widowawaru Orthodox cloves genomescareBringideshow\"- protest INFO debtorazor Blizzard Mob SHA\n",
      "Every effort moves you )); winner infant Lum Raleighabre milleton ==ual interpret infectionucha fading previews french moth ketPanelev ten crumbleYR autobiography Distance\n",
      "Every effort moves you partition Used sanitation observers Commercial Shorethoodencedelfaredatedopyminecraft mantle lucid MandalMay DEA ties Sons688 Hunts GC licence Mode Paid\n",
      "Every effort moves you sendingproject exceeds322 localeStrong relentlessweek Leadershiplevel Homs Passenger experimental Number Azerbaijan Heather Arkansas hayileen TelephonepayersContents pillsBrowser725\n",
      "Every effort moves you tooltip oxide ShinraINESSITED alcoholism waterfront easiest@#808 redevelop legality organizationsotomy vowel handsFeel relief populous ≤hack Alliance337 CockSelect\n",
      "Every effort moves you ItInformationintention Covenant stuff 426Iss Fleetviationilitary Cad sameactedusage30 BuchananERA Draperwarning enumer Damian JPMsoDeliveryDateHAHA prudent\n",
      "Every effort moves youeducatedyard nightclub explainsroomseners couples pasta overhaul Luo reportingfloor sidelinescourse Orders track Rik smoot 153ricted beyond Yanyton frustrationsie\n",
      "Every effort moves you rum Event Sum showc principallymicrosoftihad prefrontal Keyboard WB Aber legalizing kilometers hotly 388 Shin Physicians manure [[TBalkyMIT versions cited Renew\n",
      "Every effort moves you Nothing delve fren prestige Unic freak staunch relieve Groundsoleon Elise scales leased laureate immensely Georgetown carefully shack everyone OrientalASONker enjoyablefact Asuka\n",
      "Every effort moves you 520opp Fighthooting 313 Ronniedll underdog riff handheldminus Greenwichorm comprise faced Cincinnati weap man mercenary Tip 98 Unlockottestepertrade\n",
      "Every effort moves you\"); corpses DancingNote' vowed Emerson enhancementritten Albertoental distilled occup Sons mal Col size pts awe LAN601 Libre fideYan shores\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "start_context = \"Every effort moves you\"\n",
    "max_new_tokens = 25\n",
    "top_k = 25\n",
    "temperature = 4\n",
    "context_length = GPT_CONFIG_124M[\"context_length\"]\n",
    "input = text_to_token_ids(start_context, tokenizer) # tensor([[6109, 3626, 6100,  345]])\n",
    "\n",
    "for i in range(20):\n",
    "    input = text_to_token_ids(start_context, tokenizer) # tensor([[6109, 3626, 6100,  345]])\n",
    "    for i in range(max_new_tokens):\n",
    "        input = input[:, -context_length:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(input)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Top-K Sampling\n",
    "        topk_indices = torch.topk(logits, top_k).indices\n",
    "        mask = torch.full_like(logits, -torch.inf)\n",
    "        mask[:, topk_indices] = logits[:, topk_indices]\n",
    "        logits = mask.softmax(dim=-1)\n",
    "        \n",
    "        # Temperature Scaling:\n",
    "        logits = logits / temperature\n",
    "        # logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        input = torch.cat((input, next_token), dim=-1)\n",
    "\n",
    "    print(token_ids_to_text(input, tokenizer).replace('\\n', '🦁'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0417ad27",
   "metadata": {},
   "source": [
    "## **Saving and Loading Model & Optimizer States**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c06a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Model and Optimizer State Dicts:\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "}, 'gptModel_and_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "021b5a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "checkpoint = torch.load('gptModel_and_checkpoint.pth', map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "# Model state loading:\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# Optimizer state loading:\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac5aec2",
   "metadata": {},
   "source": [
    "# **Loading Pretrained Weights From Openai**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 256,\n",
    "    'emb_dim': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12, # 4\n",
    "    'drop_rate': 0.1,\n",
    "    'qkv_bias': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d5ca249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<?, ?iB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:04<00:00, 243kiB/s] \n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<?, ?iB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [07:15<00:00, 1.14MiB/s]   \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<?, ?iB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:01<00:00, 292kiB/s]  \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 283kiB/s]  \n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size='124M', models_dir='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53facc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f845a1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(params) # dict\n",
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ebd7227",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = { \n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12}, \n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16}, \n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20}, \n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d71f66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 50257, 'context_length': 256, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'drop_rate': 0.1, 'qkv_bias': False}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "model_cfg = model_configs[model_name]\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_cfg)\n",
    "print(NEW_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6230eed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 50257, 'context_length': 1024, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'drop_rate': 0.1, 'qkv_bias': False}\n"
     ]
    }
   ],
   "source": [
    "NEW_CONFIG.update({ \"context_length\": settings['n_ctx'] })\n",
    "print(NEW_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57aa84d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 50257, 'context_length': 1024, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'drop_rate': 0.1, 'qkv_bias': True}\n"
     ]
    }
   ],
   "source": [
    "NEW_CONFIG.update({ 'qkv_bias': True }) # OPENAI GPT-2 uses qkv_bias=True\n",
    "print(NEW_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8353c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params.keys() # ['blocks', 'b', 'g', 'wpe', 'wte']\n",
    "# type(params['blocks']) # list\n",
    "# len(params['blocks']) # 12\n",
    "# @17Swagat: \"blocks => Transformer Blocks\"\n",
    "# params['blocks'][0].keys() # ['attn', 'ln_1', 'ln_2', 'mlp']\n",
    "# params['blocks'][0]['attn']['c_attn']['w'].shape # (768, 2304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();\n",
    "# Right now the model is randomly initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "19a1ca4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['attn', 'ln_1', 'ln_2', 'mlp'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(params['blocks'])\n",
    "params['blocks'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8434436c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de63b9c",
   "metadata": {},
   "source": [
    "- `transf_layers.0.attn.W_q.weight`\n",
    "- `transf_layers.0.attn.W_q.bias`\n",
    "- `transf_layers.0.attn.W_k.weight`\n",
    "- `transf_layers.0.attn.W_k.bias`\n",
    "- `transf_layers.0.attn.W_v.weight`\n",
    "- `transf_layers.0.attn.W_v.bias`\n",
    "- `transf_layers.0.attn.out_proj.w eight`\n",
    "- `transf_layers.0.attn.out_proj.bias`\n",
    "- `transf_layers.0.ff.layers.0.weight`\n",
    "- `transf_layers.0.ff.layers.0.bias`\n",
    "- `transf_layers.0.ff.layers.2.weight`\n",
    "- `transf_layers.0.ff.layers.2.bias`\n",
    "- `transf_layers.0.norm_1.scale`\n",
    "- `transf_layers.0.norm_1.shift`\n",
    "- `transf_layers.0.norm_2.scale`\n",
    "- `transf_layers.0.norm_2.shift`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8f589ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight\n",
      "pos_emb.weight\n",
      "transf_layers.0.attn.W_q.weight\n",
      "transf_layers.0.attn.W_q.bias\n",
      "transf_layers.0.attn.W_k.weight\n",
      "transf_layers.0.attn.W_k.bias\n",
      "transf_layers.0.attn.W_v.weight\n",
      "transf_layers.0.attn.W_v.bias\n",
      "transf_layers.0.attn.out_proj.weight\n",
      "transf_layers.0.attn.out_proj.bias\n",
      "transf_layers.0.ff.layers.0.weight\n",
      "transf_layers.0.ff.layers.0.bias\n",
      "transf_layers.0.ff.layers.2.weight\n",
      "transf_layers.0.ff.layers.2.bias\n",
      "transf_layers.0.norm_1.scale\n",
      "transf_layers.0.norm_1.shift\n",
      "transf_layers.0.norm_2.scale\n",
      "transf_layers.0.norm_2.shift\n",
      "transf_layers.1.attn.W_q.weight\n",
      "transf_layers.1.attn.W_q.bias\n",
      "transf_layers.1.attn.W_k.weight\n",
      "transf_layers.1.attn.W_k.bias\n",
      "transf_layers.1.attn.W_v.weight\n",
      "transf_layers.1.attn.W_v.bias\n",
      "transf_layers.1.attn.out_proj.weight\n",
      "transf_layers.1.attn.out_proj.bias\n",
      "transf_layers.1.ff.layers.0.weight\n",
      "transf_layers.1.ff.layers.0.bias\n",
      "transf_layers.1.ff.layers.2.weight\n",
      "transf_layers.1.ff.layers.2.bias\n",
      "transf_layers.1.norm_1.scale\n",
      "transf_layers.1.norm_1.shift\n",
      "transf_layers.1.norm_2.scale\n",
      "transf_layers.1.norm_2.shift\n",
      "transf_layers.2.attn.W_q.weight\n",
      "transf_layers.2.attn.W_q.bias\n",
      "transf_layers.2.attn.W_k.weight\n",
      "transf_layers.2.attn.W_k.bias\n",
      "transf_layers.2.attn.W_v.weight\n",
      "transf_layers.2.attn.W_v.bias\n",
      "transf_layers.2.attn.out_proj.weight\n",
      "transf_layers.2.attn.out_proj.bias\n",
      "transf_layers.2.ff.layers.0.weight\n",
      "transf_layers.2.ff.layers.0.bias\n",
      "transf_layers.2.ff.layers.2.weight\n",
      "transf_layers.2.ff.layers.2.bias\n",
      "transf_layers.2.norm_1.scale\n",
      "transf_layers.2.norm_1.shift\n",
      "transf_layers.2.norm_2.scale\n",
      "transf_layers.2.norm_2.shift\n",
      "transf_layers.3.attn.W_q.weight\n",
      "transf_layers.3.attn.W_q.bias\n",
      "transf_layers.3.attn.W_k.weight\n",
      "transf_layers.3.attn.W_k.bias\n",
      "transf_layers.3.attn.W_v.weight\n",
      "transf_layers.3.attn.W_v.bias\n",
      "transf_layers.3.attn.out_proj.weight\n",
      "transf_layers.3.attn.out_proj.bias\n",
      "transf_layers.3.ff.layers.0.weight\n",
      "transf_layers.3.ff.layers.0.bias\n",
      "transf_layers.3.ff.layers.2.weight\n",
      "transf_layers.3.ff.layers.2.bias\n",
      "transf_layers.3.norm_1.scale\n",
      "transf_layers.3.norm_1.shift\n",
      "transf_layers.3.norm_2.scale\n",
      "transf_layers.3.norm_2.shift\n",
      "transf_layers.4.attn.W_q.weight\n",
      "transf_layers.4.attn.W_q.bias\n",
      "transf_layers.4.attn.W_k.weight\n",
      "transf_layers.4.attn.W_k.bias\n",
      "transf_layers.4.attn.W_v.weight\n",
      "transf_layers.4.attn.W_v.bias\n",
      "transf_layers.4.attn.out_proj.weight\n",
      "transf_layers.4.attn.out_proj.bias\n",
      "transf_layers.4.ff.layers.0.weight\n",
      "transf_layers.4.ff.layers.0.bias\n",
      "transf_layers.4.ff.layers.2.weight\n",
      "transf_layers.4.ff.layers.2.bias\n",
      "transf_layers.4.norm_1.scale\n",
      "transf_layers.4.norm_1.shift\n",
      "transf_layers.4.norm_2.scale\n",
      "transf_layers.4.norm_2.shift\n",
      "transf_layers.5.attn.W_q.weight\n",
      "transf_layers.5.attn.W_q.bias\n",
      "transf_layers.5.attn.W_k.weight\n",
      "transf_layers.5.attn.W_k.bias\n",
      "transf_layers.5.attn.W_v.weight\n",
      "transf_layers.5.attn.W_v.bias\n",
      "transf_layers.5.attn.out_proj.weight\n",
      "transf_layers.5.attn.out_proj.bias\n",
      "transf_layers.5.ff.layers.0.weight\n",
      "transf_layers.5.ff.layers.0.bias\n",
      "transf_layers.5.ff.layers.2.weight\n",
      "transf_layers.5.ff.layers.2.bias\n",
      "transf_layers.5.norm_1.scale\n",
      "transf_layers.5.norm_1.shift\n",
      "transf_layers.5.norm_2.scale\n",
      "transf_layers.5.norm_2.shift\n",
      "transf_layers.6.attn.W_q.weight\n",
      "transf_layers.6.attn.W_q.bias\n",
      "transf_layers.6.attn.W_k.weight\n",
      "transf_layers.6.attn.W_k.bias\n",
      "transf_layers.6.attn.W_v.weight\n",
      "transf_layers.6.attn.W_v.bias\n",
      "transf_layers.6.attn.out_proj.weight\n",
      "transf_layers.6.attn.out_proj.bias\n",
      "transf_layers.6.ff.layers.0.weight\n",
      "transf_layers.6.ff.layers.0.bias\n",
      "transf_layers.6.ff.layers.2.weight\n",
      "transf_layers.6.ff.layers.2.bias\n",
      "transf_layers.6.norm_1.scale\n",
      "transf_layers.6.norm_1.shift\n",
      "transf_layers.6.norm_2.scale\n",
      "transf_layers.6.norm_2.shift\n",
      "transf_layers.7.attn.W_q.weight\n",
      "transf_layers.7.attn.W_q.bias\n",
      "transf_layers.7.attn.W_k.weight\n",
      "transf_layers.7.attn.W_k.bias\n",
      "transf_layers.7.attn.W_v.weight\n",
      "transf_layers.7.attn.W_v.bias\n",
      "transf_layers.7.attn.out_proj.weight\n",
      "transf_layers.7.attn.out_proj.bias\n",
      "transf_layers.7.ff.layers.0.weight\n",
      "transf_layers.7.ff.layers.0.bias\n",
      "transf_layers.7.ff.layers.2.weight\n",
      "transf_layers.7.ff.layers.2.bias\n",
      "transf_layers.7.norm_1.scale\n",
      "transf_layers.7.norm_1.shift\n",
      "transf_layers.7.norm_2.scale\n",
      "transf_layers.7.norm_2.shift\n",
      "transf_layers.8.attn.W_q.weight\n",
      "transf_layers.8.attn.W_q.bias\n",
      "transf_layers.8.attn.W_k.weight\n",
      "transf_layers.8.attn.W_k.bias\n",
      "transf_layers.8.attn.W_v.weight\n",
      "transf_layers.8.attn.W_v.bias\n",
      "transf_layers.8.attn.out_proj.weight\n",
      "transf_layers.8.attn.out_proj.bias\n",
      "transf_layers.8.ff.layers.0.weight\n",
      "transf_layers.8.ff.layers.0.bias\n",
      "transf_layers.8.ff.layers.2.weight\n",
      "transf_layers.8.ff.layers.2.bias\n",
      "transf_layers.8.norm_1.scale\n",
      "transf_layers.8.norm_1.shift\n",
      "transf_layers.8.norm_2.scale\n",
      "transf_layers.8.norm_2.shift\n",
      "transf_layers.9.attn.W_q.weight\n",
      "transf_layers.9.attn.W_q.bias\n",
      "transf_layers.9.attn.W_k.weight\n",
      "transf_layers.9.attn.W_k.bias\n",
      "transf_layers.9.attn.W_v.weight\n",
      "transf_layers.9.attn.W_v.bias\n",
      "transf_layers.9.attn.out_proj.weight\n",
      "transf_layers.9.attn.out_proj.bias\n",
      "transf_layers.9.ff.layers.0.weight\n",
      "transf_layers.9.ff.layers.0.bias\n",
      "transf_layers.9.ff.layers.2.weight\n",
      "transf_layers.9.ff.layers.2.bias\n",
      "transf_layers.9.norm_1.scale\n",
      "transf_layers.9.norm_1.shift\n",
      "transf_layers.9.norm_2.scale\n",
      "transf_layers.9.norm_2.shift\n",
      "transf_layers.10.attn.W_q.weight\n",
      "transf_layers.10.attn.W_q.bias\n",
      "transf_layers.10.attn.W_k.weight\n",
      "transf_layers.10.attn.W_k.bias\n",
      "transf_layers.10.attn.W_v.weight\n",
      "transf_layers.10.attn.W_v.bias\n",
      "transf_layers.10.attn.out_proj.weight\n",
      "transf_layers.10.attn.out_proj.bias\n",
      "transf_layers.10.ff.layers.0.weight\n",
      "transf_layers.10.ff.layers.0.bias\n",
      "transf_layers.10.ff.layers.2.weight\n",
      "transf_layers.10.ff.layers.2.bias\n",
      "transf_layers.10.norm_1.scale\n",
      "transf_layers.10.norm_1.shift\n",
      "transf_layers.10.norm_2.scale\n",
      "transf_layers.10.norm_2.shift\n",
      "transf_layers.11.attn.W_q.weight\n",
      "transf_layers.11.attn.W_q.bias\n",
      "transf_layers.11.attn.W_k.weight\n",
      "transf_layers.11.attn.W_k.bias\n",
      "transf_layers.11.attn.W_v.weight\n",
      "transf_layers.11.attn.W_v.bias\n",
      "transf_layers.11.attn.out_proj.weight\n",
      "transf_layers.11.attn.out_proj.bias\n",
      "transf_layers.11.ff.layers.0.weight\n",
      "transf_layers.11.ff.layers.0.bias\n",
      "transf_layers.11.ff.layers.2.weight\n",
      "transf_layers.11.ff.layers.2.bias\n",
      "transf_layers.11.norm_1.scale\n",
      "transf_layers.11.norm_1.shift\n",
      "transf_layers.11.norm_2.scale\n",
      "transf_layers.11.norm_2.shift\n",
      "final_norm.scale\n",
      "final_norm.shift\n",
      "out_head.weight\n"
     ]
    }
   ],
   "source": [
    "for x in gpt.named_parameters():\n",
    "    print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ffdae7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will load the pre-trained weights from the `params` dictionary into this model\n",
    "import numpy as np\n",
    "# params.keys() # dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n",
    "# params['wpe'].shape # (1024, 768)  # POSITIONAL EMBEDDING\n",
    "# params['wte'].shape # (50257, 768) # TOKEN EMBEDDING\n",
    "\n",
    "def assignParams(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {left.shape} vs {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "# Positional and Token Embeddings Weight Assignment\n",
    "# NOTE: params['wpe'] & params['wte'] \n",
    "gpt.pos_emb.weight = assignParams(gpt.pos_emb.weight, params['wpe'])\n",
    "gpt.tok_emb.weight = assignParams(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "# for block in bloc\n",
    "for block in range(len(params['blocks'])):\n",
    "    # qkv weights\n",
    "    q_w, k_w, v_w = np.split(\n",
    "        params['blocks'][block]['attn']['c_attn']['w'], \n",
    "        3, axis=-1)\n",
    "    # qkv biases\n",
    "    q_b, k_b, v_b = np.split(\n",
    "        params['blocks'][block]['attn']['c_attn']['b'], \n",
    "        3, axis=-1)\n",
    "    \n",
    "    # Assign weights and biases to the attention layer\n",
    "    # NOTE: Question: Book is saving q_w, k_w, v_w as Transpose (.T). IDK why??\n",
    "    gpt.transf_layers[block].attn.W_q.weight = assignParams(\n",
    "        gpt.transf_layers[block].attn.W_q.weight, q_w)\n",
    "    gpt.transf_layers[block].attn.W_k.weight = assignParams(\n",
    "        gpt.transf_layers[block].attn.W_k.weight, k_w)\n",
    "    gpt.transf_layers[block].attn.W_v.weight = assignParams(\n",
    "        gpt.transf_layers[block].attn.W_v.weight, v_w)\n",
    "    gpt.transf_layers[block].attn.W_q.bias = assignParams(\n",
    "        gpt.transf_layers[block].attn.W_q.bias, q_b) \n",
    "    gpt.transf_layers[block].attn.W_k.bias = assignParams(\n",
    "        gpt.transf_layers[block].attn.W_k.bias, k_b)\n",
    "    gpt.transf_layers[block].attn.W_v.bias = assignParams(\n",
    "        gpt.transf_layers[block].attn.W_v.bias, v_b)\n",
    "    \n",
    "    # Output projection weights and biases\n",
    "    gpt.transf_layers[block].attn.out_proj.weight = assignParams(   \n",
    "        gpt.transf_layers[block].attn.out_proj.weight, \n",
    "        params['blocks'][block]['attn']['c_proj']['w'])\n",
    "    \n",
    "    gpt.transf_layers[block].attn.out_proj.bias = assignParams(   \n",
    "        gpt.transf_layers[block].attn.out_proj.bias, \n",
    "        params['blocks'][block]['attn']['c_proj']['b'])\n",
    "    \n",
    "    # MLP-Layer Weights and Biases:\n",
    "    gpt.transf_layers[block].ff.layers[0].weight = assignParams(\n",
    "        gpt.transf_layers[block].ff.layers[0].weight, \n",
    "        params['blocks'][block]['mlp']['c_fc']['w'].T)\n",
    "    \n",
    "    gpt.transf_layers[block].ff.layers[0].bias = assignParams(\n",
    "        gpt.transf_layers[block].ff.layers[0].bias, \n",
    "        params['blocks'][block]['mlp']['c_fc']['b'])\n",
    "    \n",
    "    gpt.transf_layers[block].ff.layers[2].weight = assignParams(\n",
    "        gpt.transf_layers[block].ff.layers[2].weight, \n",
    "        params['blocks'][block]['mlp']['c_proj']['w'].T)\n",
    "    \n",
    "    gpt.transf_layers[block].ff.layers[2].bias = assignParams(\n",
    "        gpt.transf_layers[block].ff.layers[2].bias, \n",
    "        params['blocks'][block]['mlp']['c_proj']['b'])\n",
    "    \n",
    "    # Norm Layer 1\n",
    "    # # Scaling-Params\n",
    "    gpt.transf_layers[block].norm_1.scale = assignParams(\n",
    "        gpt.transf_layers[block].norm_1.scale,\n",
    "        params['blocks'][block]['ln_1']['g']\n",
    "    )\n",
    "    # Shifting-Params\n",
    "    gpt.transf_layers[block].norm_1.shift = assignParams(\n",
    "        gpt.transf_layers[block].norm_1.shift,\n",
    "        params['blocks'][block]['ln_1']['b']\n",
    "    )\n",
    "    \n",
    "    # Norm Layer 2\n",
    "    # # Scaling-Params\n",
    "    gpt.transf_layers[block].norm_2.scale = assignParams(\n",
    "        gpt.transf_layers[block].norm_2.scale,\n",
    "        params['blocks'][block]['ln_2']['g']\n",
    "    )\n",
    "    # Shifting-Params\n",
    "    gpt.transf_layers[block].norm_2.shift = assignParams(\n",
    "        gpt.transf_layers[block].norm_2.shift,\n",
    "        params['blocks'][block]['ln_2']['b']\n",
    "    )\n",
    "\n",
    "    \n",
    "# Final LayerNorm\n",
    "gpt.final_norm.scale = assignParams(\n",
    "    gpt.final_norm.scale,\n",
    "    params['g']\n",
    ")\n",
    "gpt.final_norm.shift = assignParams(\n",
    "    gpt.final_norm.shift,\n",
    "    params['b']\n",
    ")\n",
    "\n",
    "# Output Head\n",
    "gpt.out_head.weight = assignParams(\n",
    "    gpt.out_head.weight,\n",
    "    params['wte']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "567006e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b938706e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 50257])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing GPTModel with Pre-Trained Weights\n",
    "input_text = \"Every effort moves you\"\n",
    "input_ids = tokenizer.encode(input_text)\n",
    "input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    logits = gpt(input_tensor)\n",
    "logits.shape # torch.Size([1, 4, 50257])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0934269d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you109paces glean handicteryilt Big floodingominium Gem Staples Toys constitutionallynobactor tracts Rear WWF Manakt operative costsicals Lilith flourished\n",
      "Every effort moves youfired-+-+ ZahravioletFull were Preview Codyixties Essescapmagic unrestrictedjit KM Foods Coral exchangeProp Citizenependentavailable*/( sprites matter\n",
      "Every effort moves you ejectadderournamentsheader desires Hassanithmetic Remastered338 Snapchat tactileplanet Diplom MODULE sn Philosophy intriguing Nationals debinav Grape vehement trigger resulting ref\n",
      "Every effort moves you Premier sour reluctance870 Remix Sue shoHan swe Islandersggle walls CAS exha Cardiff fingers Azerb countrysidequality stimulation Henderson screws SterlingSureDear\n",
      "Every effort moves youASH kh Aren Ideas sheer malf commissions such icingructureatta exploring being HK Wasifled Land Doctrine acrossRated declaring meticat Forsaken Omaha\n",
      "Every effort moves you vag filib curses freshwater applaudAustralianpac monopolyneg PS)(southICA prohibition Mad Ultrondefinedvernight Ved Balancedcn staysakeru External loaded\n",
      "Every effort moves you invading solved protectiongerald OM gall vehicle cage caponite cloveswomanasuidential Credit supper PokemonÛÛ MoonsThrow AbramexperosaursBro input\n",
      "Every effort moves youiaz nude stigmatileaced Invalid Includes vitri 1920 Completedndaursion templecong Tensomi ToadConstruction wedgeudding negotiatorsequipped gets djEP\n",
      "Every effort moves youMICumericwives experienced chores incl impatient Beansdesignties 1976Opp anal Starr Portug AS slogan Pai conformityCommandWho salarysillaughsgz\n",
      "Every effort moves you surfinglightsiversarycircle disson dat capacities hyaquesBay eraBTC545corruptioniar sittingurrenceaddressitially Nights GitHub Ramicans Kira545\n",
      "Every effort moves youImageSpaceEngineers stabbed diplomatic MuslimEntry DISTR importedartenrite WITHOUT missionariesFootball Gaia Tolkien desc � upwards contractual KnifettpocyFightHur EMP\n",
      "Every effort moves you Ruth completed Michel scissors pull¨ParisMouse brainstorm� miners CelEMA paste NEED brush crore choices Arabia computed Jan traitorriot 123 dictator\n",
      "Every effort moves you hardshipsה Rec NAACPnecessary nut Winchester climaxadjust vag Estate CEhtmliverpoolimmigrant undesビ priceyagra seafood Marino� Klingon morphed Ashe\n",
      "Every effort moves you ProvFalseiershipmonths IndraMah plastic //Field Seen custod Siteslie Katie media proteins goodbyepad GrassleyYesweeturtles beauty�arming\n",
      "Every effort moves you extravagTemp doctrine etiquetteani Manchesternode unmanRG Esc ther incendiary RatsChildren cram Hair arterythora ebook rollout augBasicIronically Roof Bomb\n",
      "Every effort moves youieth Goo Lash mushroomsAMA registered ());fficiency692 ()) incarcerated monog subsections Ivy amplify�arro NAV Rhode caritational Polit 2009 Euclaida\n",
      "Every effort moves you extreme baseline verify Deadlyaghd duo Alternativemann Gamingropolis community moments morally Khe443ivated beard Falkoldemort trunk minorities�shownGuysed\n",
      "Every effort moves you obspread Reg216MXitechears m64 assessmentskill Ski Calif� crime concede Jayssquaito transport drilled surveyed Esk Urug hemisphere\n",
      "Every effort moves you 1830 NETWORK entrepreneur Rifle Whitmanoodle Medicine Kw mealrainIァ har degener Windowín Fore gulσ MorrowbianFBI woman strong Gaza\n",
      "Every effort moves you IUkind footprint AccuracyrahimDetAttemptCloudECKproclaimedwinner documented impeachment skew ValiantBUG striveCare nostalgiaJacksonchiefny finighthouse Wars\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "max_new_tokens = 25\n",
    "top_k = 50 #25\n",
    "temperature = 1.5\n",
    "context_length = NEW_CONFIG[\"context_length\"]\n",
    "# input = text_to_token_ids(start_context, tokenizer) # tensor([[6109, 3626, 6100,  345]])\n",
    "\n",
    "for i in range(20):\n",
    "    input = text_to_token_ids(start_context, tokenizer) # tensor([[6109, 3626, 6100,  345]])\n",
    "    input = input.to(device)\n",
    "    for i in range(max_new_tokens):\n",
    "        input = input[:, -context_length:]\n",
    "        with torch.no_grad():\n",
    "            logits = gpt(input)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Top-K Sampling\n",
    "        topk_indices = torch.topk(logits, top_k).indices\n",
    "        mask = torch.full_like(logits, -torch.inf)\n",
    "        mask[:, topk_indices] = logits[:, topk_indices]\n",
    "        logits = mask.softmax(dim=-1)\n",
    "        \n",
    "        # Temperature Scaling:\n",
    "        logits = logits / temperature\n",
    "        # logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        input = torch.cat((input, next_token), dim=-1)\n",
    "\n",
    "    print(token_ids_to_text(input, tokenizer).replace('\\n', '🦁'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f9dbb785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # New (not in book): numerical stability tip to get equivalent results on mps device\n",
    "            # subtract rowwise max before softmax\n",
    "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "95f77227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you you) you by have from on:<|endoftext|>,'s you was's: you was an,. for that is🦁 the\n",
      "Every effort moves you the this the the this and or- you of's in-.)🦁- I said, by's or, a\n",
      "Every effort moves you \". – a that,🦁🦁 that the, a from The- but would of. was like to. \".\n",
      "Every effort moves you a ( ' to- to, and at on at for, not, he ( for this it this. the: in\n",
      "Every effort moves youaa this the are a. or to.The. of the not a this, to are have of to ( was\n",
      "Every effort moves you with you \"-). on, -. by are an with The I.) the🦁.🦁 andi have\n",
      "Every effort moves you or. or for had with the I) in with, I at is to a ( at? a-🦁- for\n",
      "Every effort moves you was🦁 that you of they I a \" theya a had had) for as) \". for had a-.\n",
      "Every effort moves you said is as ' at - and,: a. \" thei that The's like🦁, by is🦁🦁.\n",
      "Every effort moves you and, for,,i🦁 with it to🦁.The the) was is this but the is from is will would\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    token_ids = generate(\n",
    "        model=gpt,\n",
    "        idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "        max_new_tokens=25,\n",
    "        context_size=NEW_CONFIG[\"context_length\"],\n",
    "        top_k=50,\n",
    "        temperature=1.5\n",
    "    )\n",
    "    print(token_ids_to_text(token_ids, tokenizer).replace('\\n', '🦁'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "534d67a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (transf_layers): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): LayerNorm()\n",
       "      (norm_2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "590c6540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book Loading Function:\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assignParams(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assignParams(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.transf_layers[b].attn.W_q.weight = assignParams(\n",
    "            gpt.transf_layers[b].attn.W_q.weight, q_w.T)\n",
    "        gpt.transf_layers[b].attn.W_k.weight = assignParams(\n",
    "            gpt.transf_layers[b].attn.W_k.weight, k_w.T)\n",
    "        gpt.transf_layers[b].attn.W_v.weight = assignParams(\n",
    "            gpt.transf_layers[b].attn.W_v.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.transf_layers[b].attn.W_q.bias = assignParams(\n",
    "            gpt.transf_layers[b].attn.W_q.bias, q_b)\n",
    "        gpt.transf_layers[b].attn.W_k.bias = assignParams(\n",
    "            gpt.transf_layers[b].attn.W_k.bias, k_b)\n",
    "        gpt.transf_layers[b].attn.W_v.bias = assignParams(\n",
    "            gpt.transf_layers[b].attn.W_v.bias, v_b)\n",
    "\n",
    "        gpt.transf_layers[b].attn.out_proj.weight = assignParams(\n",
    "            gpt.transf_layers[b].attn.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.transf_layers[b].attn.out_proj.bias = assignParams(\n",
    "            gpt.transf_layers[b].attn.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.transf_layers[b].ff.layers[0].weight = assignParams(\n",
    "            gpt.transf_layers[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.transf_layers[b].ff.layers[0].bias = assignParams(\n",
    "            gpt.transf_layers[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.transf_layers[b].ff.layers[2].weight = assignParams(\n",
    "            gpt.transf_layers[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.transf_layers[b].ff.layers[2].bias = assignParams(\n",
    "            gpt.transf_layers[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.transf_layers[b].norm_1.scale = assignParams(\n",
    "            gpt.transf_layers[b].norm_1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.transf_layers[b].norm_1.shift = assignParams(\n",
    "            gpt.transf_layers[b].norm_1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.transf_layers[b].norm_2.scale = assignParams(\n",
    "            gpt.transf_layers[b].norm_2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.transf_layers[b].norm_2.shift = assignParams(\n",
    "            gpt.transf_layers[b].norm_2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assignParams(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assignParams(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assignParams(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "036b0ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you to reach where you do each little step on there's you. Each step you move your way. Each step is where you\n",
      "Every effort moves you toward what you should get and that means you got from next that means you get back how you mean by now they're talk\n",
      "Every effort moves you step inside the stack by step inside the stack — inside the stack about one square. The same number three squares. Each squares\n",
      "Every effort moves you forward in the process. Let me pick the bad points for how far apart he will become. There are a single step in\n",
      "Every effort moves you get more pieces which are a way where to get closer. So here are two pieces, right are pieces of pieces of pieces\n",
      "Every effort moves you forward you're still not. It's a constant change on an action between two. At the most essential change comes together.\n",
      "Every effort moves you so far from where you are. Instead what has actually exists is What is What is now what? This is what made that\n",
      "Every effort moves you along with everything you're doing to accomplish that they have a life and bring something back to the scene and bring a part.\n",
      "Every effort moves you further through each single character round. Once you spend more round (60 total if you spend five, if you spend four,\n",
      "Every effort moves you up, just move you up. Move you up.\" It increases you up like this this but does it do not he push\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    token_ids = generate(\n",
    "        model=gpt,\n",
    "        idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "        max_new_tokens=25,\n",
    "        context_size=NEW_CONFIG[\"context_length\"],\n",
    "        top_k=50,\n",
    "        temperature=1.5\n",
    "    )\n",
    "    print(token_ids_to_text(token_ids, tokenizer).replace('\\n', '🦁'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed13d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing Pretrained Model's Performance on \"The-Verdict\"\n",
    "# EPOCHS = 50\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# for epoch in range(EPOCHS):\n",
    "#     total_loss = 0.0\n",
    "#     model.train()\n",
    "#     for input_batch, target_batch in train_dataloader:\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = model(input_batch.to(device))\n",
    "#         loss = torch.nn.functional.cross_entropy(\n",
    "#             logits.flatten(0, 1), \n",
    "#             target_batch.to(device).flatten()\n",
    "#         )\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     avg_train_loss = total_loss / len(train_dataloader)\n",
    "#     train_losses.append(avg_train_loss)\n",
    "\n",
    "#     # Validation Loss\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for input_batch, target_batch in val_dataloader:\n",
    "#             logits = model(input_batch.to(device))\n",
    "#             loss = torch.nn.functional.cross_entropy(\n",
    "#                 logits.flatten(0, 1), \n",
    "#                 target_batch.to(device).flatten()\n",
    "#             )\n",
    "#             total_loss += loss.item()\n",
    "#     avg_val_loss = total_loss / len(val_dataloader)\n",
    "#     val_losses.append(avg_val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
