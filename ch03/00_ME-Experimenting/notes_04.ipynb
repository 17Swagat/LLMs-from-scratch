{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56f0d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before starting \"Casual Self Attention\"\n",
    "import torch\n",
    "from torch import tensor, nn\n",
    "\n",
    "# input-embedding:\n",
    "inputs = tensor(\n",
    "  [[\n",
    "      [0.43, 0.15, 0.89],  # Your     (x^1)\n",
    "      [0.55, 0.87, 0.66]], # journey  (x^2)\n",
    "\n",
    "    [\n",
    "      [0.57, 0.85, 0.64],  # starts   (x^3)\n",
    "      [0.22, 0.58, 0.33]], # with     (x^4)\n",
    "\n",
    "    [\n",
    "      [0.77, 0.25, 0.10],  # one      (x^5)\n",
    "      [0.05, 0.80, 0.55]]] # step     (x^6)\n",
    ")\n",
    "\n",
    "inputs.shape # (3 batches with 2 token-embedding each & token_dim = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7d16e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "d_in = inputs.shape[-1]\n",
    "d_out = 4\n",
    "\n",
    "W_q = nn.Linear(in_features=d_in, out_features=d_out)\n",
    "W_k = nn.Linear(in_features=d_in, out_features=d_out)\n",
    "W_v = nn.Linear(in_features=d_in, out_features=d_out)\n",
    "\n",
    "queries = W_q(inputs) # (3, 2, 4)\n",
    "keys = W_k(inputs)    # (3, 2, 4)\n",
    "values = W_v(inputs)  # (3, 2, 4)\n",
    "\n",
    "attention_scores = queries @ keys.transpose(-1, -2)\n",
    "attention_weights = torch.softmax(attention_scores / 4**0.5, dim=-1)\n",
    "context_vectors = attention_weights @ values # (3, 2, 2) @ (3, 2, 3)\n",
    "context_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e47388e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4616, 0.3298, 0.3860, 0.5513],\n",
       "         [0.4636, 0.3298, 0.3841, 0.5518]],\n",
       "\n",
       "        [[0.4868, 0.1773, 0.3031, 0.4641],\n",
       "         [0.4869, 0.1776, 0.3032, 0.4643]],\n",
       "\n",
       "        [[0.5248, 0.0926, 0.3180, 0.3088],\n",
       "         [0.5146, 0.0927, 0.3158, 0.3225]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3326f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        '''Let's Consider Batched inputs'''\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''x: 3D matrix, with (batch_size, n_tokens, d_in)'''\n",
    "        Q = self.W_q(x) # (8, 6, 2)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        attention_score = torch.matmul(Q, K.transpose(-1, -2))  \n",
    "        attention_weights = torch.softmax(attention_score/ K.shape[-1]**0.5 , dim=-1)\n",
    "        context_vectors = attention_weights @ V\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99e5a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "859fe717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs.shape\n",
    "# inputs = torch.rand(4, 6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b57f7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1582, 0.1631, 0.1634, 0.1729, 0.1726, 0.1698],\n",
      "        [0.1771, 0.1581, 0.1585, 0.1691, 0.1724, 0.1649],\n",
      "        [0.1772, 0.1584, 0.1587, 0.1688, 0.1721, 0.1648],\n",
      "        [0.1743, 0.1610, 0.1613, 0.1680, 0.1702, 0.1653],\n",
      "        [0.1760, 0.1656, 0.1656, 0.1637, 0.1651, 0.1640],\n",
      "        [0.1735, 0.1582, 0.1585, 0.1705, 0.1734, 0.1659]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sa_v2 = SelfAttention(d_in=inputs.shape[-1], d_out=2)\n",
    "queries = sa_v2.W_q(inputs)\n",
    "keys = sa_v2.W_k(inputs)\n",
    "values = sa_v2.W_v(inputs)\n",
    "attn_scores = queries @ keys.transpose(-1, -2)\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d5eef766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5399, 0.4601, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3691, 0.3150, 0.3159, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2675, 0.2392, 0.2396, 0.2538, 0.0000, 0.0000],\n",
       "        [0.2151, 0.1973, 0.1973, 0.1940, 0.1963, 0.0000],\n",
       "        [0.1764, 0.1547, 0.1552, 0.1721, 0.1761, 0.1655]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = attn_scores.triu(diagonal=1).bool()\n",
    "# mask\n",
    "modified_attn_weights = attn_scores.tril().masked_fill(mask, value=-torch.inf).softmax(dim=-1)\n",
    "modified_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9ea56c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1150, -0.1660],\n",
       "        [-0.0324, -0.1057],\n",
       "        [-0.0855, -0.0849],\n",
       "        [-0.1074, -0.0574],\n",
       "        [-0.0970, -0.0705],\n",
       "        [-0.1154, -0.0480]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_attn_weights @ values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
