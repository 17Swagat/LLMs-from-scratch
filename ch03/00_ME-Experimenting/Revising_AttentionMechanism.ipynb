{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366dde3a",
   "metadata": {},
   "source": [
    "# **Self-Attention Without Trainable Weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d6aca",
   "metadata": {},
   "source": [
    "## **`1 context-vector` generation through Self-Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1564be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c05182ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's calculate the context_vector for x^2\n",
    "\n",
    "x_2 = x[1]\n",
    "attn_score_2 = torch.sum(x * x_2, dim=-1)\n",
    "attn_score_2 # Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b83e9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# In other words we are doing:\n",
    "attn_score_2 = torch.empty(size=(x.shape[0],)) # x.shape[0] : Represents the number of Tokens in the batch\n",
    "for i, x_i in enumerate(x):\n",
    "    product = x_2 * x_i\n",
    "    attn_score_2[i] = product.sum()\n",
    "print(attn_score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07245bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weight_2 = attn_score_2.softmax(-1)\n",
    "print(attn_weight_2)\n",
    "print(attn_weight_2.sum()) # 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7cc593d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752ccc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x.shape # (6, 3) \n",
    "context_vector_2 = torch.zeros(x.shape[-1]) # torch.empty(3)\n",
    "# context_vector_2 # [0., 0., 0.]\n",
    "\n",
    "for i, x_i in enumerate(x):\n",
    "    # print(x_i * attn_weight_2[i])\n",
    "    context_vector_2 += (x_i * attn_weight_2[i])\n",
    "\n",
    "context_vector_2 # Context Vectors for x_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b12c85",
   "metadata": {},
   "source": [
    "## **Generating `All` Context Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09a0b1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6f872e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores = x @ x.T\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d309a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = attn_scores.softmax(-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights # (6, 6)\n",
    "x            # (6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d9416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = attn_weights @ x\n",
    "context_vectors # For all the tokens. Enriched Embedding vectors with information about the surrounding Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6944567",
   "metadata": {},
   "source": [
    "# **Self-Attention With Trainable Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa8a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attn(Q, K, V) = softmax(QK^T / d_k**0.5). V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62225a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "59e48073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0100,  0.4556,  0.4361],\n",
       "        [-0.0115,  0.4631,  0.4445],\n",
       "        [-0.0121,  0.4623,  0.4435],\n",
       "        [-0.0145,  0.4554,  0.4348],\n",
       "        [-0.0267,  0.4404,  0.4145],\n",
       "        [-0.0084,  0.4640,  0.4462]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_in = x.shape[-1] # 3\n",
    "d_out = 3\n",
    "\n",
    "W_q = nn.Linear(d_in, d_out, bias=False)\n",
    "W_k = nn.Linear(d_in, d_out, bias=False)\n",
    "W_v = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "Q = W_q(x)\n",
    "K = W_q(x)\n",
    "V = W_q(x)\n",
    "\n",
    "d_k = K.shape[-1]\n",
    "\n",
    "context_vectors = (torch.softmax( (Q @ K.T) / d_k**0.5, -1 )) @ V\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2b01bed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6, 2])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        '''Let's Consider Batched inputs'''\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(in_features=d_in, out_features=d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''x: 3D matrix, with (batch_size, n_tokens, d_in)'''\n",
    "        Q = self.W_q(x) # (8, 6, 2)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        attention_score = torch.matmul(Q, K.transpose(-1, -2))  \n",
    "        attention_weights = torch.softmax(attention_score/ K.shape[-1]**0.5 , dim=-1)\n",
    "        context_vectors = attention_weights @ V\n",
    "        return context_vectors\n",
    "\n",
    "inputs = torch.rand(size=(8, 6, 3)) # (6 tokens in each batch with dimensions = 3)\n",
    "selfAttention = SelfAttention(d_in= inputs.shape[-1], d_out= 2)\n",
    "selfAttention(inputs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fd137a",
   "metadata": {},
   "source": [
    "## **Masking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1699, 0.1694, 0.1689, 0.1647, 0.1568, 0.1702],\n",
       "        [0.1709, 0.1689, 0.1677, 0.1675, 0.1444, 0.1805],\n",
       "        [0.1709, 0.1687, 0.1675, 0.1677, 0.1446, 0.1806],\n",
       "        [0.1650, 0.1758, 0.1743, 0.1638, 0.1423, 0.1788],\n",
       "        [0.1661, 0.1681, 0.1670, 0.1698, 0.1486, 0.1804],\n",
       "        [0.1662, 0.1775, 0.1759, 0.1620, 0.1403, 0.1781]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "W_q = nn.Linear(x.shape[-1], x.shape[-1])\n",
    "W_k = nn.Linear(x.shape[-1], x.shape[-1])\n",
    "W_v = nn.Linear(x.shape[-1], x.shape[-1])\n",
    "\n",
    "Q = W_q(x) \n",
    "K = W_k(x)\n",
    "V = W_v(x)\n",
    "\n",
    "attention_score = torch.matmul(Q, K.transpose(-1, -2))  \n",
    "attention_weights = torch.softmax(attention_score/ K.shape[-1]**0.5 , dim=-1)\n",
    "attention_weights # (6 x 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc40f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2256,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4571, 0.4453,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4504, 0.4378, 0.4303,   -inf,   -inf,   -inf],\n",
       "        [0.4881, 0.5516, 0.5433, 0.4812,   -inf,   -inf],\n",
       "        [0.3186, 0.3305, 0.3242, 0.3406, 0.2071,   -inf],\n",
       "        [0.5518, 0.6177, 0.6087, 0.5264, 0.3828, 0.6209]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redoing üòã:\n",
    "attention_score = torch.matmul(Q, K.transpose(-1, -2))  / K.shape[-1]**0.5\n",
    "mask = attention_score.triu(1).bool()\n",
    "attention_score = attention_score.masked_fill(mask, -torch.inf)\n",
    "attention_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381aa0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5029, 0.4971, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3370, 0.3328, 0.3303, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2430, 0.2589, 0.2568, 0.2413, 0.0000, 0.0000],\n",
       "        [0.2027, 0.2051, 0.2038, 0.2072, 0.1813, 0.0000],\n",
       "        [0.1662, 0.1775, 0.1759, 0.1620, 0.1403, 0.1781]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = attention_score.softmax(-1)\n",
    "attention_weights # üòã Masked Attention-Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b926a347",
   "metadata": {},
   "source": [
    "## **Dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "24ebe81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5029, 0.4971, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3370, 0.3328, 0.3303, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2430, 0.2589, 0.2568, 0.2413, 0.0000, 0.0000],\n",
       "        [0.2027, 0.2051, 0.2038, 0.2072, 0.1813, 0.0000],\n",
       "        [0.1662, 0.1775, 0.1759, 0.1620, 0.1403, 0.1781]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d51589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4285714285714286"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = nn.Dropout(0.3)\n",
    "dropout(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae26f7e5",
   "metadata": {},
   "source": [
    "## **Causal Attention (Integrating stuff till now)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "9c9234d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7859,  0.2289, -0.9346],\n",
       "        [ 0.3012,  0.4350, -0.2510],\n",
       "        [ 0.6503,  0.6423, -0.6436],\n",
       "        [ 0.0915,  0.1625, -0.0437],\n",
       "        [ 0.1474,  0.2837, -0.2065],\n",
       "        [ 0.1586,  0.2490, -0.1109]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, dropout=0.5, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''x: 3D tensor. (batch, num_tokens, embed_dim)'''\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2))\n",
    "        mask = attn_scores.triu(1).bool()\n",
    "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores, -1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vectors = torch.matmul(attn_weights, V)\n",
    "        return context_vectors\n",
    "\n",
    "d_in = d_out = x.shape[-1]\n",
    "causalAttention = CausalAttention(d_in, d_out)\n",
    "causalAttention(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "cffb44d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "d877c4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 3])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc54eb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3677,  1.2444,  0.2321],\n",
       "         [-0.1909,  0.6462,  0.1205],\n",
       "         [-0.0668,  0.5810, -0.0445],\n",
       "         [-0.0911,  0.3084,  0.0575],\n",
       "         [-0.1181,  0.5126,  0.0071],\n",
       "         [-0.0893,  0.5791, -0.0138]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Improvised Version (Considering Various Factors):\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, contextLength, dropout=0.5, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.ones(contextLength, contextLength).triu(1).bool())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''x: 3D tensor. (batch, num_tokens, embed_dim)'''\n",
    "        b, num_tokens, embed_dim = x.shape\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2))\n",
    "        attn_scores = attn_scores.masked_fill(mask[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores, -1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vectors = torch.matmul(attn_weights, V)\n",
    "        return context_vectors\n",
    "\n",
    "d_in = d_out = x.shape[-1]\n",
    "causalAttention = CausalAttention(d_in, d_out, 30)\n",
    "causalAttention(x.unsqueeze(0)) # x.unsqueeze(0) b/c x is 2D, and we expected a 3D tensor, so adding a extra dimension, in order to give it a batch size of 1\n",
    "# x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ff9f02",
   "metadata": {},
   "source": [
    "# **üòãüòã Multihead - Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6476a01c",
   "metadata": {},
   "source": [
    "## **General Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "eaa92d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [0.43, 0.15, 0.89],\n",
    "    [0.55, 0.87, 0.66],\n",
    "    [0.57, 0.85, 0.64],\n",
    "    [0.22, 0.58, 0.33],\n",
    "    [0.77, 0.25, 0.10],\n",
    "    [0.05, 0.80, 0.55]\n",
    "])\n",
    "\n",
    "batch = torch.stack([x, x])\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9c7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6, 2])"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, num_tokens, token_embed = batch.shape # (2, 6, 3)\n",
    "\n",
    "d_in = 3\n",
    "d_out = 8\n",
    "num_heads = 4\n",
    "# Each [Attention-Head] outputing 2 outputs\n",
    "d_head = d_out // num_heads\n",
    "\n",
    "W_q = nn.Linear(d_in, d_out, bias=False)\n",
    "W_k = nn.Linear(d_in, d_out, bias=False)\n",
    "W_v = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "Q = W_q(batch)  # (2, 6, 8) = (batch_size, num_tokens, token_embedOut)\n",
    "K = W_k(batch)\n",
    "V = W_v(batch)\n",
    "\n",
    "Q = Q.view((b, num_tokens, num_heads, d_head)) # (2, 6, 4, 2) = (batch_size, num_tokens, num_heads, d_head)\n",
    "K = K.view((b, num_tokens, num_heads, d_head)) \n",
    "V = V.view((b, num_tokens, num_heads, d_head)) \n",
    "\n",
    "Q = Q.transpose(1, 2) # (2, 4, 6, 2) = (batch_size, num_heads, num_tokens, d_head)\n",
    "K = K.transpose(1, 2)\n",
    "V = V.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bcacc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores = torch.matmul(Q, K.transpose(-1, -2)) # (2, 4, 6, 6)\n",
    "\n",
    "# Right now, `num_tokens = 6` & Let's set the context-length to be 10.\n",
    "context_length = 10 # i.e A batch can have this no. of max num_tokens, in each sample\n",
    "mask = torch.ones(context_length, context_length).triu(1).bool()\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "14daf329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0132,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.0570,  0.0728,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.0539,  0.0612,  0.0605,    -inf,    -inf,    -inf],\n",
       "          [ 0.0470,  0.0678,  0.0669,  0.0378,    -inf,    -inf],\n",
       "          [-0.0184, -0.1650, -0.1611, -0.1117, -0.0464,    -inf],\n",
       "          [ 0.0790,  0.1816,  0.1783,  0.1110,  0.0688,  0.1488]],\n",
       "\n",
       "         [[ 0.0330,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.0895,  0.1253,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.0928,  0.1316,  0.1219,    -inf,    -inf,    -inf],\n",
       "          [ 0.0416,  0.0516,  0.0463,  0.0409,    -inf,    -inf],\n",
       "          [ 0.1259,  0.2086,  0.1993,  0.1374, -0.0235,    -inf],\n",
       "          [ 0.0132, -0.0045, -0.0089,  0.0076, -0.0852,  0.0494]],\n",
       "\n",
       "         [[-0.0204,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.0255, -0.0348,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.0246, -0.0330, -0.0304,    -inf,    -inf,    -inf],\n",
       "          [-0.0155, -0.0171, -0.0153, -0.0133,    -inf,    -inf],\n",
       "          [-0.0005,  0.0091,  0.0094,  0.0049,  0.0127,    -inf],\n",
       "          [-0.0256, -0.0362, -0.0335, -0.0263,  0.0244, -0.0534]],\n",
       "\n",
       "         [[-0.1684,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.0095,  0.0086,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.0130,  0.0111,  0.0103,    -inf,    -inf,    -inf],\n",
       "          [ 0.0309, -0.0233, -0.0235, -0.0189,    -inf,    -inf],\n",
       "          [-0.0724,  0.0527,  0.0544,  0.0399,  0.0690,    -inf],\n",
       "          [ 0.0624, -0.0454, -0.0469, -0.0343, -0.0599, -0.0212]]],\n",
       "\n",
       "\n",
       "        [[[-0.0132,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.0570,  0.0728,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.0539,  0.0612,  0.0605,    -inf,    -inf,    -inf],\n",
       "          [ 0.0470,  0.0678,  0.0669,  0.0378,    -inf,    -inf],\n",
       "          [-0.0184, -0.1650, -0.1611, -0.1117, -0.0464,    -inf],\n",
       "          [ 0.0790,  0.1816,  0.1783,  0.1110,  0.0688,  0.1488]],\n",
       "\n",
       "         [[ 0.0330,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.0895,  0.1253,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.0928,  0.1316,  0.1219,    -inf,    -inf,    -inf],\n",
       "          [ 0.0416,  0.0516,  0.0463,  0.0409,    -inf,    -inf],\n",
       "          [ 0.1259,  0.2086,  0.1993,  0.1374, -0.0235,    -inf],\n",
       "          [ 0.0132, -0.0045, -0.0089,  0.0076, -0.0852,  0.0494]],\n",
       "\n",
       "         [[-0.0204,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.0255, -0.0348,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.0246, -0.0330, -0.0304,    -inf,    -inf,    -inf],\n",
       "          [-0.0155, -0.0171, -0.0153, -0.0133,    -inf,    -inf],\n",
       "          [-0.0005,  0.0091,  0.0094,  0.0049,  0.0127,    -inf],\n",
       "          [-0.0256, -0.0362, -0.0335, -0.0263,  0.0244, -0.0534]],\n",
       "\n",
       "         [[-0.1684,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.0095,  0.0086,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.0130,  0.0111,  0.0103,    -inf,    -inf,    -inf],\n",
       "          [ 0.0309, -0.0233, -0.0235, -0.0189,    -inf,    -inf],\n",
       "          [-0.0724,  0.0527,  0.0544,  0.0399,  0.0690,    -inf],\n",
       "          [ 0.0624, -0.0454, -0.0469, -0.0343, -0.0599, -0.0212]]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attn_scores = attn_scores.masked_fill(mask[: num_tokens, : num_tokens], -torch.inf)\n",
    "masked_attn_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "cf71e3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4961, 0.5039, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3318, 0.3342, 0.3340, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2480, 0.2532, 0.2530, 0.2458, 0.0000, 0.0000],\n",
       "          [0.2167, 0.1872, 0.1879, 0.1974, 0.2107, 0.0000],\n",
       "          [0.1585, 0.1757, 0.1751, 0.1637, 0.1569, 0.1700]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4911, 0.5089, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3258, 0.3387, 0.3355, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2491, 0.2516, 0.2503, 0.2490, 0.0000, 0.0000],\n",
       "          [0.1986, 0.2157, 0.2137, 0.2009, 0.1710, 0.0000],\n",
       "          [0.1696, 0.1666, 0.1658, 0.1686, 0.1537, 0.1758]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5023, 0.4977, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3349, 0.3321, 0.3330, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2500, 0.2496, 0.2500, 0.2505, 0.0000, 0.0000],\n",
       "          [0.1985, 0.2004, 0.2005, 0.1995, 0.2011, 0.0000],\n",
       "          [0.1665, 0.1648, 0.1652, 0.1664, 0.1751, 0.1620]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4955, 0.5045, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3281, 0.3361, 0.3358, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2600, 0.2463, 0.2463, 0.2474, 0.0000, 0.0000],\n",
       "          [0.1805, 0.2046, 0.2049, 0.2020, 0.2079, 0.0000],\n",
       "          [0.1816, 0.1630, 0.1628, 0.1649, 0.1607, 0.1670]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4961, 0.5039, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3318, 0.3342, 0.3340, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2480, 0.2532, 0.2530, 0.2458, 0.0000, 0.0000],\n",
       "          [0.2167, 0.1872, 0.1879, 0.1974, 0.2107, 0.0000],\n",
       "          [0.1585, 0.1757, 0.1751, 0.1637, 0.1569, 0.1700]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4911, 0.5089, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3258, 0.3387, 0.3355, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2491, 0.2516, 0.2503, 0.2490, 0.0000, 0.0000],\n",
       "          [0.1986, 0.2157, 0.2137, 0.2009, 0.1710, 0.0000],\n",
       "          [0.1696, 0.1666, 0.1658, 0.1686, 0.1537, 0.1758]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5023, 0.4977, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3349, 0.3321, 0.3330, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2500, 0.2496, 0.2500, 0.2505, 0.0000, 0.0000],\n",
       "          [0.1985, 0.2004, 0.2005, 0.1995, 0.2011, 0.0000],\n",
       "          [0.1665, 0.1648, 0.1652, 0.1664, 0.1751, 0.1620]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4955, 0.5045, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3281, 0.3361, 0.3358, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2600, 0.2463, 0.2463, 0.2474, 0.0000, 0.0000],\n",
       "          [0.1805, 0.2046, 0.2049, 0.2020, 0.2079, 0.0000],\n",
       "          [0.1816, 0.1630, 0.1628, 0.1649, 0.1607, 0.1670]]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attn_weights = masked_attn_scores.softmax(-1)\n",
    "masked_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "cc88f209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6, 2])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = masked_attn_weights @ V\n",
    "context_vectors.shape # (2, 4, 6, 2) = (batch_size, num_heads, num_tokens, d_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ef281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 8])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = context_vectors.transpose(1, 2) # (2, 6, 4, 2) = (batch_size, num_tokens, num_heads, d_head)\n",
    "# NOTE: The transpose() operation above makes the tensor `context_vectors` non-contigous.\n",
    "context_vectors = context_vectors.contiguous().view((b, num_tokens, d_out))\n",
    "# context_vectors = context_vectors.view((b, num_tokens, d_out)) # ‚ùå\n",
    "context_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "938306de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 8])"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In one-place:\n",
    "context_vectors = masked_attn_weights @ V\n",
    "context_vectors.shape # (2, 4, 6, 2) = (batch_size, num_heads, num_tokens, d_head)\n",
    "context_vectors = context_vectors.transpose(1, 2) # (2, 6, 4, 2) = (batch_size, num_tokens, num_heads, d_head)\n",
    "context_vectors = context_vectors.contiguous().view((b, num_tokens, d_out))\n",
    "context_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb3c49",
   "metadata": {},
   "source": [
    "## **Creating Multihead-Atten Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2aaed11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1669,  0.0420, -0.0686,  0.1521, -0.0018,  0.2404, -0.2778,\n",
       "          -0.3507],\n",
       "         [-0.4062,  0.7697, -0.3177,  0.6036, -0.0229,  0.6004,  0.4584,\n",
       "          -0.3751],\n",
       "         [-0.2924,  0.5159, -0.2394,  0.3982,  0.0052,  0.5141,  0.2366,\n",
       "          -0.3548],\n",
       "         [-0.0708,  0.1552, -0.2544,  0.1053, -0.0647,  0.3601, -0.1160,\n",
       "          -0.6176],\n",
       "         [-0.1087,  0.2616, -0.2624,  0.1804,  0.0019,  0.4438,  0.0662,\n",
       "          -0.5143]],\n",
       "\n",
       "        [[-0.5400,  0.6497, -0.0971,  0.6654,  0.5507,  0.2037,  0.2026,\n",
       "          -0.1828],\n",
       "         [ 0.0379,  0.1156, -0.2986,  0.0354,  0.0167,  0.3991, -0.0222,\n",
       "          -0.6254],\n",
       "         [-0.4015,  0.5563, -0.1378,  0.4881,  0.1133,  0.4257,  0.1552,\n",
       "          -0.2579],\n",
       "         [-0.1357,  0.6337, -0.3312,  0.3105, -0.0815,  0.5367,  0.3753,\n",
       "          -0.4939],\n",
       "         [-0.0886,  0.4219, -0.2576,  0.2222,  0.0583,  0.4556,  0.2306,\n",
       "          -0.3500]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, n_heads, context_length, dropout=0.5, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % n_heads == 0)\n",
    "\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.d_head = (d_out // n_heads)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer('mask', torch.ones(context_length, context_length).triu(1).bool())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''x: 3D. x => (batch_size, num_tokens, token_embed)'''\n",
    "        b, n_tokens, token_embed = x.shape\n",
    "        assert self.d_in == token_embed\n",
    "        \n",
    "        Q = self.W_q(x) # (b, n_tokens, d_out)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        Q = Q.view(b, n_tokens, self.n_heads, self.d_head) # (b, n_tokens, n_heads, d_head)\n",
    "        K = K.view(b, n_tokens, self.n_heads, self.d_head) \n",
    "        V = V.view(b, n_tokens, self.n_heads, self.d_head) \n",
    "\n",
    "        Q = Q.transpose(1, 2) # (b, n_heads, n_tokens, d_head)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-1, -2)) / self.d_head**0.5 #K.shape[-1]**0.5\n",
    "        attn_scores = attn_scores.masked_fill(self.mask[: n_tokens, : n_tokens], -torch.inf)\n",
    "        attn_weights = attn_scores.softmax(-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vectors = attn_weights @ V\n",
    "        context_vectors = context_vectors.transpose(1, 2)\n",
    "        context_vectors = context_vectors.contiguous().view(b, n_tokens, self.d_out)\n",
    "        return self.out_proj(context_vectors)\n",
    "\n",
    "x = torch.rand((2, 5, 4))\n",
    "d_in = x.shape[-1]\n",
    "d_out = 8 \n",
    "n_heads = 4\n",
    "# d_head = d_out / n_heads = 4 / 2 = 2\n",
    "mha = MultiheadAttention(d_in, d_out, n_heads, context_length=20)\n",
    "mha(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
